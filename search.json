[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yava Vilar Valera",
    "section": "",
    "text": "My name is Yava Vilar Valera.\nI was born in Valencia, Spain, but I grew up in Navaluenga, a small village in the province of Avila. I later moved to Belgium where I lived for three years and where I learnt french. I came back to Valencia with fifteen where some years later, in 2020, I started my Degree in Economics. I completed my Bachelor in 2024 in Nantes, France, through an International Double Degree program.\nCurrently I am studying a Master’s degree in Econometrics and Statistics at the University of Nantes."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Essay: The Spanish Uneven Industrialization and Regional Inequality",
    "section": "",
    "text": "In my second year of studying Economics, I wrote a short essay for the Economic History course on a topic that seemed particularly interesting to me: how Spain industrialized unevenly in the nineteenth century and how this led to increased regional economic inequality. Industrialization was a significant process in history that brought about many important changes in societies, such as the introduction of machinery and advanced technologies, urbanization, capitalism and economic growth, and shifts in mentality and political ideas. However, inequalities became a major issue, as the gap between the rich and the poor widened. This could occur on an individual level due to disparities in income between owners and workers, but it could also be pronounced at a regional level, where industrialization affected some regions first while others remained stagnant. My essay focused on this latter case."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Academic works",
    "section": "",
    "text": "Master 1 Dissertation: How can an earthquake affect children health?\n\n\n\n\n\n\nYava Vilar Valera\n\n\n03 Jun 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTechonological change and employment\n\n\n\n\n\n\nYava Vilar Valera\n\n\n23 May 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Spanish Uneven Industrialization and Regional Inequality\n\n\n\n\n\n\nYava Vilar Valera\n\n\n03 Dec 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "University of Valencia",
    "section": "",
    "text": "PhantomJS not found. You can install it with webshot::install_phantomjs(). If it is installed, please make sure the phantomjs executable can be found via the PATH variable.\n\n\nShiny applications not supported in static R Markdown documents"
  },
  {
    "objectID": "posts/welcome/My Map.html",
    "href": "posts/welcome/My Map.html",
    "title": "Untitled",
    "section": "",
    "text": "Listening on http://127.0.0.1:7103"
  },
  {
    "objectID": "posts/welcome/Advices to International studies.html",
    "href": "posts/welcome/Advices to International studies.html",
    "title": "University of Valencia",
    "section": "",
    "text": "prueba"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Yava Vilar Valera",
    "section": "",
    "text": "My name is Yava Vilar Valera.\nI was born in Valencia, Spain, but I grew up in Navaluenga, a small village in the province of Avila. I later moved to Belgium where I lived for three years and where I learnt french. I came back to Valencia with fifteen where some years later, in 2020, I started my Degree in Economics. I completed my Bachelor in 2024 in Nantes, France, through an International Double Degree program.\nCurrently I am studying a Master’s degree in Econometrics and Statistics at the University of Nantes."
  },
  {
    "objectID": "posts/welcome/Spanish economy.html",
    "href": "posts/welcome/Spanish economy.html",
    "title": "Essay: Technological change and employment",
    "section": "",
    "text": "Technology is rapidly growing and bringing innovations to society. While these advancements benefit us in many ways—such as through medical breakthroughs, task facilitation, faster communication, and improved transportation—they also raise concerns about the unemployment they might cause if human labor is replaced.\nThe book The Era of Digital Disruption by Javier Andrés and Rafael Doménech is one of many that explore how the digital revolution is transforming the economy and work. The authors analyze how technology has radically changed the way we consume, produce, behave, and how this transformation affects productivity, employment, and income distribution.\nI wrote a one-thousand-word essay explaining this phenomenon and discussing the authors’ stance on the relationship between technology and employment:"
  },
  {
    "objectID": "posts/Nepal thesis/Nepal project.html",
    "href": "posts/Nepal thesis/Nepal project.html",
    "title": "Master’s Thesis: Assessing the Impact of the Nepal 2015 Gorkha Earthquake on Children’s Health",
    "section": "",
    "text": "Earthquakes, being one of the most devastating natural phenomena on Earth, can cause great damage to human health, fostering, in addition to numerous emotional damages, the emergence of diseases. Among them, cough can reduce the quality of life of those who suffer from it, and its frequency in the population may increase after such an event, given the resulting dust and pollution, the loss of economic means and the worsening of the quality and quantity of clinical services available, among others.\nOn the other hand, Nepal is an underdeveloped country where many children still do not have access to decent and dignified living conditions, and its frequent exposure to earthquakes aggravate these issues. Children under five represent a group of individuals in society who are particularly susceptible to diseases, while their physical and emotional well-being is essential for their development as individuals. The analysis seeks to identify the extent of damage caused, in terms of prevalence of childhood coughing, by the 2015 Nepal Gorkha earthquake.\nThis earthquake, which struck on April 25, 2015, had a magnitude of 7.8 and caused massive destruction and loss of life across central Nepal. It resulted in nearly 9,000 deaths and over 22,000 injuries, while economic losses were estimated at around $7 billion USD, equivalent to about one-third of Nepal’s GDP at the time.\n\n\nData were sourced from Demographic and Health Surveys (DHS), a set of nationally representative surveys that collect information on health and demographics in developing countries. Conducted by ICF International and funded primarily by the United States Agency for International Development (USAID), these surveys address topics such as infant mortality, nutrition, reproductive health, and family planning, providing detailed and internationally comparable information. DHS contain several data files for different family members, such as woman, man, children, and households as a whole. In the analysis, children and households’ files regarding the country of Nepal for the years 2011 (5,038 observations) and 2016 (5,306 observations) are managed. Additionally, geographic coordinates of each surveyed household were required to compute distances to the earthquake’s epicenters, a key aspect of the research. Geospatial data have also been included to specify control variables in the model.\n\n\n\nThe Difference-in-Differences is an econometric technique used to estimate the causal effect of an intervention or policy by comparing changes in outcomes over time between a treatment group, made up of units who receive the intervention, and a control group that serves as counterfactual for treated individuals (i.e. what would have happened to the treatment group in the absence of the intervention). In this case, the earthquake is interpreted as the intervention. Therefore, we consider that individuals under the treatment are children living in areas close to epicentres, whereas the control set is composed of children who live in remote areas. The impact on cough prevalence was measured considering as treated the children living within a wide range of kilometres of either of the two major epicenters (Gorkha district on 15 April, 7,8 magnitude; and Dolakha district, on 12 may, 7,3 magnitude), with the remaining area of the country as control units, the main results of which will be shown below. The mathematical equation is presented next:\nYit = α + β1𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡𝑖 + β2𝑃𝑜𝑠𝑡𝑡 + β3 (Treatment ∗ Post) 𝑖𝑡 + β4X𝑖𝑡 + β5D𝑖𝑡 + ϵ\nWhere 𝑌 reflects the outcome of interest (cough prevalence) for individual 𝑖 at time 𝑡; α is a constant; β the coefficient of each correspondent variable; 𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡 indicates whether the child belongs to the treatment group (1 if that is the case, 0 otherwise); and 𝑃𝑜𝑠𝑡 takes the value 1 if individuals belong to the post-earthquake period, 0 if not. X corresponds to a set of control variables and D represents region fixed effects. Finally, ϵ is the error term.\n\n\n\nFindings have demonstrated a significant impact up to a distance of 80 kilometers. Table 1 presents the estimates obtained from the above equation considering this distance range. β3 , the coefficient corresponding to the variable Treatment*After, is the DiD estimator which measures the causal effect of the treatment. Its significance at the 10% threshold indicates a significant difference, over the two periods analyzed, in the cough rate in the areas affected by the earthquake relative to non-affected regions. The positiveness of the sign indicates that being within 80 kilometers in the post-treatment group increases the likelihood of coughing compared to the remaining areas.\n\n\n\n\n\nFigure 1 shows the evolution of cough prevalence in both treatment and control groups from the year 2001. We can observe how the cough incidence index, measured from 0 to 1 as a proportion of the sample analyzed, has decreased for both groups in the ten years prior to the earthquake, although it increased slowly in 2011 relative to 2006. After the shock in 2015, the proportion of children suffering from the disease goes up, even if not in a very pronounced way, for treated children. The control region, on the contrary, slightly lowers its cough incidence. The groups compared do not perfectly exhibit parallel trends during the pre-earthquake period, although they are similar. Even though covariates have been included in the model, possibly they would not have followed an exactly parallel evolution had the earthquake not taken place, and this could represent some source of little bias in the results. Figure 2 exhibits a map of Nepal showing the two major epicenters as well as the households surveyed who have been found to be injured by the earthquake.\n\n\n\n\nIn conclusion, the study has analyzed the impact that the 2015 Gorkha earthquake of Nepal had on cough prevalence in children under five. For that, children close to epicenters have been compared to the remaining regions of the country over time. Results have revealed an increase in the rate of the disease in areas within a circle of up to 80 kilometers of any of the two major epicenters. Given the frequency of earthquakes in Nepal and its low buildings construction quality, it is recommended to invest in resistant material to these hazards. Likewise, the importance of counting with health services and qualified professionals after such an event is highlighted. Children are the future of society, and their care is crucial for their development and happiness."
  },
  {
    "objectID": "experience.html",
    "href": "experience.html",
    "title": "Relevant experience",
    "section": "",
    "text": "International Double Degree Experience\n\n\n\n\n\n\nYava Vilar Valera\n\n\n03 Dec 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "academic works.html",
    "href": "academic works.html",
    "title": "Academic projects",
    "section": "",
    "text": "Can we distinguish AI from human writing?\n\n\n\n\n\n\nJasmine, Emma, Valorys, Yava\n\n\n17 May 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing the Determinants of Real Estate Prices in France\n\n\n\n\n\n\nMarie Kerhoas, Yava Vilar Valera\n\n\n23 Feb 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssessing the Empirical Validity of the Kuznets Environmental Curve\n\n\n\n\n\n\nMarie Kerhoas, Yava Vilar Valera\n\n\n31 Dec 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPenalized Regressions: Analysis of Employment Growth Rate in Canada, 1981-2024\n\n\n\n\n\n\n\n\n\n06 Nov 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaster’s Thesis: Assessing the Impact of the Nepal 2015 Gorkha Earthquake on Children’s Health\n\n\n\n\n\n\n\n\n\n03 Jun 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting: Evolution of the Spanish Population\n\n\n\n\n\n\n\n\n\n30 Apr 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series: Analysis of the Mortality Rate of Children Under Five in Nepal\n\n\n\n\n\n\n\n\n\n19 Apr 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVerification of Salmon Authenticity Using the PLS-DA method\n\n\n\n\n\n\n\nJasmine Dupau, Yava Vilar Valera\n\n\n15 Mar 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNatural Resources and Civil Wars\n\n\n\n\n\n\n\n\n\n03 Jun 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTechnological Change and Employment\n\n\n\n\n\n\n\n\n\n23 May 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Spanish Uneven Industrialization and Regional Inequality\n\n\n\n\n\n\n\n\n\n03 Dec 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "experiences/International experience/International Double Degree.html",
    "href": "experiences/International experience/International Double Degree.html",
    "title": "International Double Degree Experience",
    "section": "",
    "text": "Irse a vivir, trabajar o estudiar fuera suele ser una experiencia que llama mucho la atención. Unos lo temen y prefieren evitarlo, otros lo idealizan y esperan aprender de ella un sinfín de cosas. Una de las maneras más comunes y fáciles hoy en día, si eres estudiante, de conseguir pasar un tiempo en el extranjero es a través de las becas Erasmus.\nEn mi caso, como estudiante de Economía en la Universidad de Valencia, quise participar en un programa de Doble Titulación Internacional con la Universidad de Nantes. Es decir, fue un intercambio de dos años en los que culminé mi carrera con los títulos de graduada de estas dos universidades. En mi universidad de destino me trataron como una estudiante propia de la universidad y seguí todas las clases e hice todos los exámenes con una promoción común, al igual que ocurrió durante mis dos primeros años en Valencia.\nEl programa que decidí escoger me intereso desde un principio ya que buscaba vivir fuera durante un tiempo prolongado. El francés es un idioma que ya conocía con anterioridad, lo cual facilito que me aceptaran en el programa, y siendo una persona a la que le gusta estudiar, quise añadir ese plus a mi carrera universitaria.\nestudios\nA las pocas semanas de llegar, me di cuenta de que cursar una Doble Titulación no solo requiere esfuerzo académico, sino también un gran compromiso personal. Es mucho más que estudiar en una nueva universidad: implica gestionar tu vida de forma independiente, desde las amistades que construyes hasta las decisiones y responsabilidades que asumes en lo cotidiano. Todo esto, en un entorno donde las costumbres, horarios y formas de comportarse son diferentes a lo que conocía.\nEn cuanto a la adaptación al país, esta fue relativamente sencilla para mí, ya que tenía conocimientos previos de francés y había vivido experiencias similares al mudarme anteriormente tanto dentro como fuera de mi país. Sin embargo, lo que realmente supuso un reto durante los primeros meses fue la exigencia académica. Ahí fue donde me di cuenta de la responsabilidad y el coraje necesarios para alcanzar altos objetivos mientras estudiaba en el extranjero. A medida que el tiempo pasaba y me adaptaba al nuevo sistema de aprendizaje, al ritmo de clases y examenes, se fue volviendo mas llevadero. No porque realmente el nivel de exigencia hubiera bajado, sino porque ya lo habia integrado en mi vida.\nvida social"
  },
  {
    "objectID": "experiences/International experience/International Double Degree.html#mi-experiencia",
    "href": "experiences/International experience/International Double Degree.html#mi-experiencia",
    "title": "International Double Degree Experience",
    "section": "Mi experiencia",
    "text": "Mi experiencia\nA continuación dividiré mi experiencia en algunas categorías : la ciudad, vida social, estudios, y consejos que le daría a cualquier persona que quiera estudiar en el extranjero.\n\nLa ciudad\nNantes es una ciudad situada en el noroeste de Francia, capital de la región Loire-Atlantique. Cuenta con mas de 300 000 habitantes y una gran vida universitaria. Para los españoles, Nantes podría ser considerada la Salamanca de Francia. Es una ciudad de tamaño medio, ni pequeña ni grande, aunque con mucho ocio, no necesitaras trasladarte a otras grandes ciudades para encontrar los servicios más comunes. A tres horas y media en coche de Burdeos y de Paris, se queda algo alejada de otras grandes ciudades del país, aunque hay bonitas ciudades más pequeñas que visitar a los alrededores como Rennes, La Rochelle, Angers, y pueblos con encanto y playa como Pornic. El clima deja algo que desear, ya que suele llover con frecuencia y estar nublado. Aun así, que no se exagere demasiado: también hay días de sol, y momentos de calor he pasado. Sin embargo, no suele ser suficiente. Un paraguas y vitamina D en pastillas sería una recomendación para vivir en esta ciudad. El transporte más común entre las personas jóvenes es el tranvía. Pasa con bastante frecuencia y está bien conectado, e incluso por las noches los fines de semana (viernes y sábados) hay servicio hasta las 2:30 de la madrugada y vuelve a funcionar desde las 4:30, lo cual es una ventaja si te gusta la vida nocturna. También hay autobuses y bicicletas que puedes alquilar. Durante los dos anos de mi estancia, estuve alojada en una habitación estudiantil en una residencia que la universidad reserva para los estudiantes internacionales. Se trata de una residencia publica gestionada por el CROUS, cuyo precio es asequible en comparación con el ámbito privado. Mi habitación tenía 9 metros cuadrados, un baño individual, una nevera y congelador, con cocina compartida. Aunque no era muy grande, conté con el espacio necesario para dormir, estudiar, moverme, e incluso en una ocasión dos amigas de España vinieron a visitarme y pudimos dormir tres personas en esos pocos metros cuadrados! En cuanto a deportes, Nantes hay muchas instalaciones deportivas cercanas a los campus universitarios y muchas opciones baratas para estudiantes. Un abono de 50 euros anuales permite a un estudiante practicar hasta tres deportes diferentes a la semana, dentro de una gran variedad de deportes que elegir. En mi caso, el primer año me inscribí a esta opción, hice escalada, bádminton y probe el gimnasio. El segundo ano opté por ir de vez en cuando a nadar de forma libre a una piscina para tal efecto cerca de mi facultad y residencia. Finalmente, los precios de Nantes son algo mas caros que en España, sobre todo a nivel de precio de alojamiento, comida, transporte y ocio, pero a nivel de servicios para estudiantes, suele ser mas asequible. Un ejemplo de ello es el deporte mencionado anteriormente. También se puede comer de lunes a viernes en un restaurante CROUS por 3,30 el menú. Además, hay mas ayudas publicas y las becas son mas generosas.\n\n\nVida social\nMi vida social en Nantes ha sido bastante favorable ya que soy una persona a la que le gusta salir y hacer planes, y allí he conocido a personas con estos mismos intereses con las que he compartido muchos momentos de diversión y amistad. La gran cantidad de culturas que he conocido hizo mi estancia mucho mas interesante y enriquecedora. Irse a vivir fuera puede unirte mucho a personas de tu mismo pais o de otros paises que esten en la misma situacion que tu, ya que se crea un vinculo de apoyo mutuo en un entorno en el que esto se convierte en algo muy valorado.\nEn mi caso, el primer ano conoci a varias personas procedentes de varias culturas. Por un lado, me hice una amiga española que tenía su habitación a pocas puertas de la mía y que veía a menudo en una de las cocinas de mi residencia, cuya estancia en Nantes fue un ano. Por otro lado, hice buenos amigos de Colombia, que conocí por primera vez a los pocos días de llegar a Nantes en la cocina. Hoy en día sigue siendo mi amigo, aunque la distancia haga que ya no nos veamos físicamente. Cuando él regreso a Colombia, creé amistad con otro colombiano y empezamos a salir de vez en cuando de fiesta con estudiantes internacionales de la misma residencia: uno de Brasil, uno de Alemania, y tres del Líbano. Con ellos me lo pasé realmente bien, fueron las primeras veces que realmente sali con un grupo de chicos con los que me sentía cómoda y feliz. El curso siguiente este grupo se desvaneció ya que todos se fueron de la residencia, algunos regresaron a sus países y otros se mudaron porque la universidad ya no les renovaba una plaza en la residencia. Yo me quedé. Cuando volví en septiembre mi segundo año después de haber pasado el verano en Valencia, y vi que todas estas personas a las que había conocido ya no estaban, me sentí triste y nostálgica, pensando en que no volvería a verlos por los pasillos ni en mi residencia.\nPero la vida sigue adelante, los días fueron pasando, y de nuevo me fui creando nuevas amistades. Entré en un máster que entraba dentro de mi programa de intercambio, y ese ano que me esperaba me trajo muchos momentos increíbles y que disfruté muchísimo. Amigos franceses que a penas hice el primer ano, los hice en el máster, y fueron realmente simpáticas conmigo, me alegraron las clases e hice planes con ellas fuera de la universidad. Además, conocí a mucha gente latina: de México, Colombia, Ecuador, Venezuela, Perú… Havana café, en el centro de la ciudad, y el Sao, situado en la isla de Nantes, son los bares que se convirtieron en sitios fijos de ocio. Cada día que salía conocía a alguien nuevo, luego me los volvía a encontrar en el mismo sitio… y con ellos descubrí mi pasión por el baile latino. Bailábamos de manera informal bachata, salsa, merengue, y me encantaba. Luego me apunté a clases de salsa, lo cual me hizo conocer aun a mas gente. Al final, llego un momento en el que salía casi todos los fines de semana, excepto cuando tenia que estudiar mucho, y he tenido experiencias de todo tipo. Algunos momentos menos divertidos ha habido, pero en general, mi vida social en Nantes ha sido muy enriquecedora. Nantes tiene muchas opciones para jóvenes, gente abierta, comunidad latina, franceses a los que le gusta la marcha… ¡Es difícil aburrirse!\n\n\nEstudios\nEn Nantes, como destino erasmus, hay que estudiar. No es extremadamente difícil, pero si hay que esforzarse y dedicarle tiempo a la universidad. Esto en función de los objetivos académicos que tengas, por supuesto. Con revisar la asignatura y conocer lo fundamental de cada curso, no habrá problemas en aprobar. Además, algunos profesores son más generosos con los estudiantes erasmus, hacen exámenes orales individuales y puntúan de manera más flexible. Pero no todos. En mi caso, al estar cursando una doble titulación, me puntuaban como al resto de la clase y hacia siempre los mismos exámenes que ellos. Yo le dedique mucho tiempo al estudio. El primer ano que llegué, me estresé y me sentía insegura frente a la nueva dificultad. Bajé algo mis notas, lo cual me preocupo. Pero seguí esforzándome, porque me importaba. Y así, siguiendo a pesar de las dificultades, uno se adapta y eso que costaba al principio acaba convirtiéndose en algo no tan difícil. El segundo semestre mejoré mis notas, al igual que el segundo ano en el máster. Es importante mencionar que el sistema de notas y aprobados en Francia es diferente al de España. Para empezar, las notas van del 0 a 20, en lugar de 0 a 10. Con un 10 sobre 20, se considera aprobado. Sin embargo, a diferencia de España, las asignaturas se agrupan en bloques de dos o tres asignaturas, y si suspendes una de ellas, puedes compensar con la nota de otra asignatura del mismo bloque. Incluso suspendiendo un bloque entero, puedes compensar con la nota de otro bloque. A fin de cuentas, debes tener una media superior a 10 para pasar de curso sin necesidad de pasar por las recuperaciones. No obstante, a veces hay excepciones de asignaturas o trabajos que hay que aprobar de forma independiente, y que si no lo haces, deberás pasar otro examen, con un máximo de dos oportunidades.  La forma de corregir suele ser mas exigente que en España, y el nivel de dificultad de los exámenes puede variar. En mi caso, he sacado mas 18s/20 en Francia que 9s/10 en España, pero también mas 12s/20 en Francia que 6s/10 en España. En Nantes, he visto notas mas diversas en una misma persona que en Valencia. Digamos que la varianza de las notas en los estudiantes de Nantes es mayor que la de los estudiantes en Valencia, según lo que yo he visto en algunas personas y también en mi propio caso."
  },
  {
    "objectID": "posts/Ressources et guerres/Ressources et guerres.html",
    "href": "posts/Ressources et guerres/Ressources et guerres.html",
    "title": "Natural Resources and Civil Wars",
    "section": "",
    "text": "The link between natural resources and civil wars is a central topic in the study of contemporary conflicts. In many regions, the abundance of resources such as petroleum, minerals, wood, or diamonds has been both a blessing and a curse. While in theory these resources should boost economic development, in practice they have provoked prolonged internal conflicts. Wars tend to intensify around this wealth, as armed groups seek to finance their operations through exploitation of these resources. Similarly, extreme climate events, such as extended droughts, can aggravate tensions by increasing competition for scarce and essential goods like water and arable land.\nI analyzed this relationship in my research document in the third grade of my degree in Economics, written in French. To do this, I based my project on two papers that I synthesized: “Le rôle des ressources naturelles dans les conflits armés africains” by Philippe Hugon (2009) and “Drought and civil war in Sub-Saharan Africa” by Mathieu Couttenier and Raphael Soubeyran (2013). You can find the project below, or you can click here."
  },
  {
    "objectID": "about me.html",
    "href": "about me.html",
    "title": "About me",
    "section": "",
    "text": "Born in Valencia, Spain, I grew up in Navaluenga, a small village in the province of Ávila, Spain. I later moved to Belgium where I lived for three years and learned French. In 2020, I began my degree in Economics at the Universitat de València. Two years later I enrolled in an International Double Degree program with Nantes Université. This unique opportunity allowed me to obtain my Bachelor in Economics from both universities in 2024.\nCurrently, I am pursuing a Master’s degree in Econometrics and Statistics at Nantes University, where I have acquired extensive skills in data management and coding, in addition to enhancing my statistical abilities to analyze economic issues.\nI define myself as a very passionate and motivated student, with a strong willingness to learn and achieve new goals. Not only do I like addressing questions of social and economic nature, but also writing and engaging in thorough and creative work. My academic and international experiences have shaped my global perspective and sparked a deep curiosity about the world. This drives my interest in working on global challenges.\nYou can find a version of my CV here. Feel free to contact me at vilarvaleray@gmail.com, I am always happy to discuss :-)"
  },
  {
    "objectID": "about me.html#about-me",
    "href": "about me.html#about-me",
    "title": "Yava Vilar Valera",
    "section": "",
    "text": "Born in Valencia (Spain), I grew up in Navaluenga, a small village in the province of Ávila, Spain. I later moved to Belgium where I lived for three years and learnt French. Then, I returned to Valencia, where, in 2020, I began my degree in Economics at the Universitat de València. Two years later I enrolled in an International Double Degree program with Nantes Université, which allowed me to obtain my Bachelor in Economics from both universities in 2024.\nCurrently, I am pursuing a Master’s program in Econometrics and Statistics at Nantes University, where I have acquired extensive skills in software and data management, in addition to enhancing my statistical abilities to analyze economic issues.\nI define myself as a very passionate and motivated student, with a strong willingness to learn and achieve new goals. Not only do I like addressing questions of social and economic nature, but also writing and engaging in thorough and creative work. My academic and international experiences have shaped my global perspective and sparked a deep curiosity about the world. This drives my interest in working on global challenges, particularly those related to economic development."
  },
  {
    "objectID": "posts/Séries temp Dorota/Séries temp.html",
    "href": "posts/Séries temp Dorota/Séries temp.html",
    "title": "Time Series: Analysis of the Mortality Rate of Children Under Five in Nepal",
    "section": "",
    "text": "Mortality rate reflects the number of children who die before reaching the age of five per 1,000 live births. In Nepal, this rate has historically been high due to factors such as poverty, limited access to healthcare, and undernutrition, though it has shown significant improvement over recent decades.\nUsing World Bank annual data from 1960 onwards, I applied the Box-Jenkins methodology to analyze the time series of under-five mortality in Nepal. The objective was to identify the most appropriate ARIMA model to capture the underlying trends and generate reliable forecasts. Additionally, I examined the relationship between under-five mortality and two key socio-economic variables: the fertility rate and GDP per capita.\nYou can find the project below or click here."
  },
  {
    "objectID": "posts/prévision et conjocture/prev et conj.html",
    "href": "posts/prévision et conjocture/prev et conj.html",
    "title": "Forecasting: Evolution of the Spanish Population",
    "section": "",
    "text": "This is a piece of work I did for my “Prévision et Conjoncture” course, where I learned several models and techniques to make future predictions about any variable. These models include Holt-Winters, CES (S)ARIMA and ADAM-ETS models. Population evolution seemed riveting to me and chose Spain as the territory to investigate. I assessed the accuracy of the different models and ultimately used the one that provided the lowest error rate when forecasting. Even though I focused on and made predictions on historical data (1277-1838), the same methodology can be extrapolated to current values. R was the software I used to manage the data.\n\nIntroduction\nSince the beginning of humanity, world’s population has been continously developing and growing. From a wide range of biological to economic causes that shape it, the population follows a trajectory of great relevance to analyze since this trajectory has numerous implications in the economic and social structure, in addition to other areas as important such as the environment and biodiversity. On the one hand, economic growth has historically driven population growth while a greater number of people stimulates demand for goods and services, employment, and economic activity in general. On the other hand, the limitation of available resources and environmental degradation seems to be greater as the population continues to grow. This work focuses on making prediction tests about the population of Spain based on historical data going from 1277 to 1838, in order find a suitable model that could be used in a real context.\n\n\n1. Data\nData source comes from databases provided by “La Fundación Real del Pino”, a private non-profit organization located in Madrid, Spain. Founded in 2006, its main goal is to promote research in diverse social sciences fields, particularly in economics and economic history. The series is composed of 561 years, plus 12 years that will be reserved to compare model-predictions with actual values. It is important to note that data before 1750 has been constructed from mathematical equations and models, which may not capture the complexity of the phenomenon they seek to represent. Therefore, careful attention is required when interpreting the results of this analysis. The original series is available in Figure 1.\n                                       Figure 1: Total Population in Spain, 1277-1838\n\nAn adjusted series to correct for one Level-Shift (LS) outlier has been constructed. Outliers are atypical, extreme values that can affect the quality of model fit and the reliability of predictions. In this case, the outlier detected in the year 1508 reflects a change of the population’s trajectory from a significant slowdown driven by the expulsion of the Jews from the country to a rapid and natural recovery.\nSimilarly, a differentiation has been applied to the original series in order to meet the stationarity hypotheses (constant mean and variance over time), essential to be able to model the data. The final series corrected from outliers and differenciated is available in Figure 2.\n                                       Figure 2: Adjusted series, 1277-1838\n\n\n\n2. Linear predictions\n\n2.1. Model’s parameters\nIn order to analyze temporal series and make future predictions, it is essential to previously estimate models that capture the dependence parameters of past values and other predictor variables. The main assumption of linear models is that the relationship between variables is linear. The models’ parameters that have been monopolized are summarized in Table 1 and explained next:\n\nTable 1: Model’s parameters\n\n\n\n\n\n\n\n\n\n\n\n\nAR(1)\nARIMA(0,2,2)\nHolt-Winters\nADAM-ETS\nSSARIMA\nCES\n\n\n\n\nAR1\n0.4820\n\n\n\n\n\n\n\nMA1\n\n-0.5602\n\n\n0.5009\n\n\n\nMA2\n\n-0.4167\n\n\n0.2134\n\n\n\nMA3\n\n\n\n\n0.0755\n\n\n\nAlpha\n\n\n1\n1\n\n\n\n\nBeta\n\n\n0.4401\n0.9999\n\n\n\n\nPhi\n\n\n\n1\n\n\n\n\nDistribution\nNormal\nNormal\n\nNormal Generalised\nNormal\nPartial\n\n\nAIC\n- 1367.96\n\n-1344.38\n-3395.009\n-1367.81\n-1231.55\n\n\nAICc\n\n\n-1344.33\n-3394.748\n-1367.70\n-1231.50\n\n\n\n\nAR(1): describes how the current value of a variable is influenced by its immediately preceding value and a stochastic error term. The parameter of this model is 0.4820. A value lower than 1 indicates a stationary series that tends to revert its mean, that is, that tends to converge towards a mean value in the long term. The positiveness of the coefficient reveals an increase of each current value as long as time goes up. Indeed, a positive term AR(1) implies that when the value of the time series t-1 is greater than its mean, the value at time t is also greater than its mean, suggesting a pattern of growth over time.\nARIMA: It combines three components: autoregression (AR), differencing (I for Integrated), and moving averages (MA). The AR part of the model uses the relationship between an observation and a number of lagged observations (previous time points). p is the number of lagged observations included in the model. The I component involves differencing the data to make it stationary. The parameter d indicates the number of times the series is differenciated. The MA part of the model uses the relationship between an observation and a residual error from a moving average model applied to lagged observations. The parameter q is the number of lagged forecast errors in the prediction equation. Our model is characterized by having two parameters Moving Average processus significant at the 1% level. This shows a dependency relationship between the value at a specific point of time t and error term both at the immediate previous period, t-1, and at the second previous period t-2. Negative coefficients indicates this relationship is negative and the greater coefficient of MA(1) compared to MA(2) suggests that the error term at t-1 has a higher effect on the current value.\nHolt-Winters: Appeared in the 1960s, Holt-Winters method is a prognostic technique that can capture both tendence and seasonality in data, with the advantage of being able to adapt to no-seasonal series. Updates to the level, trend, and seasonality components are based on three smoothing parameters: α, β, and γ, where α (alpha) controls the smoothing of the level component, β (beta) controls the trend component, and γ (gamma) the seasonality component. As the series analyzed on this work does not have a seasonal component, Holt-Winters model has been estimated in R specifying an unexistent gamma parameter. In the instances when α and β parameters are close to 1, we talk about a “smooth” forecast, while if they are close to 0, they reflect a “strong smoothing”. In the first case, recent past has a big weight on forecasting. In the second case, a higher importance is given to distant past. In this case, we could say near past has a bigger weight than remote past, because α is completely equal to 1 and β coefficient is relatively important.\nADAM-ETS: The ADAM-ETS method is an approach that combines exponential smoothing (ETS) approaches with state space models to model time series with error, trend, and seasonality components, also allowing the inclusion of other covariates. The smoothing coefficient alpha controls the relative importance of past observations in estimating the current level in the ETS model. In this case, the estimated value of alpha is 1.0, specifying that full weight is given to the most recent observation in the calculation of the current level. Beta captures the relevance of diƯerences between past observations in estimating the current trend in the ETS model, which has a value of 0.9999. This indicates that diƯerences between past observations have a very important weight in approximating the current trend. Phi represents the smoothing coefficient associated with the trend. A value equal to 1 suggests a complete weight to recent observations when estimating the trend, which means that the trend will adjust more quickly to recent changes in the data.\nSSARIMA: They are a prediction technique that combines the State Space with ARIMA methodology for the analysis and forecasting of temporal data, whose acronym meaning is: “State Space ARIMA”. In this way, these models can include ARIMA components to model the time structure of the series, as well as additional state space components to shape other unobservable factors that can influence the time series. Auto.sarima function in R preconised an ARIMA(0,0,3) model with a constant. We can see in Table 1 that the weight of each previous period MA decreases with the higher number of parameters. That is, the influence of the most distant periods is smaller compared to the closest ones. Indeed, coefficient of MA(1) is 0,5 while that of MA(3) equals 0,075.\nCES: Proposed by Svetunkov, Kourentzes and Ord in 2022, CES models refer to “Complex Exponential Smoothing” models. They are an extension of traditional exponential lysing methods that are based on the theory of complex variables and oƯer an innovative way to model and forecast time series. They can predict both stationary and non-stationary series, and can be adapted to seasonal data through their formulation in a state space form. Auto.ces function of R software estimated a CES(n) model, where n represents the non-seasonality term.\n\nAdditionally, AIC (Akaike Information Criterion) and AICc (Corrected Information Criterion) have been included in order to compare models and be able to determine which one should be prioritized if we factor in this criterion. The AIC seeks to minimize the discrepancy between the model and the data, penalizing the complexity of the model. This means that a good model will be one that has a good fit but with a minimum number of parameters. The AICc is a correction to the AIC, especially useful when the sample size is small compared to the number of parameters.\nADAM-ETS offers the lowest AIC and AICc. This is the reason why this model is preferred to others if we consider this criterion. However, the differences of AIC and AICc between the different methods are small, meaning they have a similar quality in terms of their ability to fit the data and their complexity.\n\n\n2.2. Model’s predictions\nOnce the model’s parameters are defined, we can proceed to make predictions. For this, nine years of the original series had been kept with the aim to compare them with the forecast generated by the models. Figure 3 shows the forecast evolution of these nine years for each of the model along with actual data, going from 1839 to 1848. A naive prediction has also been included, which is a simple forecasting approach based on the assumption that the future value of a time series will be equal to the last observed value in the series. In other words, this method assumes there is no change in the time series and that the future value will be equal to the last observed one. Thus, this approach is useful as a reference to determine whether more complex models provide any significant prediction improvement.\n                                       Figure 3: Model’s predictions\n\n\n\n\n\nThe models that seem to better fit the real series are Holt-Winters, ARIMA and CES models, because they are the ones that have predicted a higher increase from 1838 to 1847. We observe the preference of ARIMA, which only contained MA components, compared to models with only an AR component. This suggests that the time series is more influenced by past errors than by past values. That is, the data may exhibit erratic or random behaviour, where future values depend primarily on past errors and not on a clear trend or pattern. Likewise, we can observe that Holt-Winters and CES methods have managed to fit the data relatively well. Indeed, they are designed to capture the trend and seasonality of the series (that in this case, there is no seasonality). The series grows rapidly over time, with a clear increasing trend, and these models, in particular Holt-Winters, are the ones, with ARIMA, that have the most predicted a pronounced increase with a higher growth rate. It appears that ADAM-ETS, on the contrary, even if it had the lowest AIC and AICc, presents higher error rates.\n\n\n\n3. Prediction evaluation\nTo evaluate the model’s performance, we are going to use four evaluation metrics:\n\nMean Squated Errors (MSE) : a statistical measure that provides a measure of how well the model achieves this goal. Indeed, this metric offers a quantitative assessment of the accuracy of a prediction model relative to the actual values. It is based on the difference between model predictions and true observations, squared to penalize larger errors, and then the average of these squared differences is calculated.\nDiebold-Mariano (DM) test: a statistical test used to compare the predictive accuracy of two time series models. This test was proposed by Francis X. Diebold and Robert S. Mariano in 1995, whose purpose is to determine whether there is a significant difference in the predictive performance between two models. The null hypothesis is that both models have the same predictive accuracy, while the alternative hypothesis is that at least one of the models offers a more accurate forecast. It will be used to compare each of the models with the naive forecast, which helps determine if the model in question is more interesting than making a simple prediction.\n𝑅 OOS: a measure of the predictive ability of a model when applied to new data that were not used to train the model. In essence, it measures how much variability in the dependent variable the model can explain in data not used in the fitting process. A higher out-of-sample R-squared value indicates a better predictive ability of the model on new data. It is also a valuable tool for comparing and evaluating the predictive performance of different models relative to a reference or benchmark model. If the value of this metric is greater than 0, it will be interpreted that the model in question has a superior predictive ability to the benchmark model. If it is less than zero, it will be the opposite, while if it is equal to 0, there will be no predictive difference. The naive forecast will be used as benchmark.\nCumulative Squared Prediction Error (CSPE): often used to evaluate the accuracy of predictions in time series models. It is calculated by adding the squared errors of the predictions over a given period. Squared errors are used to give more weight to large errors, reflecting the importance of avoiding large deviations in predictions.\nTable 2 summarizes the values of the three first mentioned metrics, while Figure 4 shows the CSPE.\n\nTable 2: Model’s performance indicators\n\n\nModel\nMSE\nR OOS\nDM statistic\nDM p. value\n\n\n\n\nAR(1)\n0.1109\n0.2472\n0\n1\n\n\nARIMA\n0.0796\n0.4597\n0\n1\n\n\nHolt-Winters\n0.0918\n0.3769\n0\n1\n\n\nADAM-ETS\n0.1359\n0.0778\n0\n1\n\n\nCES\n0.0857\n0.4183\n0\n1\n\n\nSSARIMA\n0.1130\n0.2331\n0\n1\n\n\nNaive\n0.1474\n0\n\n\n\n\n\n                                       Figure 4: Model’s CSPE\n\n\n\n\n\n\nWe can judge that all the models seem to provide better prediction results compared to a simple naive forecast. Indeed, the 𝑅 𝑂𝑂𝑆 is always positive and even some models have relatively elevated values such as ARIMA or CES, between 0.4 and 0.5. Equally important, the Mean Squared Error of each model is lower than 0.147 , the MSE of the naive model. However, if we look at the DM test statistic and p. value, we can see that null hypothesis is accepted, meaning that there is no significant difference in the forecast accuracy between model i and the naive prediction. If we consider the CSPE, ARIMA seems to be the best model because the accumulated sum of errors is the smallest one, even if at the firsts stages, it starts with a higher prediction error than other methods such as SSARIMA, ADAM-ETS and AR1. On the contrary, these last models end up having much higher CSPE values. This observation can suggest that there exists a difference on the prediction ability between models in function of the forecast horizon. At the very short-term, ADAM-ETS, SSARIMA and AR(1) provide more reliable results, whereas from a horizon of about 7 or 8 periods, ARIMA, CES and Holt-Winters become more accurate. Finally, if we had to choose a better model on which we could comparatively rely, it would be ARIMA, because it presents the highest 𝑅 𝑂𝑂𝑆 value and the lowest MSE and CSPE.\n\n\nConclusion\nIn conclusion, this work analyzed the trend of the historic evolution of the Spanish population. We have chosen and selected different prediction models to evaluate which one is more accurately adjusted to the structure of the series and which can be more reliable for use in a real-life situation. The result is that the ARIMA structure with three Moving Average processes has a minor prediction error. This model has predicted a tendency of increase for the near historical years, adjusting to the real life, with a satisfactory error term. Nevertheless, the analysis and exploration of other techniques might continue to get models with even lower prediction errors, and cross-validation with other independent data sources is recommended to confirm the robustness of the conclusions drawn from this constructed series."
  },
  {
    "objectID": "Personal interests.html",
    "href": "Personal interests.html",
    "title": "Personal interests",
    "section": "",
    "text": "One of my highest passions in life is music. Recently I’ve developed a very deep interest for many different genders of Latin music, especially Bachata, Kizomba and Salsa. All of them are danceable and listening to them always provide me with feelings of happiness, hope, joy, and motivation, regardless of my day-mood. It’s like a natural antidote against any emotional breakdown. Dancing makes part of my prefered sports and leisure activites, and practice it as soon as I can. Going to concerts is"
  },
  {
    "objectID": "Personal interests.html#music-dancing",
    "href": "Personal interests.html#music-dancing",
    "title": "Personal interests",
    "section": "",
    "text": "One of my highest passions in life is music. Recently I’ve developed a very deep interest for many different genders of Latin music, especially Bachata, Kizomba and Salsa. All of them are danceable and listening to them always provide me with feelings of happiness, hope, joy, and motivation, regardless of my day-mood. It’s like a natural antidote against any emotional breakdown. Dancing makes part of my prefered sports and leisure activites, and practice it as soon as I can. Going to concerts is"
  },
  {
    "objectID": "Personal interests.html#mountain-sea-sports",
    "href": "Personal interests.html#mountain-sea-sports",
    "title": "Personal interests",
    "section": "Mountain & Sea Sports",
    "text": "Mountain & Sea Sports"
  },
  {
    "objectID": "Personal interests.html#writing",
    "href": "Personal interests.html#writing",
    "title": "Personal interests",
    "section": "Writing",
    "text": "Writing"
  },
  {
    "objectID": "posts/spanish economy/Spanish economy.html",
    "href": "posts/spanish economy/Spanish economy.html",
    "title": "Technological Change and Employment",
    "section": "",
    "text": "Abstract\nTechnology is rapidly growing and bringing innovations to society. While these advancements benefit us in many ways - such as through medical breakthroughs, task facilitation, faster communication, and improved transportation - they also raise concerns about the unemployment they might cause if human labor is replaced.\nThe book The Era of Digital Disruption by Javier Andrés and Rafael Doménech is one of many that explore how the digital revolution is transforming the economy and work. The authors analyze how technology has radically changed the way we consume, produce, behave, and how this transformation affects productivity and employment.\nI wrote a one-thousand-word essay in my second year of the Bachelor in Economcis explaining this phenomenon and showing the authors’ stance on the relationship between technology and employment.\n\n\nEssay\nCurrently, many jobs are threatened by a growing technological change in the form of machines and robots that can perform tasks more efficiently than humans and and at a lower price, and therefore, replace them. The unease about this phenomenon is not new. As Javier Andrés and Rafael Doménech point out, since the first Industrial Revolution, the introduction of mechanization led to a significant loss of labor in agriculture. However, this change was followed by a structural shift towards the secondary sector, resulting in the creation of new jobs and employment opportunities in the industry. Given the new context we face, where changes are happening at a dizzying pace, the current revolution differs from previous ones in a greater uncertainty and the possibility of observing greater technological unemployment. How significant is this possibility? Will it affect all types of work in the same way? Will the effect be the same in the short and long term?\nFirst, the authors point out an important difference between a time horizon of three or four decades, which is the subject of analysis in their work, and the very long term. In the latter case, it is more likely that technological change will lead to massive unemployment; however, the book presents various arguments to demonstrate that in the coming decades, the chances of this happening are much lower. On the one hand, it supports the predictive economic theory that technological innovations mean an increase in productivity that helps improve worker efficiency, which in turn increases wages, employment, and aggregate demand. Additionally, there is evidence from various countries where productivity has increased while unemployment has simultaneously decreased. For instance, between 1900 and 2018 in the United States, a country and period that has witnessed significant technological revolutions, GDP per capita multiplied by 8.8, while unemployment decreased by 0.2%. The authors argue that if unemployment has increased in a particular country, as in the case of Spain or Greece in recent decades, it is due to specific characteristics of their labor markets and not to technological change.\nOn the other hand, although it is inevitable that some jobs will be lost because they can be performed more economically and effectively by a robot or machine, new types of jobs are created simultaneously—a phenomenon that is well-advanced in the United States and China. To name a few examples, in the tourism sector, customers are increasingly less attended to in person, and many activities are conducted online, such as booking flight tickets or hotel reservations. While this may result in fewer staff at service counters, there is a growing demand for marketing workers who study customer preferences and create and digitally promote products and services. Entrepreneurs can also build businesses based on the ownership of robots, and the growing availability of databases requires more data scientists. Likewise, technology can be beneficial if the increase in capital efficiency leads to a similar increase in worker efficiency by facilitating their tasks. When these two factors are complementary, labor demand expands. According to the book, the effect generated by complementarity is greater than the substitutive effect, so the overall economy would benefit from technological innovations.\nHowever, the fact that the economy as a whole benefits does not mean that all individuals and economic sectors do. In fact, one of the main disadvantages is that this change is likely to be accompanied by an increase in inequality between social groups and a disparate evolution of labor demand depending on worker qualifications. Polarization is the main phenomenon. That is, machines and robots tend to replace medium-skilled jobs, those characterized by routine tasks, while concentrating employment at the two extremes of qualification: at one end are sectors requiring a high level of knowledge and skills, and at the other, those requiring more non-routine manual labor. This process is known as “routine-biased technical change,” which characterizes our era compared to the “skill-biased technical change” prevalent during the Industrial Revolution, whose implication was that low-skilled workers are threatened, while high-skilled workers benefit from increased demand for labor and higher wages.\nFurthermore, social concerns are growing regarding the quality of employment, which, although it has increased over an extended time horizon, has recently been declining. In particular, wages, working hours, social protection in markets, and job security have been negatively impacted in some productive sectors. Additionally, as a consequence of polarization, a considerable portion of routine workers will inevitably be shifted to non-routine manual tasks, where job quality and remuneration are generally lower.\nIn conclusion, the book argues that technological change will not cause massive unemployment in the coming decades. However, attention must be paid to certain sectors of society, as while some will benefit from reduced work effort and higher wages, others will face lower demand and lower wages, potentially increasing income inequality. To address these issues, it is important to adopt policies, strategies, and labor regulations that adjust and renew at the same pace as the changing needs of the population."
  },
  {
    "objectID": "posts/Data Viz/Data Viz.html",
    "href": "posts/Data Viz/Data Viz.html",
    "title": "About Data Visualization",
    "section": "",
    "text": "DataViz (Data Visualization) is about transforming complex data into simple visual objects. These may include graphics, histograms, key values, maps, boxplots, tables, and so on. Its main goal is to identify relationships, patterns and trends within the data, and make it understandable to the viewer. DataViz is a paramount and worthwhile tool that I had the chance to be introduced to during my Master’s Degree. I mainly worked with R-Shiny, Python Dash, and SAS Visual Analytics software."
  },
  {
    "objectID": "posts/essay EH2/index.html",
    "href": "posts/essay EH2/index.html",
    "title": "The Spanish Uneven Industrialization and Regional Inequality",
    "section": "",
    "text": "Abstract\nI wrote a short essay for the economic history course on a topic that engaged me: how Spain industrialized unevenly in the nineteenth century and how this led to increased regional economic inequality. Industrialization was a significant process in history that brought about many important changes in societies, such as the introduction of machinery and advanced technologies, urbanization, capitalism and economic growth, and shifts in mentality and political ideas. However, inequalities became a major issue, as the gap between the rich and the poor widened. This could occur on an individual level due to disparities in income between owners and workers, but it could also be pronounced at a regional level, where industrialization affected some regions first while others remained stagnant. My essay focused on this latter case.\n\n\nEssay\nIndustrialization in Spain, whose origin dates to the first decades of the 19th century, represented a decisive point in the contemporary history of the country in terms of regional inequalities. Different studies have shown that the beginning of every modern economic growth spreads unevenly across space, affecting a few regions in a first instance and then increasing inequality (Williamson, 1965). Figure 1 shows this has been the Spanish case. Data provided by A.Carreras and X.Tafunell (2021) has been collected that analyse the trajectory of inequalities in terms of GDP per capita of the Spanish regions from the beginning of industrialization to its peak. For this, they have used the Gini index, an economic indicator that measures the level of inequality, ranging from zero (maximum uniformity) to one (maximum concentration). In it an ascending index is observed from the beginning of the 19th century until 1940.\n\n\n\n\n\nCatalonia and in a lesser extent the Basque Country were the leading regions that led the beginning of industrialization, while most of the rest of Spain faced various obstacles that prevented a rapid resurgence of this sector. As we can see in Figure 2, in which the three richest and most disadvantaged provinces have been selected in 1900 according to their nominal GDP per capita (NCDGP), Barcelona, Guipúzcoa and Vizcaya are the provinces found in the three first places. At the other extreme we find Cáceres, León and Lugo. With these figures, the differences between “rich” and “poor” are very appreciable.\n\n\n\n\n\nCatalonia is a region that rapidly became a powerful textile industry in the eighteenth century. On the contrary, in that same century, in the whole of Spain livestock and farming “represented around 70% of the total production” (Llombart, 2013) and it remained mainly agrarian until mid-twentieth century. Why was practically only this region able to catch-up with the main European industrial powers? Before such a revolution, it was a very agrarian society as the rest of Spain. However, along the eighteenth century it experienced social and economic changes, a phenomenon rigorously studied by P.Vilar in his work “La Catalogne dans l’Espagne moderne”, in which the foundations of the economic splendor experienced in the 19th century are set. On the one hand, the demographic increase is relevant. According to data provided by Jordi Nadal (1992), population went from being made up of 508,000 inhabitants in 1717 to 899,531 in 1787. From this, a growth rate of 77.07%1 is deducted. If we compare it with data on the growth of the whole Spain between these two dates, 7,500000 and 10,541.2000 respectively (Nadal, 1975), with a resulting growth rate of 38.79%, we can see that Catalan population has grown to a greater extent in relation to Spain, participating thus in the European demographic boom. This favorable boom for the Catalan economy was accompanied by an increase in agricultural productivity, especially in the vine sector, favored, among others, by the introduction of new irrigation techniques and the limitation of fallow land due to the use of natural fertilizers.\nNot only there was an intensive growth, also the cultivated areas increased. “With the exception, perhaps, of Cerdanya, there is no ‘country’ or region of the Principality of which some text from the 18th century does not indicate a more or less marked progress in the area under cultivation […] no region of some importance in the Catalan economic complex escaped this 18th century agricultural renovation” (Vilar,1962). With this information available and considering the Cobb-Douglas production function, it does not seem surprising that Catalonia participated in the process of modern economic growth. The rest of Spain, on the other hand, only managed to grow sparsely at an extensive level throughout 19th century, especially after Madoz disentailment- a liberal measure established in 1854 that disentailed land from communal properties- due to a land breaking process (Nadal, 1975).\nAccording to R. Garrabou and J.Sanz, cultivated area increased from 10.5 to 18.8 hectares between 1800 and 1888. Nevertheless, there was no improvement in productivity until the mid-20th century. This was, on one hand, because a large part of the territory was dedicated to the cultivation of cereals that, due to environmental and meteorological conditions, such as that in Spain it does not rain enough, meant an inefficient use of the soil. On the other hand, there were many small farmers who could not invest in technical-productive improvements due to their scarcity of economic resources, while large owners lacked incentives to do so because they acted as suppliers of land and job seekers for which they received large amounts of income in exchange for small costs (wages were low), from which sufficient profits are deducted and they are not interested in maximizing production by increasing productivity.\nAs far as agriculture is concerned, having understood that Catalonia experienced a strong expansion while most of the Spanish provinces remained stagnant, it is convenient to analyse the effects these differences had on the industry development. A fundamental element that is present in every industrialization process is structural change, understood as a transfer of economic activity from the primary to the secondary sector, resulting from an increase in agricultural productivity that allows a massive movement from the countryside to the city. If Spain did not experience structural change until the middle of the 20th century, Catalonia did so at the beginning of the previous century. Furthermore, agricultural progress translates into the creation of a manufacturing market. One of the main problems faced by the industry in Spain was the lack of a market for industrial goods because there was not enough demand, an argument developed and defended by different authors, among which Jordi Nadal (1975) stands out. Agriculture is a provider of food, a basic need for the survival of humans, therefore it is understood that the first thing that a society is going to demand is this. If agriculture remains stagnant, food supply fails to meet all demand, which is responsible for a rise in prices that prevents people from demanding, for example, industrial goods. If to this fact we add the paltry wages of small farmers and day laborers, who were most of the population, there is more evidence to reinforce the population’s lack of purchasing power. In Catalonia the opposite happened. Maluquer (1987) has shown how this region benefited from an agricultural expansion accompanied by an increase in income whose distribution was sufficiently egalitarian and wide to supply the entire population.\nWhat has been explained so far concerns the role played by agriculture. However, we may also consider the relationship between relative prices of production factors . We have descriptive evidence from recent studies (Martínez-Galarraga, Prat, 2016) that affirm that in the cotton sector of the Catalan textile industry relative price of labor and capital factors prompted the mechanization of cotton spinning tasks by adopting the Spinning Jenny, imitating the British model. This happened when most of Spain was still agrarian, in addition to the already mentioned reasons, because the existing relationship between factor prices along with their availability encouraged continued growth based on intensive use of the labor factor. As reported by A. Carreras and X. Tafunell (2021), labor and land were abundant at an affordable price, while capital was scarce and expensive.\nOnce the foundations of the textile industry in Catalonia were established, it grew even more from the second half of the 19th century, a period in which national and international market began to integrate. This process had special repercussions on Catalonia industry. Many studies, analysing the case from the new economic geography models, have shown its first industrialization steps were reinforced by market integration that concentrated and agglomerated the industry in the region. From there arose economies of agglomeration in the productive processes that attracted firms and workers. According to the indications of Jordi Nadal (1975), in 1850 the Catalan spindles represented 6.01 percent of the Spanish set, becoming 18.52 percent in 1861. On the other hand, market integration promoted regional specialization and the interior of Spain specialized in agriculture since that represented their comparative advantage, so this was a disadvantage for the industry. “Summarizing, this period was characterized by a growth in the distance between industrial and agrarian Spain. Castile became an industrial desert. In contrast, Catalonia was Spain’s factory.” (Paluzie, Pons, Tirado, 2002).\nIn this same period another steel industry gained strength in the Basque Country, becoming the most powerful one in southern Europe. This region was privileged by the existence of a quite useful material -non phosphorous iron ore- that favored by a “double freight” system in which the mineral was exported to Great Britain in exchange for the import of Welsh coal, made possible 6 the emergence of an steel industry. In 1902, the Altos Hornos de Vizcaya company was created, “the largest company in Spain in the 20th century”. The Basque Country stands out by the coal’s possession at a cheap price, an energy source that signified a weakness for the Spanish industrialization due to its scarcity and hence expensive adoption.\nIt is worth mentioning the textile and metallurgic industries benefited from the protection of the sector through the establishment of tariff measures. For instance, the 1802 law had a relevant weight in the initial development of Catalonia industry since, by preventing the importation of foreign yarns, it promoted the adoption of more modern machines -from the “bergadana” to the “mule” (Nadal,1975). Regarding the Basque Country, tariff measures also played a fundamental role in protecting industry from mainly two countries: the United Kingdom and Germany, which had a considerable steel production. Unfortunately, protectionism had adverse effects on Spanish agriculture since, among others, it maintained food prices high, eliminating incentives to improve and modernize it.\nOnce we have understood that Catalonia and the Basque Country were the succeeding places to industrialize in the nineteenth century, we must ask ourselves why this made them to achieve greater levels of GDP per capita than the regions that remained agrarian. To this aim, let consider, as economic theory predicts, that the main determinant of GDP per capita growth is labor productivity growth and that this latter depends on efficiency levels and capital deepening. As we know that industrialization is based on an intensive use of capital whereas agrarian societies are labor intensive sectors, and that the use of capital is generally associated with higher efficiency since more output is obtained with lower amounts of inputs, we can deduct that every industrialized region will account with more labor productivity and therefore with a higher GDP per capita. Finally, inequalities among regions were reduced from 1960s following the Williamson´s U-inverted theory. The migratory flow from the most backward to the most prosperous regions that acted as regulator of per capita income and the implementation of public policies made it possible to happen."
  },
  {
    "objectID": "posts/Variables latentes/Tableaux Saumons.html",
    "href": "posts/Variables latentes/Tableaux Saumons.html",
    "title": "Verification of Salmon Authenticity Using the PLS-DA method",
    "section": "",
    "text": "This a project we made on the PLS-DA (Partial Least Squares Discriminant Analysis) technique, a method used to classify data into groups. This approach is particularly helpful when dealing with datasets that have many variables, especially when it is necessary to separate the data into different categories. The technique works by creating latent variables, which are new variables that summarize the most relevant information from the original data. We applied this method in the context of verifying salmon authenticity, as demonstrated below.\n\nIntroduction\nThe objective of our study is to find a predictive model of salmon authenticity on the dataset from the publication “Data fusion and multivariate analysis for food authenticity analysis.” written by Hong, Y., Birse, N., Quinn, B., Li, Y., Jia, W., McCarron, P., … & Elliott, C. T. in 2023. The initial dataset contains information on the 38-element chemical composition of 521 salmon from four different countries. However, elements present in extremely high quantities or too low to be measured accurately were removed from the dataset before further analysis, leaving only 20 chemical elements. Similarly, the values of these elements were normalized on a scale of 0 to 1.\nWe decided to apply the PLS-DA (Partial Least Squares regression Discriminant Analysis) approach. This method consists in classifying individuals according to the categories of the qualitative variable Y (the origin of the salmon in our study), in a discriminating manner by finding the best possible latent variables, these being combinations of the predictors and the qualitative variable. Moreover, this approach seems particularly interesting in this analysis given that some of the chemical elements (explanatory variables) show strong multicollinearity between them, making multinomial regression inviable. To start our study, we perform a preliminary descriptive analysis to better understand the relationships between the variables and the structure of our dataset. Next, the PLS-DA model will be explored by looking for latent components or variables, with cross-validation to assess the performance of the PLS-DA model. To do this, we first set up a training sample, made up of 80% of our population, and a test sample with the remaining 20% to analyze the quality of the prediction. The model will be considered of good quality if it has a low error rate, i.e. when the majority of salmon rankings are validated.\n\n\n1. Descriptive analysis\nAs introduced, descriptive analysis is an essential step prior to any type of work on a database. We begin by projecting a histogram of the dependent qualitative variable “Class”, which indicates the country of origin of each salmon, available in Figure 1. In addition, the origin of these fish is distributed according to the two different production methods.\n             Figure 1: Salmon by country and production method\n\n\n\n\n\nLooking at the distribution of this variable, we can see that farmed salmon are the most represented, with the Scottish and Norwegian countries alone recording almost 300 salmon, accompanied by 90 Irish salmon. Wild-caught salmon, represented by Alaska and Ireland, are in the minority, with overall numbers hovering around 150 salmon, or half the number of the other production method.\nNext, we find it interesting to understand this distribution through the analysis of chemical elements, which will enable us to highlight the particularities of each country. To do this, we decided to represent the distribution of each chemical element with boxplots in Figure 2.\n             Figure 2: Statistical distribution of chemical elements\n\n\n\n\n\nWe observe that a group of variables made up of “Al”, “Cr”, “Cu”, “Fe”, “Co”, “Mn”, “Mo”, “Ni”, “Sr” and “V” show very distant points. After analysis, the salmon from the fishery meet these characteristics. Alaskan salmon, for example, are very rich in copper, molybdenum and nickel. Icelandic salmon are also highly concentrated in iron, strontium and vanadium.\nFinally, analysis of the relationships between predictors using correlation calculations informs us that a number of variables are strongly positively correlated with each other, such as “Zn” and “Se”, “Se” and “Rb” or “Rb” and “Cs”. This strong collinearity reinforces the use of PLS-DA regression in this study.\n\n\n2. PLS-DA Model\nBefore performing the classification, we split the database into a training set, with 80% of all observations, and a test set, with the remaining 20% of observations. First, the PLS-DA model was run on the training set using the MixOmics library. For dimension reduction, we selected the 20 initial components, corresponding to the 20 predictors, which helped us to visualize and identify the most appropriate number of components to minimize the classification error rate while resulting in a parsimonious model. Parsimony suggests that, among models that are equally effective at explaining or predicting data, the one that is simplest or uses the fewest variables is preferred. This translates into the search for models that perform well, but are no more complex than necessary. Model simplicity can help avoid over-fitting, when a model over-fits training data and has difficulty generalizing to new data. Figure 3 shows the classification error rate as a function of the number of components or dimensions. The error rate decreases significantly up to dimension 5, with a value equal to 0.06, then increases slightly and decreases again insignificantly. We would have to enlarge up to component 18 to find an error value smaller than that provided by dimension 5, however, this does not seem appropriate given the relevance of the parsimony criterion.\n          Figure 3: Classification error rate as a function of the number of components\n\n\n\n\n\nOn the other hand, Figure 4 shows the salmon rankings performed on five dimensions. We can see that only individuals 98 and 25, belonging to Alaskan, failed to group correctly. We can also see that the Norway, Scotland and Iceland-F groups do not have a clear separation, which could mislead the prediction.\n            Figure 4: PLS-DA on five dimensions\n\n\n\n\n\n\n\n2. Performance evaluation\nThe test sample was then used to check the model’s performance. The “overall”, “max.dist”, “centroids.dist” and “mahalanobis.dist” metrics are commonly used, with distinct performance measurement methods, to assess the quality of the PLS-DA model in terms of its ability to distinguish different groups. We select the “max.dist” metric as it stands out from the other metrics for its objective of maximizing the separation between classes, which is a key criterion in our study for achieving a clear distinction between them. In addition, in Figure 5 it provides the ideal number of components described above. This distance metric reveals an accuracy rate of 0.95 and an error rate of 0.049.\nSimilarly, a confusion matrix was run on Table 1 to visualize the distribution of salmon between their initial classes and those predicted by the model. Only one salmon was predicted for Norway and one for Scotland while belonging to the Iceland-F class, and three salmon for Scotland while belonging to Norway. These results confirm the model’s ability to distinguish between classes and its accuracy. This is confirmed by Figure 5, which shows the ROC (Recieving Operating Characteristic) curve. Indeed, all the lines corresponding to the different classes provide information on the quality of the specificity and sensitivity indices, which are very good in view of the very high curves, also indicating AUC values close to 1.\n\nTabe 1: Confusion matrix PLS-DA\n\n\n\nAlaskan\nIceland-F\nIceland-W\nNorway\nScotland\n\n\n\n\nAlaskan\n19\n0\n0\n0\n0\n\n\nIceland-F\n0\n9\n0\n1\n1\n\n\nIceland-W\n0\n0\n18\n0\n0\n\n\nNorway\n0\n0\n0\n18\n3\n\n\nScotland\n0\n0\n0\n0\n35\n\n\n\n             Figure 5: ROC Curve\n\n\n\n\n\n\n\n3. The most discriminating and interesting chemical elements to keep \nFinally, we identify the important variables for class prediction by studying their weights on all five components.\nIn Figure 6, we can see that the most discriminating chemical elements, with an error rate of 6%, are, in order of importance, boron, lithium, cadium, carbon monoxide, selenium, iron and caesium. \n             Figure 6: Variables contribution to components\n\n\n\n\n\n\n\nConclusion\nTo conclude, the study consisted in predicting the geographical origin of the salmon in the database using PLS-DA regression. Before applying the prediction model, we carried out a univariate descriptive analysis of our qualitative variable “Class”, which enabled us to observe the distribution of the four countries to be discriminated and to discover which production mode was most represented. We then proceeded with the five-component PLS-DA analysis, built on the training set with the “max.dist” metric. We obtained a high-performance model with good predictive quality, presenting an AUC close to 1 for each group to be discriminated, accompanied by a very low error rate of 4%. The chemical elements that emerged as the most discriminating were lithium, boron, iron, carbon monoxide, selenium, cadmium and cesium. However, class separation was not very clear-cut. Similarly, it would have been interesting to perform a discriminant factor analysis (DFA) to compare and complete our interpretation of the discriminating power of the predictors."
  },
  {
    "objectID": "About the blog.html",
    "href": "About the blog.html",
    "title": "About",
    "section": "",
    "text": "This blog offers a glimpse into my academic journey. Feel free to explore my projects, complete with their virtues and imperfections, which reflect a process of deep engagement with my own learning."
  },
  {
    "objectID": "about me.html#bio",
    "href": "about me.html#bio",
    "title": "Yava Vilar Valera",
    "section": "",
    "text": "Born in Valencia (Spain), I grew up in Navaluenga, a small village in the province of Ávila, Spain. I later moved to Belgium where I lived for three years and learnt French. Then, I returned to Valencia, where, in 2020, I began my degree in Economics at the Universitat de València. Two years later I enrolled in an International Double Degree program with Nantes Université, which allowed me to obtain my Bachelor in Economics from both universities in 2024.\nCurrently, I am pursuing a Master’s program in Econometrics and Statistics at Nantes University, where I have acquired extensive skills in software and data management, in addition to enhancing my statistical abilities to analyze economic issues."
  },
  {
    "objectID": "about me.html#interests",
    "href": "about me.html#interests",
    "title": "Yava Vilar Valera",
    "section": "",
    "text": "I define myself as a very passionate and motivated student, with a strong willingness to learn and achieve new goals. Not only do I like addressing questions of social and economic nature, but also writing and engaging in thorough and creative work. My academic and international experiences have shaped my global perspective and sparked a deep curiosity about the world. This drives my interest in working on global challenges, particularly those related to economic development.\nSided to my study life, one of my highest passions in life is music. I love listening to it, dancing and going to concerts. It only brings positive things to my life. Nature -mountains, beaches, landscapes- also fascinate me."
  },
  {
    "objectID": "about me.html#brief-bio",
    "href": "about me.html#brief-bio",
    "title": "Yava Vilar Valera",
    "section": "",
    "text": "Born in Valencia (Spain), I grew up in Navaluenga, a small village in the province of Ávila, Spain. Located in a valley, this has profoundly impacted my fascination for nature, mountains, and landscapes. I later moved to Belgium where I lived for three years and learned French. Then, I returned to Valencia, where, in 2020, I began my degree in Economics at the Universitat de València. Two years later I enrolled in an International Double Degree program with Nantes Université. This unique opportunity has allowed me to obtain my Bachelor in Economics from both universities in 2024.\nCurrently, I am pursuing a Master’s program in Econometrics and Statistics at Nantes University, where I have acquired extensive skills in software and data management, in addition to enhancing my statistical abilities to analyze economic issues.\nI define myself as a very passionate and motivated student, with a strong willingness to learn and achieve new goals. Not only do I like addressing questions of social and economic nature, but also writing and engaging in thorough and creative work. My academic background and experiences living in different countries have shaped my global perspective and sparked a deep curiosity about the world. This drives my interest in working on global challenges."
  },
  {
    "objectID": "about me.html#about-me---bio-interests",
    "href": "about me.html#about-me---bio-interests",
    "title": "About",
    "section": "",
    "text": "Born in Valencia, Spain, I grew up in Navaluenga, a small village in the province of Ávila, Spain. I later moved to Belgium where I lived for three years and learned French. In 2020, I began my degree in Economics at the Universitat de València. Two years later I enrolled in an International Double Degree program with Nantes Université. This unique opportunity allowed me to obtain my Bachelor in Economics from both universities in 2024.\nCurrently, I am pursuing a Master’s degree in Econometrics and Statistics at Nantes University, where I have acquired extensive skills in software and data management, in addition to enhancing my statistical abilities to analyze economic issues.\nI define myself as a very passionate and motivated student, with a strong willingness to learn and achieve new goals. Not only do I like addressing questions of social and economic nature, but also writing and engaging in thorough and creative work. My academic and international experiences have shaped my global perspective and sparked a deep curiosity about the world. This drives my interest in working on global challenges.\nOutside of academia, I dedicate part of my free time to music and dancing, which are among my greatest passions.\nYou can find a version of my CV here."
  },
  {
    "objectID": "R/TD3/TD3.html",
    "href": "R/TD3/TD3.html",
    "title": "Analyzing Olympic Games : Reading large datasets in R",
    "section": "",
    "text": "These exercises show how to import files in formats that can handle large databases. Similarly, the dplyr, tidyr, string and lubridate libraries will be used to analyze information relating to the historic Olympic Games.\ninstall.packages(\"RSQLite\")\ndata_parquet &lt;- arrow::read_parquet(\"C:/Users/yavav/OneDrive - Universitat de València/tokyo_athletes.parquet\")\ndata_parquet &lt;- as.data.frame(data_parquet)\n\nView(data_parquet)\ndim(data_parquet) #Trois variables et 11085 colonnes\n\n[1] 11085     3\n\nstr(data_parquet) #Colonnes présentes: Name, NOC, Discipline\n\n'data.frame':   11085 obs. of  3 variables:\n $ Name      : chr  \"AALERUD Katrine\" \"ABAD Nestor\" \"ABAGNALE Giovanni\" \"ABALDE Alberto\" ...\n $ NOC       : chr  \"Norway\" \"Spain\" \"Italy\" \"Spain\" ...\n $ Discipline: chr  \"Cycling Road\" \"Artistic Gymnastics\" \"Rowing\" \"Basketball\" ...\nchemin_acces &lt;- \"C:/Users/yavav/OneDrive - Universitat de València/olympics_athletes.json\"\n\nlibrary(jsonlite)\ndatos_json &lt;- fromJSON(chemin_acces)\ndatos_json &lt;- as.data.frame(datos_json)\ndim(datos_json) #269731 lignes et 13 colonnes\n\n[1] 269731     13\n\nstr(datos_json) #On observe le nom des différentes colonnes\n\n'data.frame':   269731 obs. of  13 variables:\n $ ID    : int  1 2 3 4 5 5 5 5 5 5 ...\n $ Name  : chr  \"A Dijiang\" \"A Lamusi\" \"Gunnar Nielsen Aaby\" \"Edgar Lindenau Aabye\" ...\n $ Sex   : chr  \"M\" \"M\" \"M\" \"M\" ...\n $ Age   : int  24 23 24 34 21 21 25 25 27 27 ...\n $ Height: int  180 170 NA NA 185 185 185 185 185 185 ...\n $ Weight: num  80 60 NA NA 82 82 82 82 82 82 ...\n $ Team  : chr  \"China\" \"China\" \"Denmark\" \"Denmark/Sweden\" ...\n $ NOC   : chr  \"CHN\" \"CHN\" \"DEN\" \"DEN\" ...\n $ Games : chr  \"1992 Summer\" \"2012 Summer\" \"1920 Summer\" \"1900 Summer\" ...\n $ City  : chr  \"Barcelona\" \"London\" \"Antwerpen\" \"Paris\" ...\n $ Sport : chr  \"Basketball\" \"Judo\" \"Football\" \"Tug-Of-War\" ...\n $ Event : chr  \"Basketball Men's Basketball\" \"Judo Men's Extra-Lightweight\" \"Football Men's Football\" \"Tug-Of-War Men's Tug-Of-War\" ...\n $ Medal : chr  NA NA NA \"Gold\" ...\nlibrary(RSQLite)\n\ncon &lt;- dbConnect(SQLite(), dbname =\"C:/Users/yavav/Documents/R avancé et Git/data.sqlite\")\ntables &lt;- dbListTables(con)\nprint(tables) ##Tables: Tokyo_medals et Tokyo_teams\n\n[1] \"C:/Users/yavav/OneDrive - Universitat de València/__MACOSX/olympics_athletes.json\"\n[2] \"olympics_athletes\"                                                                \n[3] \"tokyo_athletes\"                                                                   \n[4] \"tokyo_medals\"                                                                     \n[5] \"tokyo_teams\"\nlibrary(RSQLite)\nlibrary(DBI)\ndbWriteTable(con, \"tokyo_athletes\", data_parquet, overwrite=TRUE)\ndbWriteTable(con, \"olympics_athletes\", datos_json, overwrite=TRUE)\ncolonnes_olympics_athletes &lt;- dbListFields(con, \"olympics_athletes\")\n\n# Afficher la liste des colonnes\nprint(colonnes_olympics_athletes)\n\n [1] \"ID\"     \"Name\"   \"Sex\"    \"Age\"    \"Height\" \"Weight\" \"Team\"   \"NOC\"   \n [9] \"Games\"  \"City\"   \"Sport\"  \"Event\"  \"Medal\" \n\ncolonnes_tokyo_athletes &lt;- dbListFields(con, \"tokyo_athletes\")\n\n# Afficher la liste des colonnes\nprint(colonnes_tokyo_athletes)\n\n[1] \"Name\"       \"NOC\"        \"Discipline\"\n##Olympics\n\nlibrary(tibble)\n\n#Lire la table depuis la base de données\ntable_olympics_athletes &lt;- dbReadTable(con, \"olympics_athletes\")\n\n# Convertir la table en tibble\ntibble_olympics_athletes &lt;- as_tibble(table_olympics_athletes)\n\n##Athletes\n\n#Lire la table depuis la base de données\ntable_tokyo_athletes &lt;- dbReadTable(con, \"tokyo_athletes\")\n\n# Convertir la table en tibble\ntibble_tokyo_athletes &lt;- as_tibble(table_tokyo_athletes)"
  },
  {
    "objectID": "R/TD3/TD3.html#exercises-semaine-3",
    "href": "R/TD3/TD3.html#exercises-semaine-3",
    "title": "Exercices Semaine 3",
    "section": "",
    "text": "Ce tutorial se concentre sur l’apprentissage d’importation des fichiers avec des formats permettant de gérer de grandes bases de données. De même, les librairies dplyr, tidyr, string et lubridate seront exploitées pour analyser les informations relatives aux Jeux Olympiques historiques.\n\ninstall.packages(\"RSQLite\")\n\n\nLire le fichier parquet avec la librairie arrow, dimension et nom des colonnes\n\n\ndata_parquet &lt;- arrow::read_parquet(\"C:/Users/yavav/OneDrive - Universitat de València/tokyo_athletes.parquet\")\ndata_parquet &lt;- as.data.frame(data_parquet)\n\nView(data_parquet)\n\n\ndim(data_parquet) #Trois variables et 11085 colonnes\n\n[1] 11085     3\n\nstr(data_parquet) #Colonnes présentes: Name, NOC, Discipline\n\n'data.frame':   11085 obs. of  3 variables:\n $ Name      : chr  \"AALERUD Katrine\" \"ABAD Nestor\" \"ABAGNALE Giovanni\" \"ABALDE Alberto\" ...\n $ NOC       : chr  \"Norway\" \"Spain\" \"Italy\" \"Spain\" ...\n $ Discipline: chr  \"Cycling Road\" \"Artistic Gymnastics\" \"Rowing\" \"Basketball\" ...\n\n\n\nLire le fichier json avec la libraire jsonlite, dimension et nom des colonnes\n\n\nchemin_acces &lt;- \"C:/Users/yavav/OneDrive - Universitat de València/olympics_athletes.json\"\n\nlibrary(jsonlite)\ndatos_json &lt;- fromJSON(chemin_acces)\ndatos_json &lt;- as.data.frame(datos_json)\n\n\ndim(datos_json) #269731 lignes et 13 colonnes\n\n[1] 269731     13\n\nstr(datos_json) #On observe le nom des différentes colonnes\n\n'data.frame':   269731 obs. of  13 variables:\n $ ID    : int  1 2 3 4 5 5 5 5 5 5 ...\n $ Name  : chr  \"A Dijiang\" \"A Lamusi\" \"Gunnar Nielsen Aaby\" \"Edgar Lindenau Aabye\" ...\n $ Sex   : chr  \"M\" \"M\" \"M\" \"M\" ...\n $ Age   : int  24 23 24 34 21 21 25 25 27 27 ...\n $ Height: int  180 170 NA NA 185 185 185 185 185 185 ...\n $ Weight: num  80 60 NA NA 82 82 82 82 82 82 ...\n $ Team  : chr  \"China\" \"China\" \"Denmark\" \"Denmark/Sweden\" ...\n $ NOC   : chr  \"CHN\" \"CHN\" \"DEN\" \"DEN\" ...\n $ Games : chr  \"1992 Summer\" \"2012 Summer\" \"1920 Summer\" \"1900 Summer\" ...\n $ City  : chr  \"Barcelona\" \"London\" \"Antwerpen\" \"Paris\" ...\n $ Sport : chr  \"Basketball\" \"Judo\" \"Football\" \"Tug-Of-War\" ...\n $ Event : chr  \"Basketball Men's Basketball\" \"Judo Men's Extra-Lightweight\" \"Football Men's Football\" \"Tug-Of-War Men's Tug-Of-War\" ...\n $ Medal : chr  NA NA NA \"Gold\" ...\n\n\n\nConnexion à la base de données SQL site avec dbConnect\n\n\nlibrary(RSQLite)\n\ncon &lt;- dbConnect(SQLite(), dbname =\"C:/Users/yavav/Documents/R avancé et Git/data.sqlite\")\n\n\nCréer deux nouvelles tables avec dbListTables\n\n\ntables &lt;- dbListTables(con)\nprint(tables) ##Tables: Tokyo_medals et Tokyo_teams\n\n[1] \"C:/Users/yavav/OneDrive - Universitat de València/__MACOSX/olympics_athletes.json\"\n[2] \"olympics_athletes\"                                                                \n[3] \"tokyo_athletes\"                                                                   \n[4] \"tokyo_medals\"                                                                     \n[5] \"tokyo_teams\"                                                                      \n\n\n\nlibrary(RSQLite)\nlibrary(DBI)\ndbWriteTable(con, \"tokyo_athletes\", data_parquet, overwrite=TRUE)\ndbWriteTable(con, \"olympics_athletes\", datos_json, overwrite=TRUE)\n\n\nInspecter la table olympics_athletes en utilisant la fonction dbListFields\n\n\ncolonnes_olympics_athletes &lt;- dbListFields(con, \"olympics_athletes\")\n\n# Afficher la liste des colonnes\nprint(colonnes_olympics_athletes)\n\n [1] \"ID\"     \"Name\"   \"Sex\"    \"Age\"    \"Height\" \"Weight\" \"Team\"   \"NOC\"   \n [9] \"Games\"  \"City\"   \"Sport\"  \"Event\"  \"Medal\" \n\ncolonnes_tokyo_athletes &lt;- dbListFields(con, \"tokyo_athletes\")\n\n# Afficher la liste des colonnes\nprint(colonnes_tokyo_athletes)\n\n[1] \"Name\"       \"NOC\"        \"Discipline\"\n\n\n\nImporter cette table depuis la base de données en utilisant la fonction dbReadTable. Convertir la table en tibble.\n\n\n##Olympics\n\nlibrary(tibble)\n\n#Lire la table depuis la base de données\ntable_olympics_athletes &lt;- dbReadTable(con, \"olympics_athletes\")\n\n# Convertir la table en tibble\ntibble_olympics_athletes &lt;- as_tibble(table_olympics_athletes)\n\n##Athletes\n\n#Lire la table depuis la base de données\ntable_tokyo_athletes &lt;- dbReadTable(con, \"tokyo_athletes\")\n\n# Convertir la table en tibble\ntibble_tokyo_athletes &lt;- as_tibble(table_tokyo_athletes)"
  },
  {
    "objectID": "R/TD3/TD3.html#libraires-dplyr-tidyr",
    "href": "R/TD3/TD3.html#libraires-dplyr-tidyr",
    "title": "Exercices Semaine 3",
    "section": "Libraires dplyr, tidyr",
    "text": "Libraires dplyr, tidyr\n\nConvertir la colonne Sex en variable catégorielle avec la fonction mutate\n\n\n#Importation des libraires \nlibrary(dplyr)\nlibrary(tidyr)\n\n\ntibble_olympics_athletes &lt;- tibble_olympics_athletes %&gt;% mutate(Sex = as.factor(Sex))\n\n\nCréer deux colonnes à partir de la colonne Games. La première colonne Year sera une colonne de type integer contenant l’année des jeux. La deuxième colonne isSummer sera une colonne booléenne qui indiquera si les jeux sont des jeux d’été ou d’hiver.\n\n\nhelp(\"separate_wider_delim\")\n\nseparate_wider_delim(tibble_olympics_athletes, cols = Games, delim= \" \", names=c(\"Year\", \"isSummer\"), cols_remove = FALSE)\n\n# A tibble: 269,731 × 15\n      ID Name   Sex     Age Height Weight Team  NOC   Year  isSummer Games City \n   &lt;int&gt; &lt;chr&gt;  &lt;fct&gt; &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;\n 1     1 A Dij… M        24    180     80 China CHN   1992  Summer   1992… Barc…\n 2     2 A Lam… M        23    170     60 China CHN   2012  Summer   2012… Lond…\n 3     3 Gunna… M        24     NA     NA Denm… DEN   1920  Summer   1920… Antw…\n 4     4 Edgar… M        34     NA     NA Denm… DEN   1900  Summer   1900… Paris\n 5     5 Chris… F        21    185     82 Neth… NED   1988  Winter   1988… Calg…\n 6     5 Chris… F        21    185     82 Neth… NED   1988  Winter   1988… Calg…\n 7     5 Chris… F        25    185     82 Neth… NED   1992  Winter   1992… Albe…\n 8     5 Chris… F        25    185     82 Neth… NED   1992  Winter   1992… Albe…\n 9     5 Chris… F        27    185     82 Neth… NED   1994  Winter   1994… Lill…\n10     5 Chris… F        27    185     82 Neth… NED   1994  Winter   1994… Lill…\n# ℹ 269,721 more rows\n# ℹ 3 more variables: Sport &lt;chr&gt;, Event &lt;chr&gt;, Medal &lt;chr&gt;\n\n\n\nMoyenne d’age des athletes pour chacune des éditions des JO d’été.\n\n\nany(is.na(tibble_olympics_athletes$Age)) #Ce code permet de vérifier la présence des NA's au sein de la colonne Age\n\n[1] TRUE\n\ntibble_olympics_athletes %&gt;% group_by(Games) %&gt;%\n  summarise(mean_age=mean(Age,na.rm=TRUE)) #Ce code calcule la moyenne d'age pour chacune des éditions des JO, en excluant les valeurs manquantes du calcul. \n\n# A tibble: 51 × 2\n   Games       mean_age\n   &lt;chr&gt;          &lt;dbl&gt;\n 1 1896 Summer     23.6\n 2 1900 Summer     28.9\n 3 1904 Summer     26.7\n 4 1906 Summer     27.1\n 5 1908 Summer     27.0\n 6 1912 Summer     27.5\n 7 1920 Summer     29.3\n 8 1924 Summer     28.1\n 9 1924 Winter     27.6\n10 1928 Summer     28.0\n# ℹ 41 more rows\n\n\n\nEdition qui a compté avec les athlètes les plus jeunes ? Les plus vieux ?\n\n\ntibble_olympics_athletes %&gt;% group_by(Games) %&gt;%\n  summarise(mean_age=mean(Age,na.rm=TRUE)) %&gt;% \n  arrange(mean_age) %&gt;% slice(1) #Cela permet d'ordonner de manière croissante les moyennes d'âge ainsi que de sélectionner la première ligne (les plus jeunes)\n\n# A tibble: 1 × 2\n  Games       mean_age\n  &lt;chr&gt;          &lt;dbl&gt;\n1 1984 Winter     23.4\n\ntibble_olympics_athletes %&gt;% group_by(Games) %&gt;%\n  summarise(mean_age=mean(Age,na.rm=TRUE)) %&gt;% \n  arrange(desc(mean_age)) %&gt;% slice(1) #Cela ordonne de manière décroissante pour trouver les plus âgés \n\n# A tibble: 1 × 2\n  Games       mean_age\n  &lt;chr&gt;          &lt;dbl&gt;\n1 1932 Summer     30.2\n\n\nOn observe que l’édition qui a compté avec les athlètes les plus jeunes ont été les jeux d’hiver 1984, tandis que celle avec les plus vieux, les jeux d’été en 1932.\n\nDiscipline des JO d’été dont la taille des athlètes féminines est la plus grande ?\n\n\ntibble_olympics_athletes %&gt;% filter(Sex==\"F\") %&gt;% arrange(desc(Height)) %&gt;% slice(1) %&gt;% select(Height, Sport) #On filtre les femmes, on ordonne et sélectionne la ligne ainsi que les variables d'intérêt. \n\n# A tibble: 1 × 2\n  Height Sport     \n   &lt;int&gt; &lt;chr&gt;     \n1    213 Basketball\n\n\nLa discipline dont la taille des athlètes féminines est la plus grande est le basketball.\n\nQuelle a été cette discipline au cours de chacune des éditions ?\n\n\ntibble_olympics_athletes %&gt;% filter(Sex==\"F\" & !is.na(Height)) %&gt;% group_by(Games) %&gt;% \n  arrange(desc(Height)) %&gt;% slice(1) %&gt;% select(Games, Height, Sport) #On filtre pour les femmes et pour la taille sans valeurs manquantes, puis on calcule la taille plus grande pour chaque game et on sélectionne la colonne du sport correspondant. \n\n# A tibble: 45 × 3\n# Groups:   Games [45]\n   Games       Height Sport         \n   &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;         \n 1 1920 Summer    175 Diving        \n 2 1924 Summer    175 Fencing       \n 3 1924 Winter    165 Figure Skating\n 4 1928 Summer    183 Fencing       \n 5 1928 Winter    165 Figure Skating\n 6 1932 Summer    183 Fencing       \n 7 1932 Winter    168 Figure Skating\n 8 1936 Summer    183 Fencing       \n 9 1936 Winter    168 Figure Skating\n10 1948 Summer    180 Athletics     \n# ℹ 35 more rows\n\n\n\nNombre de fois où chaque discipline a été la discipline avec les plus grandes athlètes\n\n\ntibble_olympics_athletes %&gt;% filter(Sex==\"F\" & !is.na(Height)) %&gt;% group_by(Games) %&gt;% \n  arrange(desc(Height)) %&gt;% slice(1) %&gt;% select(Games, Height, Sport) %&gt;% pull(Sport) %&gt;% table() #On extrait la colonne sport et fait un tablau de fréquence. \n\n.\n       Alpine Skiing            Athletics           Basketball \n                   3                    5                    9 \n           Bobsleigh Cross Country Skiing               Diving \n                   3                    4                    1 \n             Fencing       Figure Skating           Ice Hockey \n                   4                    4                    1 \n                Luge        Speed Skating             Swimming \n                   3                    4                    1 \n          Volleyball \n                   3"
  },
  {
    "objectID": "R/TD3/TD3.html#libraries-stringr-lubridate",
    "href": "R/TD3/TD3.html#libraries-stringr-lubridate",
    "title": "Exercices Semaine 3",
    "section": "Libraries stringr, lubridate",
    "text": "Libraries stringr, lubridate\nOn dispose du texte suivant :\n\ntexte &lt;- \"Les jeux olympiques d’été se déroulent normalement tous les 4 ans, durant les mois de Juillet et Août. Les jeux de Rio ont eu lieu du 5 Août 2016 au 20 Août 2016, ceux de Tokyo du 23 Juillet 2021 au 8 Août 2021, et ceux de Paris auront lieu du 26 Juillet 2024 au 11 Août 2024. Plus de 10000 athlètes sont attendus du monde\nentier à Paris.\"\n\n\nExtraire les dates des différentes éditions des JO\n\n\n#Chargement de librairie \nlibrary(stringr)\n\n\nstr_extract_all(texte, \"\\\\b(\\\\d{1,2} \\\\w+ \\\\d{4})\\\\b\")\n\n[[1]]\n[1] \"5 Août 2016\"     \"20 Août 2016\"    \"23 Juillet 2021\" \"8 Août 2021\"    \n[5] \"26 Juillet 2024\" \"11 Août 2024\"   \n\n\n\nRemplacer les noms des mois par leur numéro pour rendre ces éléments convertibles en date en utilisant la fonction str_replace\n\n\nstr_replace_all(texte, c(\"Juillet\"=\"7\", \"Août\"=\"8\"))\n\n[1] \"Les jeux olympiques d’été se déroulent normalement tous les 4 ans, durant les mois de 7 et 8. Les jeux de Rio ont eu lieu du 5 8 2016 au 20 8 2016, ceux de Tokyo du 23 7 2021 au 8 8 2021, et ceux de Paris auront lieu du 26 7 2024 au 11 8 2024. Plus de 10000 athlètes sont attendus du monde\\nentier à Paris.\"\n\n\n\nlibrary(lubridate)\n\n\npatron_dates &lt;- \"\\\\b(\\\\d{1,2} \\\\w+ \\\\d{4})\\\\b\"\n\n# Extraire et convertir toutes les dates\ndates &lt;- str_extract_all(texte, patron_dates) %&gt;%\n  lapply(function(matches) {\n    dmy(matches)\n  })\n\n#Imprimer le résultat\nprint(dates)\n\n[[1]]\n[1] \"2016-08-05\" \"2016-08-20\" \"2021-07-23\" \"2021-08-08\" \"2024-07-26\"\n[6] \"2024-08-11\"\n\n\n\nCombien de jours ont séparés les éditions de Rio et Tokyo ? Et sépareront les éditions de Tokyo et de Paris ? Faites le même calcul en semaines.\n\n\n##Différence de jours entre les éditions Rio et Tokyo \n\nRio_Tokyo &lt;- difftime(dates[[1]][3], dates[[1]][2], units = \"days\") #Diff entre le troisième et deuxième élément de la liste dates en jour\nprint(Rio_Tokyo)\n\nTime difference of 1798 days\n\n##Différence de jours entre les éditions Tokyo et Paris\n\nTokyo_Paris &lt;- difftime(dates[[1]][5], dates[[1]][4], units = \"days\") #Diff entre le cinquième et sixième élément de la liste dates en jour\nprint(Tokyo_Paris)\n\nTime difference of 1083 days\n\n##Différence de semaines entre les éditions Rio et Tokyo\n\nRio_Tokyo_semaines &lt;- difftime(dates[[1]][3], dates[[1]][2], units = \"weeks\") #Diff entre le troisième et deuxième élément de la liste dates en semaines\nprint(Rio_Tokyo_semaines)\n\nTime difference of 256.8571 weeks\n\n##Différence de semaines entre les éditions Tokyo et Paris\n\nTokyo_Paris_semaines &lt;- difftime(dates[[1]][5], dates[[1]][4], units = \"weeks\") #Diff entre le cinquième et sixième élément de la liste dates en semaines\nprint(Tokyo_Paris_semaines)\n\nTime difference of 154.7143 weeks"
  },
  {
    "objectID": "R/TD1/TD1.html",
    "href": "R/TD1/TD1.html",
    "title": "Analyzing Olympic Games : Basic code in R",
    "section": "",
    "text": "This series of exercises in R focuses on analyzing data relating to the Olympic infrastructure for the 2024 Games in Paris. We’ll be using R’s basic functions exclusively to explore, analyze and extract key information about Olympic and Paralympic venues.\n\nRead file with database, name and type of columns, number of rows\n\n\n#Lire et nommer le fichier \ndata_ex &lt;- read.csv(\"C:/Users/yavav/Downloads/paris-2024-sites-olympiques-et-paralympiques-franciliens (1).csv\", sep=\";\", dec=\",\", quote=\"\\\"\", na.strings = NA)\n\n#Noms et nature des colonnes \nstr(data_ex)\n\n'data.frame':   31 obs. of  4 variables:\n $ geo_point                     : chr  \"48.841319, 2.253076\" \"48.924388, 2.359871\" \"48.92467, 2.332428\" \"48.815115, 2.08208\" ...\n $ nom                           : chr  \"Parc des Princes\" \"Stade de France\" \"Village olympique\" \"Château de Versailles\" ...\n $ sites_olympiques_paralympiques: chr  \"Site olympique\" \"Site olympique,Site paralympique\" \"\" \"Site olympique,Site paralympique\" ...\n $ sports                        : chr  \"Football\" \"Athlétisme,Para athlétisme,Rugby\" \"\" \"Sports équestres,Para équitation,Pentathlon moderne\" ...\n\n#Nombre de lignes \nnrow(data_ex) \n\n[1] 31\n\n\nThe dataframe contains 4 columns: geo_point, olympic village, olympic_venues and sports, coded in chr (character), and31 columns.\n\nHow many Olympic venues are there?\n\n\nx &lt;- 0\n\nfor (value in data_ex$sites_olympiques_paralympiques) {\n  if (value == \"Site olympique\" | value == \"Site olympique,Site paralympique\") {\n    x &lt;- x + 1\n  } } \n\nprint(paste(\"Número total de Sites olympiques:\", x))\n\n[1] \"Número total de Sites olympiques: 26\"\n\n\n\nHow many Paralympic venues are there?\n\n\ny &lt;- 0\n\nfor (value in data_ex$sites_olympiques_paralympiques) {\n  if (value == \"Site paralympique\" | value == \"Site olympique,Site paralympique\") {\n    y &lt;- y + 1\n  } } \n\nprint(paste(\"Número total de Sites paralympiques:\", y))\n\n[1] \"Número total de Sites paralympiques: 19\"\n\n\n\nWhich sites host several sporting disciplines?\n\n\n#On cherche  les cellules de la colonne sports qui continnent des éléments séparés par un virgule, indice d'acceuil de plus d'un sport\n\nsites_plusieurs_sports &lt;- data_ex[grepl(\",\", data_ex$sports), c(\"nom\")]\nsites_plusieurs_sports\n\n [1] \"Stade de France\"                                \n [2] \"Château de Versailles\"                          \n [3] \"Grand Palais\"                                   \n [4] \"Invalides\"                                      \n [5] \"Pont d'Iéna\"                                    \n [6] \"Arena Bercy\"                                    \n [7] \"Arena Paris nord\"                               \n [8] \"Arena Paris Sud 6 (Porte de Versailles)\"        \n [9] \"Stade de la Concorde\"                           \n[10] \"Arena Champs de Mars\"                           \n[11] \"Stade Tour Eiffel\"                              \n[12] \"Arena La Chapelle\"                              \n[13] \"Centre aquatique\"                               \n[14] \"Arena Paris Sud 1 (Porte de Versailles)\"        \n[15] \"Arena Paris Sud 4 (Porte de Versailles)\"        \n[16] \"La Défense Arena\"                               \n[17] \"Stade nautique\"                                 \n[18] \"Vélodrome National de Saint-Quentin-en-Yvelines\"\n[19] \"Stade Roland Garros\"                            \n\n\n\nWhat para-Olympic disciplines are hosted at these venues in the Paris region?\n\n\ndisciplines_plusieurs_sports &lt;- data_ex[grepl(\",\", data_ex$sports), c(\"nom\", \"sports\")] \ndisciplines_plusieurs_sports\n\n                                               nom\n2                                  Stade de France\n4                            Château de Versailles\n8                                     Grand Palais\n9                                        Invalides\n11                                     Pont d'Iéna\n12                                     Arena Bercy\n13                                Arena Paris nord\n14         Arena Paris Sud 6 (Porte de Versailles)\n15                            Stade de la Concorde\n17                            Arena Champs de Mars\n18                               Stade Tour Eiffel\n20                               Arena La Chapelle\n22                                Centre aquatique\n23         Arena Paris Sud 1 (Porte de Versailles)\n24         Arena Paris Sud 4 (Porte de Versailles)\n26                                La Défense Arena\n27                                  Stade nautique\n29 Vélodrome National de Saint-Quentin-en-Yvelines\n30                             Stade Roland Garros\n                                                             sports\n2                                  Athlétisme,Para athlétisme,Rugby\n4               Sports équestres,Para équitation,Pentathlon moderne\n8                 Escrime,Escrime fauteuil,Taekwondo,Para taekwondo\n9        Tir à l'arc,Para tir à l'arc,Athlétisme,Cyclisme sur route\n11                                    Cyclisme sur route,Athlétisme\n12     Basketball,Basket fauteuil,Gymnastique artistique,trampoline\n13                Boxe,Escrime,Pentathlon moderne,Volley-ball assis\n14                                  Haltérophilie,Handball,Goalball\n15                 Basketball 3x3,BMX freestyle,Breaking,Skateboard\n17                              Judo,Para judo,Lutte,Rugby fauteuil\n18                                     Volleyball de plage,Cécifoot\n20 Badminton,Para Badminton,Gymnastique rythmique,Para powerlifting\n22                          Natation artistique,Plongeon,Water-polo\n23                                                Volleyball,Boccia\n24                             Tennis de table,Para tennis de table\n26                                Natation,Para natation,Water-polo\n27                              Canoë,Para canoë,Aviron,Para aviron\n29                       Cyclisme sur piste,Para cyclisme sur piste\n30                                      Tennis,Tennis fauteuil,Boxe\n\n\nThe resulting dataframe displays the name of the site with its associated disciplines.\n\nSites with the most followers\n\n\nmax_sports &lt;- max(sapply(strsplit(data_ex$sports, \",\"),length))\n\nfiles_max_sports &lt;- which(sapply(strsplit(data_ex$sports, \",\"),length)==max_sports)\n\ndata_max_sports &lt;- data_ex[files_max_sports, c(\"nom\", \"sports\")]\n\ndata_max_sports\n\n                    nom\n8          Grand Palais\n9             Invalides\n12          Arena Bercy\n13     Arena Paris nord\n15 Stade de la Concorde\n17 Arena Champs de Mars\n20    Arena La Chapelle\n27       Stade nautique\n                                                             sports\n8                 Escrime,Escrime fauteuil,Taekwondo,Para taekwondo\n9        Tir à l'arc,Para tir à l'arc,Athlétisme,Cyclisme sur route\n12     Basketball,Basket fauteuil,Gymnastique artistique,trampoline\n13                Boxe,Escrime,Pentathlon moderne,Volley-ball assis\n15                 Basketball 3x3,BMX freestyle,Breaking,Skateboard\n17                              Judo,Para judo,Lutte,Rugby fauteuil\n20 Badminton,Para Badminton,Gymnastique rythmique,Para powerlifting\n27                              Canoë,Para canoë,Aviron,Para aviron\n\n\nWe can see that there are a total of 8 venues hosting a total of 4 sports each.\n\nWhich discipline will take place at the greatest number of venues? What are these venues?\n\n\n# Conter la fréquence de chaque discipline\nfrequence_disciplines &lt;- table(unlist(strsplit(data_ex$sports, \",\")))\n\n# Discipline avec plus grande fréquence\ndiscipline_plus_frequente &lt;- names(frequence_disciplines)[which.max(frequence_disciplines)]\n\n# Résultat\ncat(\"La discipline qui aura lieu sur le plus grand nombre de sites est:\", discipline_plus_frequente, \"\\n\")\n\nLa discipline qui aura lieu sur le plus grand nombre de sites est: Athlétisme \n\n\n\n# Filtrer les sites qui incluent le mot \"Athlétisme\" dans la colonne Discipline\nsites_atletisme &lt;- data_ex[grepl(\"Athlétisme\", data_ex$sports, ignore.case = TRUE), c(\"nom\", \"sports\")]\n\n# Imprimer les résultats\ncat(\"Les sites où l'athlétisme aura lieu sont les suivants:\\n\")\n\nLes sites où l'athlétisme aura lieu sont les suivants:\n\nprint(sites_atletisme)\n\n                                   nom\n2                      Stade de France\n6  Terrain des Essences - La Courneuve\n9                            Invalides\n11                         Pont d'Iéna\n25             Hôtel de ville de Paris\n                                                       sports\n2                            Athlétisme,Para athlétisme,Rugby\n6                                             Para athlétisme\n9  Tir à l'arc,Para tir à l'arc,Athlétisme,Cyclisme sur route\n11                              Cyclisme sur route,Athlétisme\n25                                                 Athlétisme\n\n\nAthletics will be held at 5 venues in Paris.\n\nWhich are the two nearest venues?\n\n\n#Libraires \n\nlibrary(geosphere)\nlibrary(tidyr)\n\n#Séparer les coordonnées géographiques en deux colonnes par la virgule \n\ndata_ex &lt;- separate(data_ex, geo_point, into = c(\"lat\", \"long\"), sep = \", \")\n\n# Fonction pour calculer la distance euclidienne\n\ndata_ex$lat &lt;- as.numeric(data_ex$lat)\ndata_ex$long &lt;- as.numeric(data_ex$long)\n\ndistances &lt;- distHaversine(data_ex[, c(\"long\", \"lat\")])\nprint(distances)\n\n [1] 12109.0289  2007.5022 22017.3512 28908.6007  1204.2823 36658.5531\n [7] 26702.8086   422.2544 11450.5703 13052.7520  6674.7313 18001.9809\n[13] 22981.5082  4582.2876  8949.4822  9269.0396   436.5820 20677.9533\n[19] 27022.4177 10476.8609 12176.4034 11437.7799   419.6006  5441.5332\n[25]  9902.1865 30003.3283 42913.1029  5092.1872 16782.0752 23146.9519\n\n# Matrice pour garder les distances\ndistances &lt;- matrix(NA, nrow = nrow(data_ex), ncol = nrow(data_ex))\n\n# Calculer les distances entre toutes les coordonnées avec un boucle \nfor (i in 1:(nrow(data_ex) - 1)) {\n  for (j in (i + 1):nrow(data_ex)) {\n    distances[i, j] &lt;- distHaversine(\n      c(data_ex$lat[i], data_ex$long[i]),\n      c(data_ex$lat[j], data_ex$long[j])\n    )\n    distances[j, i] &lt;- distances[i, j]  # Symétrie\n  }\n}\n\n# Convertir la matirce en dataframe\ndistances_df &lt;- as.data.frame(distances)\n\n#Le sites les plus proches\nmin_distance &lt;- min(distances_df, na.rm=TRUE)\nmin_distance \n\n[1] 91.69166\n\n#Min distances\nmin_distannce &lt;- which(distances_df==min_distance, arr.ind=TRUE)\nmin_distannce\n\n     row col\n[1,]  29  19\n[2,]  19  29\n\n\nThe minimum distance is 91.69 km, identified by min_distance. Min_distannce identifies the row and column of the 91.69 value within the matrix. This corresponds to the names of the sites in columns 19 and 29 of the dataframe data_ex. Stade BMX de Saint-Quentin-en-Yvelines and Vélodrome National de Saint-Quentin-en-Yvelines are the nearest sites.\n\nWhich are the most distant sites\n\n\nmax_distance &lt;- max(distances_df, na.rm=TRUE)\nprint(max_distance)\n\n[1] 74882.55\n\nmax_distannce &lt;- which(distances_df==max_distance, arr.ind=TRUE)\nmax_distannce\n\n     row col\n[1,]  27   7\n[2,]   7  27\n\n\nColline d’Elancourt (7) and Stade Nautique (27) are the furthest away, with a distance of 74882.55 km.\n\nApartment at the center of gravity of all Olympic sites\n\n\n# Calculer les coordonnées moyennes (barycentriques)\nbarycentre &lt;- c(mean(data_ex$lat), mean(data_ex$long))\n\n# Trouver le site le plus proche du barycentre\nsite_proche_barycentre &lt;- data_ex[which.min(geosphere::distVincentySphere(barycentre, cbind(data_ex$lat, data_ex$long))), ]\n\nsite_proche_barycentre\n\n        lat     long               nom   sites_olympiques_paralympiques\n18 48.85723 2.296084 Stade Tour Eiffel Site olympique,Site paralympique\n                         sports\n18 Volleyball de plage,Cécifoot\n\n\nThe Eiffel Tower Stadium is at the center of all the Olympic venues."
  },
  {
    "objectID": "R/TD1/TD1.html#exercises-semaine-1",
    "href": "R/TD1/TD1.html#exercises-semaine-1",
    "title": "Exercices Semaine 1",
    "section": "",
    "text": "Cet ensemble d’exercices en R se concentre sur l’analyse des données relatives aux infrastructures olympiques des Jeux de 2024 à Paris. Nous utiliserons exclusivement les fonctions de base de R pour explorer, analyser et extraire des informations clés sur les sites olympiques et paralympiques.\n\nLire le fichier avec la base de données, nom et nature des colonnes, nombre de lignes\n\n\n#Lire et nommer le fichier \ndata_ex &lt;- read.csv(\"C:/Users/yavav/Downloads/paris-2024-sites-olympiques-et-paralympiques-franciliens (1).csv\", sep=\";\", dec=\",\", quote=\"\\\"\", na.strings = NA)\n\n#Noms et nature des colonnes \nstr(data_ex)\n\n'data.frame':   31 obs. of  4 variables:\n $ geo_point                     : chr  \"48.841319, 2.253076\" \"48.924388, 2.359871\" \"48.92467, 2.332428\" \"48.815115, 2.08208\" ...\n $ nom                           : chr  \"Parc des Princes\" \"Stade de France\" \"Village olympique\" \"Château de Versailles\" ...\n $ sites_olympiques_paralympiques: chr  \"Site olympique\" \"Site olympique,Site paralympique\" \"\" \"Site olympique,Site paralympique\" ...\n $ sports                        : chr  \"Football\" \"Athlétisme,Para athlétisme,Rugby\" \"\" \"Sports équestres,Para équitation,Pentathlon moderne\" ...\n\n#Nombre de lignes \nnrow(data_ex) \n\n[1] 31\n\n\nLe dataframe contient 4 colonnes: geo_point, village olympique, sites_olympiques_paralympiques et sports, codées en chr (caractère), ainsi que31 colonnes.\n\nCombien il y a-t-il de sites olympiques\n\n\nx &lt;- 0\n\nfor (value in data_ex$sites_olympiques_paralympiques) {\n  if (value == \"Site olympique\" | value == \"Site olympique,Site paralympique\") {\n    x &lt;- x + 1\n  } } \n\nprint(paste(\"Número total de Sites olympiques:\", x))\n\n[1] \"Número total de Sites olympiques: 26\"\n\n\n\nCombien il y a-t-il de sites paralympiques\n\n\ny &lt;- 0\n\nfor (value in data_ex$sites_olympiques_paralympiques) {\n  if (value == \"Site paralympique\" | value == \"Site olympique,Site paralympique\") {\n    y &lt;- y + 1\n  } } \n\nprint(paste(\"Número total de Sites paralympiques:\", y))\n\n[1] \"Número total de Sites paralympiques: 19\"\n\n\n\nQuels sont les sites qui aceuillent plusieurs disciplines sportives\n\n\n#On cherche  les cellules de la colonne sports qui continnent des éléments séparés par un virgule, indice d'acceuil de plus d'un sport\n\nsites_plusieurs_sports &lt;- data_ex[grepl(\",\", data_ex$sports), c(\"nom\")]\nsites_plusieurs_sports\n\n [1] \"Stade de France\"                                \n [2] \"Château de Versailles\"                          \n [3] \"Grand Palais\"                                   \n [4] \"Invalides\"                                      \n [5] \"Pont d'Iéna\"                                    \n [6] \"Arena Bercy\"                                    \n [7] \"Arena Paris nord\"                               \n [8] \"Arena Paris Sud 6 (Porte de Versailles)\"        \n [9] \"Stade de la Concorde\"                           \n[10] \"Arena Champs de Mars\"                           \n[11] \"Stade Tour Eiffel\"                              \n[12] \"Arena La Chapelle\"                              \n[13] \"Centre aquatique\"                               \n[14] \"Arena Paris Sud 1 (Porte de Versailles)\"        \n[15] \"Arena Paris Sud 4 (Porte de Versailles)\"        \n[16] \"La Défense Arena\"                               \n[17] \"Stade nautique\"                                 \n[18] \"Vélodrome National de Saint-Quentin-en-Yvelines\"\n[19] \"Stade Roland Garros\"                            \n\n\n\nQuelles sont les disciplines para-olympiques acceuillies dans ces sites franciliens\n\n\ndisciplines_plusieurs_sports &lt;- data_ex[grepl(\",\", data_ex$sports), c(\"nom\", \"sports\")] \ndisciplines_plusieurs_sports\n\n                                               nom\n2                                  Stade de France\n4                            Château de Versailles\n8                                     Grand Palais\n9                                        Invalides\n11                                     Pont d'Iéna\n12                                     Arena Bercy\n13                                Arena Paris nord\n14         Arena Paris Sud 6 (Porte de Versailles)\n15                            Stade de la Concorde\n17                            Arena Champs de Mars\n18                               Stade Tour Eiffel\n20                               Arena La Chapelle\n22                                Centre aquatique\n23         Arena Paris Sud 1 (Porte de Versailles)\n24         Arena Paris Sud 4 (Porte de Versailles)\n26                                La Défense Arena\n27                                  Stade nautique\n29 Vélodrome National de Saint-Quentin-en-Yvelines\n30                             Stade Roland Garros\n                                                             sports\n2                                  Athlétisme,Para athlétisme,Rugby\n4               Sports équestres,Para équitation,Pentathlon moderne\n8                 Escrime,Escrime fauteuil,Taekwondo,Para taekwondo\n9        Tir à l'arc,Para tir à l'arc,Athlétisme,Cyclisme sur route\n11                                    Cyclisme sur route,Athlétisme\n12     Basketball,Basket fauteuil,Gymnastique artistique,trampoline\n13                Boxe,Escrime,Pentathlon moderne,Volley-ball assis\n14                                  Haltérophilie,Handball,Goalball\n15                 Basketball 3x3,BMX freestyle,Breaking,Skateboard\n17                              Judo,Para judo,Lutte,Rugby fauteuil\n18                                     Volleyball de plage,Cécifoot\n20 Badminton,Para Badminton,Gymnastique rythmique,Para powerlifting\n22                          Natation artistique,Plongeon,Water-polo\n23                                                Volleyball,Boccia\n24                             Tennis de table,Para tennis de table\n26                                Natation,Para natation,Water-polo\n27                              Canoë,Para canoë,Aviron,Para aviron\n29                       Cyclisme sur piste,Para cyclisme sur piste\n30                                      Tennis,Tennis fauteuil,Boxe\n\n\nLe dataframe résultant permet de visualiser le nom du site avec les différentes disciplines associées.\n\nSites qui aceuillent le plus de disciples différentes\n\n\nmax_sports &lt;- max(sapply(strsplit(data_ex$sports, \",\"),length))\n\nfiles_max_sports &lt;- which(sapply(strsplit(data_ex$sports, \",\"),length)==max_sports)\n\ndata_max_sports &lt;- data_ex[files_max_sports, c(\"nom\", \"sports\")]\n\ndata_max_sports\n\n                    nom\n8          Grand Palais\n9             Invalides\n12          Arena Bercy\n13     Arena Paris nord\n15 Stade de la Concorde\n17 Arena Champs de Mars\n20    Arena La Chapelle\n27       Stade nautique\n                                                             sports\n8                 Escrime,Escrime fauteuil,Taekwondo,Para taekwondo\n9        Tir à l'arc,Para tir à l'arc,Athlétisme,Cyclisme sur route\n12     Basketball,Basket fauteuil,Gymnastique artistique,trampoline\n13                Boxe,Escrime,Pentathlon moderne,Volley-ball assis\n15                 Basketball 3x3,BMX freestyle,Breaking,Skateboard\n17                              Judo,Para judo,Lutte,Rugby fauteuil\n20 Badminton,Para Badminton,Gymnastique rythmique,Para powerlifting\n27                              Canoë,Para canoë,Aviron,Para aviron\n\n\nOn observe qu’il y a un total de 8 sites qui acceuillent un total de 4 sports chacun.\n\nQuel discipline aura lieu sur le plus grand nombre de sites ? Quels sont ces sites ?\n\n\n# Conter la fréquence de chaque discipline\nfrequence_disciplines &lt;- table(unlist(strsplit(data_ex$sports, \",\")))\n\n# Discipline avec plus grande fréquence\ndiscipline_plus_frequente &lt;- names(frequence_disciplines)[which.max(frequence_disciplines)]\n\n# Résultat\ncat(\"La discipline qui aura lieu sur le plus grand nombre de sites est:\", discipline_plus_frequente, \"\\n\")\n\nLa discipline qui aura lieu sur le plus grand nombre de sites est: Athlétisme \n\n\n\n# Filtrer les sites qui incluent le mot \"Athlétisme\" dans la colonne Discipline\nsites_atletisme &lt;- data_ex[grepl(\"Athlétisme\", data_ex$sports, ignore.case = TRUE), c(\"nom\", \"sports\")]\n\n# Imprimer les résultats\ncat(\"Les sites où l'athlétisme aura lieu sont les suivants:\\n\")\n\nLes sites où l'athlétisme aura lieu sont les suivants:\n\nprint(sites_atletisme)\n\n                                   nom\n2                      Stade de France\n6  Terrain des Essences - La Courneuve\n9                            Invalides\n11                         Pont d'Iéna\n25             Hôtel de ville de Paris\n                                                       sports\n2                            Athlétisme,Para athlétisme,Rugby\n6                                             Para athlétisme\n9  Tir à l'arc,Para tir à l'arc,Athlétisme,Cyclisme sur route\n11                              Cyclisme sur route,Athlétisme\n25                                                 Athlétisme\n\n\nOn voit que l’athlétisme aura lieu sur 5 sites à Paris.\n\nQuels sont les deux sites les plus proches ?\n\n\n#Libraires \n\nlibrary(geosphere)\nlibrary(tidyr)\n\n#Séparer les coordonnées géographiques en deux colonnes par la virgule \n\ndata_ex &lt;- separate(data_ex, geo_point, into = c(\"lat\", \"long\"), sep = \", \")\n\n# Fonction pour calculer la distance euclidienne\n\ndata_ex$lat &lt;- as.numeric(data_ex$lat)\ndata_ex$long &lt;- as.numeric(data_ex$long)\n\ndistances &lt;- distHaversine(data_ex[, c(\"long\", \"lat\")])\nprint(distances)\n\n [1] 12109.0289  2007.5022 22017.3512 28908.6007  1204.2823 36658.5531\n [7] 26702.8086   422.2544 11450.5703 13052.7520  6674.7313 18001.9809\n[13] 22981.5082  4582.2876  8949.4822  9269.0396   436.5820 20677.9533\n[19] 27022.4177 10476.8609 12176.4034 11437.7799   419.6006  5441.5332\n[25]  9902.1865 30003.3283 42913.1029  5092.1872 16782.0752 23146.9519\n\n# Matrice pour garder les distances\ndistances &lt;- matrix(NA, nrow = nrow(data_ex), ncol = nrow(data_ex))\n\n# Calculer les distances entre toutes les coordonnées avec un boucle \nfor (i in 1:(nrow(data_ex) - 1)) {\n  for (j in (i + 1):nrow(data_ex)) {\n    distances[i, j] &lt;- distHaversine(\n      c(data_ex$lat[i], data_ex$long[i]),\n      c(data_ex$lat[j], data_ex$long[j])\n    )\n    distances[j, i] &lt;- distances[i, j]  # Symétrie\n  }\n}\n\n# Convertir la matirce en dataframe\ndistances_df &lt;- as.data.frame(distances)\n\n#Le sites les plus proches\nmin_distance &lt;- min(distances_df, na.rm=TRUE)\nmin_distance \n\n[1] 91.69166\n\n#Min distances\nmin_distannce &lt;- which(distances_df==min_distance, arr.ind=TRUE)\nmin_distannce\n\n     row col\n[1,]  29  19\n[2,]  19  29\n\n\nOn observe que la distance minimale est de 91.69 km, identifiée par min_distance. Min_distannce permet d’identifier la ligne et la colonne de la valeur 91.69 au sein de la matrice. Cela correspond aux noms des sites des colonnes 19 et 29 du dataframe data_ex. Stade BMX de Saint-Quentin-en-Yvelines et Vélodrome National de Saint-Quentin-en-Yvelines sont les sites les plus proches.\n\nQuels sont les sites les plus éloignés\n\n\nmax_distance &lt;- max(distances_df, na.rm=TRUE)\nprint(max_distance)\n\n[1] 74882.55\n\nmax_distannce &lt;- which(distances_df==max_distance, arr.ind=TRUE)\nmax_distannce\n\n     row col\n[1,]  27   7\n[2,]   7  27\n\n\nOn observe que Colline d’Elancourt (7) et Stade Nautique (27) sont les sites les plus éloignés avec une distance de 74882.55 km.\n\nAppartement situé au barycentre de l’ensemble des sites olympiques\n\n\n# Calculer les coordonnées moyennes (barycentriques)\nbarycentre &lt;- c(mean(data_ex$lat), mean(data_ex$long))\n\n# Trouver le site le plus proche du barycentre\nsite_proche_barycentre &lt;- data_ex[which.min(geosphere::distVincentySphere(barycentre, cbind(data_ex$lat, data_ex$long))), ]\n\nsite_proche_barycentre\n\n        lat     long               nom   sites_olympiques_paralympiques\n18 48.85723 2.296084 Stade Tour Eiffel Site olympique,Site paralympique\n                         sports\n18 Volleyball de plage,Cécifoot\n\n\nLe Stade Tour Eiffel se situe au barycentre de l’ensemble des sites olympiques."
  },
  {
    "objectID": "R/TD2/TD 2.html",
    "href": "R/TD2/TD 2.html",
    "title": "Analyzing Olympic Games : Fonctions in R",
    "section": "",
    "text": "The aim of these exercises is to implement functions for analyzing historical data relating to the Olympic Games. With the functions generated, we’ll be able to exploit information such as the number of medals won by each athlete during their participation in the program, the nationalities most represented at the Olympic Games and much more.\n\nRun the following command. What does the object contain?\n\n\nlibrary(rvest)\nlibrary(purrr)\n\nlist_tables &lt;-\n  session(\"https://fr.wikipedia.org/wiki/Liste_des_m%C3%A9daill%C3%A9s_olympiques\") |&gt;\n  html_elements(\".wikitable\") |&gt;\n  html_table()\n\nThe object contains a list with 6 elements containing different databases.\n\nWe’re interested in the first table. Create an object entitledédata_medailles_sport_ete containing the first element of list_tables. The table is not well formatted. Delete the first column, the column names and the first row. Rename columns to c(“Discipline”, “Annees”, “Editions”, “Epreuves_2020”, “Or”, “Argent”, “Bronze”, “Total”, “Athletes_medailles”, “Athletes_or”). The “Editions”, “Epreuves_2020”, “Or”, “Argent”, “Bronze” and “Total” columns will be converted into integer columns.\n\n\n#Créer data_medailles_sport_ete contenant la première table de la liste \n\ndata_medailles_sport_ete &lt;- list_tables[[1]]\n\n# Supprimer la première colonne, les noms de colonnes et la première ligne\n\ndata_medailles_sport_ete &lt;- data_medailles_sport_ete[-1,-1]\ncolnames(data_medailles_sport_ete) &lt;- NULL\n\n# Renommer des colonnes \n\ncolnames(data_medailles_sport_ete) &lt;-  c(\"Discipline\", \"Annees\", \"Editions\",\n  \"Epreuves_2020\", \"Or\", \"Argent\", \"Bronze\", \"Total\", \"Athletes_medailles\",\n  \"Athletes_or\")\n\n#Coder en integer \n\ndata_medailles_sport_ete[, 3:8] &lt;- lapply(data_medailles_sport_ete[, 3:8], as.integer)\n\n\nThe 3 disciplines with the most medals awarded\n\n\nlibrary(tidyverse)\nlibrary(dplyr)\n\n#Option avec le tidyverse\n\nTop3_disciplines &lt;- data_medailles_sport_ete |&gt; arrange(desc(Total)) |&gt; slice(1:3)\n\nTop3_disciplines\n\n# A tibble: 3 × 10\n  Discipline             Annees Editions Epreuves_2020    Or Argent Bronze Total\n  &lt;chr&gt;                  &lt;chr&gt;     &lt;int&gt;         &lt;int&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt;\n1 Athlétisme             Depui…       29            48  1028   1030   1027  3085\n2 Natation(hommes, femm… Depui…       29            37   607    604    603  1814\n3 Lutte                  1896,…       28            18   430    429    499  1358\n# ℹ 2 more variables: Athletes_medailles &lt;chr&gt;, Athletes_or &lt;chr&gt;\n\n#Une autre option avec la fct orderet regarder les 3 premiers\n\norder(data_medailles_sport_ete$Total, decreasing=TRUE)\n\n [1]  1 18 17 12  5 25  2  7  6 14  9 16 28  8 21 23 26 22 20  3 24 15 10  4 29\n[26] 30 13 19 27 11\n\n\nAthletics, swimming and wrestling are the three disciplines with the most medals distributed.\n\nThe 3 disciplines with the fewest events in 2020\n\n\ndata_medailles_sport_ete_ordonne_epreuves2020 &lt;- data_medailles_sport_ete |&gt; arrange(Epreuves_2020) |&gt; slice(1:3)\ndata_medailles_sport_ete_ordonne_epreuves2020 ##Sélectionner les 3 dernières\n\n# A tibble: 3 × 10\n  Discipline Annees             Editions Epreuves_2020    Or Argent Bronze Total\n  &lt;chr&gt;      &lt;chr&gt;                 &lt;int&gt;         &lt;int&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt;\n1 Football   1900–1928,depuis …       27             2    34     34     35   103\n2 Golf       1900–1904,depuis …        4             2     8      8      9    19\n3 Handball   1936,depuis 1972         14             2    26     26     26    78\n# ℹ 2 more variables: Athletes_medailles &lt;chr&gt;, Athletes_or &lt;chr&gt;\n\n\nSoccer, golf and handball were the disciplines with the fewest events in 2020.\n\nCalculate a function since which represents since which year the edition is on the program\n\n\ncalcul_nb_editions_int &lt;- function(depuis){\n  annee &lt;- 2020\n  int &lt;- 4\n  x &lt;- ((annee-depuis)/4)+1\n  if (depuis&lt;1916){\n    nb_ed &lt;- x-3\n    nb_ed\n  }\n  if(depuis&gt;1916){\n    nb_ed &lt;- x-2\n    nb_ed\n  }\n  if(depuis&gt;1944){\n    nb_ed &lt;- x\n    nb_ed\n  }\n  nb_ed\n}\n\n#Exemple d'utilisation\ncalcul_nb_editions_int(1992)\n\n[1] 8\n\n\n\nPropose a new function calcul_nb_editions_str, this time taking strings as input.\n\n\ncalcul_nb_editions_str &lt;- function(x){\n  \n  z &lt;- str_split(x,\",\",simplify = TRUE)\n  r &lt;- as.matrix(ifelse(!grepl(\"depuis|-\",z),\"P\",z))\n  # permet de compter le nombre de valeur unique pour l'année \n  \n  res_annee &lt;- 0\n  res_depuis &lt;- 0\n  res_tiret &lt;- 0\n  \n  calcul_nb_editions_int &lt;- function(depuis){\n    annee &lt;- 2020\n    int &lt;- 4\n    x &lt;- ((annee-depuis)/4)+1\n    if (depuis&lt;1916){\n      nb_ed &lt;- x-3\n      nb_ed\n    }\n    if(depuis&gt;1916){\n      nb_ed &lt;- x-2\n      nb_ed\n    }\n    if(depuis&gt;1944){\n      nb_ed &lt;- x\n      nb_ed\n    }\n    return(nb_ed)\n  }\n  \n  calcul_nb_editions_interne  &lt;- function(deb,fi){\n    \n    int &lt;- 4\n    x &lt;- ((fi-deb)/4)+1\n    if (fi &lt; 1916 & deb &lt; 1916){\n      nb_ed &lt;- x\n      nb_ed\n    }\n    if (deb &lt; 1916 & fi &lt; 1944){\n      nb_ed &lt;- x-1\n      nb_ed\n    }\n    if (deb &lt; 1916 & fi&gt; 1944){\n      nb_ed &lt;- x-3\n      nb_ed\n    }\n    if(deb&gt;1916 & fi &lt; 1944){\n      nb_ed &lt;- x\n      nb_ed\n    }\n    if(deb &gt; 1916 & fi &gt; 1944){\n      nb_ed &lt;- x-2\n      nb_ed\n    }\n    \n    if (deb &gt; 1944 & fi &gt; 1944){\n      nb_ed &lt;- x\n      nb_ed\n    }\n    \n    return(nb_ed)\n  }\n  \n  \n  for (j in 1:nrow(r)) {\n    if (any(str_detect(r[j, 1], \"-\"))) {\n      e &lt;- sapply(str_split(r[j, 1], \"-\"), as.integer)\n      debut &lt;- e[1, 1]\n      fin &lt;- e[2, 1]\n      res_tiret &lt;- calcul_nb_editions_interne(debut, fin)\n    }\n    \n    if (any(str_detect(r[j, 1], \"depuis\"))) {\n      d &lt;- as.integer(str_extract_all(r[j, 1], \"\\\\d+\"))\n      res_depuis &lt;- calcul_nb_editions_int(d)\n    }\n    \n    if(is.character(r[j, 1])){\n      sp &lt;- str_detect(r,\"P\")\n      res_annee &lt;- sum(sp)  # premier cas de fonction \n    }\n  }\n  \n  resultat &lt;- sum(res_depuis, res_tiret, res_annee)\n  return(resultat)\n}\n\n#Exemple d'utilisation\ncalcul_nb_editions_str(\"1896, 1904, depuis 1920\")\n\n[1] 26\n\n\n\nGeneric function calcul_nb_editions and two implementations calcul_nb_editions.integer and calcul_nb_editions.character\n\n\n# Définition de la fonction génerique\ncalcul_nb_editions &lt;- function(x) {\n  UseMethod(\"calcul_nb_editions\")\n}\n\n#Fonction integer\n\ncalcul_nb_editions.integer &lt;- function(x) {\n  nb_participations &lt;- length(unique(x))\n  return(nb_participations)\n}\n\ncalcul_nb_editions.numeric &lt;- function(x) {\n  # Vérifier si x est entier\n  if (x == floor(x)) {\n    return(calcul_nb_editions.integer(as.integer(x)))\n  } else {\n    stop(\"La méthode n'est définie que pour les entiers\")\n  }\n}\n\n#Fonction caractère\n\ncalcul_nb_editions.character &lt;- function(x) {\n  annee_actuelle &lt;- as.integer(format(Sys.Date(), \"%Y\"))\n  annees &lt;- unlist(str_extract_all(x, \"\\\\b\\\\d{4}\\\\b\"))\n  depuis_annees &lt;- as.integer(unlist(str_extract_all(x, regex(\"(?&lt;=depuis )\\\\d{4}\", ignore_case = TRUE)))) \n  \n  plages &lt;- str_extract_all(x, \"\\\\d{4}–\\\\d{4}\")\n  plages &lt;- unlist(plages)\n  for (plage in plages) {\n    annees_debut_fin &lt;- as.integer(unlist(str_extract_all(plage, \"\\\\d{4}\")))\n    annees &lt;- c(annees, seq(annees_debut_fin[1], annees_debut_fin[2], by = 4))\n  }\n\n  for (depuis_annee in depuis_annees) {\n    annees_depuis &lt;- seq(depuis_annee, annee_actuelle, by = 4)\n    annees &lt;- c(annees, annees_depuis)\n  }\n  nb_editions &lt;- length(unique(as.integer(annees)))\n  \n  return(nb_editions)\n}\n\nResults:\n\ncalcul_nb_editions(2000)\n\n[1] 1\n\ncalcul_nb_editions(\"1904–1924, depuis 1948\")\n\n[1] 26\n\n\nThe result of the first calculation of the function is equal to 1 and the result of the second calculation is equal to 26.\n\nFunction calcul_medailles_individuelles, which determines the maximum number of medals an athlete has won in each Olympiad.\n\n\ncalcul_medailles_individuelles &lt;- function(athletes_medailles) {\n  # Dataframe pour stocker les résultats\n  resultats_df &lt;- data.frame(Athlete = character(), TotalMedailles = integer(), stringsAsFactors = FALSE)\n  \n  # Séparation des athlètes\n  athletes_details &lt;- strsplit(athletes_medailles, \"\\\\)\\\\s*(?=[A-Z])\", perl = TRUE)[[1]]\n  \n  for (detail in athletes_details) {\n    detail &lt;- gsub(\"[()]\", \"\", detail) # Nettoyer la chaîne\n    name_medals_parts &lt;- strsplit(detail, \"\\\\s(?=\\\\d+-\\\\d+-\\\\d+)\", perl = TRUE)[[1]] # \n    \n  #Séparer le nom des médailles\n    \n    if (length(name_medals_parts) &lt; 2) next\n    \n    athlete_name &lt;- name_medals_parts[1]\n    medals_info &lt;- strsplit(name_medals_parts[2], \"-\")[[1]]\n    total_medals &lt;- sum(as.integer(medals_info))\n    \n    resultats_df &lt;- rbind(resultats_df, data.frame(Athlete = athlete_name, TotalMedailles = total_medals, stringsAsFactors = FALSE))\n  }\n  \n  return(resultats_df)\n}\n\n#Exemple d'utilisation\ncalcul_medailles_individuelles(\"Paavo Nurmi (FIN) (9-3-0)\")\n\n          Athlete TotalMedailles\n1 Paavo Nurmi FIN             12\n\n\n\nTop 3 athletes\n\n\n# On applique la fonction précédemment créée calcul_medailles_individuelles\nliste_df &lt;- lapply(data_medailles_sport_ete$Athletes_medailles, calcul_medailles_individuelles)\n\n# Un seul dataframe\ndf_athletes_medailles &lt;- do.call(rbind, liste_df)\n\n# On sélectionne les trois premiers athlètes\ntop_3_athletes &lt;- head(df_athletes_medailles[order(-df_athletes_medailles$TotalMedailles),], 3)\n\ntop_3_athletes\n\n                   Athlete TotalMedailles\n39      Michael Phelps USA             28\n23    Larissa Latynina URS             18\n18 Edoardo Mangiarotti ITA             13\n\n\nThe top 3 athletes were Michael Phelps, Larissa Latynina and Edoardo Mangiarotti with a total of 28, 18 and 13 medals respectively.\n\nThe 3 most represented nationalities, for all events, in the ranking of individual gold medals won\n\n\nextraire_nationalite_athlete &lt;- function(texte) {\n  # Extraire les morceaux contenant la nationalité et les médailles d'or\n  matches &lt;- gregexpr(\"\\\\([A-Z]{3}\\\\) \\\\((\\\\d+)-\", texte)\n  nationalites_et_or &lt;- regmatches(texte, matches)\n  \n  # Vecteur pour les nationalités\n  nationalites &lt;- vector(\"list\", length(nationalites_et_or[[1]]))\n  \n  for(i in seq_along(nationalites_et_or[[1]])) {\n    match &lt;- nationalites_et_or[[1]][i]\n    nationalite &lt;- substr(match, 2, 4) # Extraire la nationalité\n    or &lt;- as.numeric(substr(match, 8, nchar(match)-1)) # Extraire le nombre de médailles d'or\n    \n    if(or &gt; 0) { \n      if(!is.null(nationalites[[nationalite]])) {\n        nationalites[[nationalite]] &lt;- nationalites[[nationalite]] + 1\n      } else {\n        nationalites[[nationalite]] &lt;- 1\n      }\n    }\n  }\n  \n  return(unlist(nationalites))\n}\n\nresultats_nationalites &lt;- lapply(data_medailles_sport_ete$Athletes_medailles, extraire_nationalite_athlete)\n\nnationalites_agg &lt;- Reduce(`+`, resultats_nationalites)\n\n# Les 3 nationalités les plus représentées\ntop_3_nationalites &lt;- sort(nationalites_agg, decreasing = TRUE)[1:3]\n\ntop_3_nationalites\n\nCUB HUN URS \n 35  33  33 \n\n\nCuba, Hungary and the Soviet Union are the 3 most represented nationalities."
  },
  {
    "objectID": "R/TD2/TD 2.html#exercices-semaine-2",
    "href": "R/TD2/TD 2.html#exercices-semaine-2",
    "title": "Exercices Semaine 2",
    "section": "",
    "text": "Ces exercises ont pour but l’implémentation des fonctions pour analyser les données historiques relatives aux jeux olympiques. Avec les fonctions qui seront générées, on pourra exploiter des informations telles que le nombre de médailles obtenues par athlète au cours de leur participation au programme, les nationalités les plus représentées aux Jeux Olympiques et bien encore plus!\n\nLancer la commande suivante. Que contient l’objet?\n\n\nlibrary(rvest)\nlibrary(purrr)\n\nlist_tables &lt;-\n  session(\"https://fr.wikipedia.org/wiki/Liste_des_m%C3%A9daill%C3%A9s_olympiques\") |&gt;\n  html_elements(\".wikitable\") |&gt;\n  html_table()\n\nL’objet contient une liste avec 6 éléments contenant différentes bases de données.\n\nOn s’intéresse à la première table. Créer un objet intitulédata_medailles_sport_ete contenant le premier élément de list_tables. La table n’est pas bien formattée. Supprimer la première colonne, les noms de colonnes et la première ligne. Renommer les colonnes en c(“Discipline”, “Annees”, “Editions”, “Epreuves_2020”, “Or”, “Argent”, “Bronze”, “Total”, “Athletes_medailles”, “Athletes_or”). Les colonnes Editions, Epreuves_2020, Or, Argent, Bronze, Total seront converties en colonnes d’entiers.\n\n\n#Créer data_medailles_sport_ete contenant la première table de la liste \n\ndata_medailles_sport_ete &lt;- list_tables[[1]]\n\n# Supprimer la première colonne, les noms de colonnes et la première ligne\n\ndata_medailles_sport_ete &lt;- data_medailles_sport_ete[-1,-1]\ncolnames(data_medailles_sport_ete) &lt;- NULL\n\n# Renommer des colonnes \n\ncolnames(data_medailles_sport_ete) &lt;-  c(\"Discipline\", \"Annees\", \"Editions\",\n  \"Epreuves_2020\", \"Or\", \"Argent\", \"Bronze\", \"Total\", \"Athletes_medailles\",\n  \"Athletes_or\")\n\n#Coder en integer \n\ndata_medailles_sport_ete[, 3:8] &lt;- lapply(data_medailles_sport_ete[, 3:8], as.integer)\n\n\nLes 3 disciplines avec le plus de médailles distribuées\n\n\nlibrary(tidyverse)\nlibrary(dplyr)\n\n#Option avec le tidyverse\n\nTop3_disciplines &lt;- data_medailles_sport_ete |&gt; arrange(desc(Total)) |&gt; slice(1:3)\n\nTop3_disciplines\n\n# A tibble: 3 × 10\n  Discipline             Annees Editions Epreuves_2020    Or Argent Bronze Total\n  &lt;chr&gt;                  &lt;chr&gt;     &lt;int&gt;         &lt;int&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt;\n1 Athlétisme             Depui…       29            48  1028   1030   1027  3085\n2 Natation(hommes, femm… Depui…       29            37   607    604    603  1814\n3 Lutte                  1896,…       28            18   430    429    499  1358\n# ℹ 2 more variables: Athletes_medailles &lt;chr&gt;, Athletes_or &lt;chr&gt;\n\n#Une autre option avec la fct orderet regarder les 3 premiers\n\norder(data_medailles_sport_ete$Total, decreasing=TRUE)\n\n [1]  1 18 17 12  5 25  2  7  6 14  9 16 28  8 21 23 26 22 20  3 24 15 10  4 29\n[26] 30 13 19 27 11\n\n\nL’athléthisme, la natation et la lutte sont les trois disciplines avec le plus de médailles distribuées.\n\nLes 3 disciplines avec le moins d’épreuves en 2020\n\n\ndata_medailles_sport_ete_ordonne_epreuves2020 &lt;- data_medailles_sport_ete |&gt; arrange(Epreuves_2020) |&gt; slice(1:3)\ndata_medailles_sport_ete_ordonne_epreuves2020 ##Sélectionner les 3 dernières\n\n# A tibble: 3 × 10\n  Discipline Annees             Editions Epreuves_2020    Or Argent Bronze Total\n  &lt;chr&gt;      &lt;chr&gt;                 &lt;int&gt;         &lt;int&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt;\n1 Football   1900–1928,depuis …       27             2    34     34     35   103\n2 Golf       1900–1904,depuis …        4             2     8      8      9    19\n3 Handball   1936,depuis 1972         14             2    26     26     26    78\n# ℹ 2 more variables: Athletes_medailles &lt;chr&gt;, Athletes_or &lt;chr&gt;\n\n\nLe football, le golf et le handball ont été les disciplines avec le moins d’épreuves en 2020.\n\nCalculer une fonction depuis qui représente depuis quelle année l’édition est au programme\n\n\ncalcul_nb_editions_int &lt;- function(depuis){\n  annee &lt;- 2020\n  int &lt;- 4\n  x &lt;- ((annee-depuis)/4)+1\n  if (depuis&lt;1916){\n    nb_ed &lt;- x-3\n    nb_ed\n  }\n  if(depuis&gt;1916){\n    nb_ed &lt;- x-2\n    nb_ed\n  }\n  if(depuis&gt;1944){\n    nb_ed &lt;- x\n    nb_ed\n  }\n  nb_ed\n}\n\n#Exemple d'utilisation\ncalcul_nb_editions_int(1992)\n\n[1] 8\n\n\n\nProposer une nouvelle fonction calcul_nb_editions_str qui prendra cette fois-ci en entrée des chaînes de caractères\n\n\ncalcul_nb_editions_str &lt;- function(x){\n  \n  z &lt;- str_split(x,\",\",simplify = TRUE)\n  r &lt;- as.matrix(ifelse(!grepl(\"depuis|-\",z),\"P\",z))\n  # permet de compter le nombre de valeur unique pour l'année \n  \n  res_annee &lt;- 0\n  res_depuis &lt;- 0\n  res_tiret &lt;- 0\n  \n  calcul_nb_editions_int &lt;- function(depuis){\n    annee &lt;- 2020\n    int &lt;- 4\n    x &lt;- ((annee-depuis)/4)+1\n    if (depuis&lt;1916){\n      nb_ed &lt;- x-3\n      nb_ed\n    }\n    if(depuis&gt;1916){\n      nb_ed &lt;- x-2\n      nb_ed\n    }\n    if(depuis&gt;1944){\n      nb_ed &lt;- x\n      nb_ed\n    }\n    return(nb_ed)\n  }\n  \n  calcul_nb_editions_interne  &lt;- function(deb,fi){\n    \n    int &lt;- 4\n    x &lt;- ((fi-deb)/4)+1\n    if (fi &lt; 1916 & deb &lt; 1916){\n      nb_ed &lt;- x\n      nb_ed\n    }\n    if (deb &lt; 1916 & fi &lt; 1944){\n      nb_ed &lt;- x-1\n      nb_ed\n    }\n    if (deb &lt; 1916 & fi&gt; 1944){\n      nb_ed &lt;- x-3\n      nb_ed\n    }\n    if(deb&gt;1916 & fi &lt; 1944){\n      nb_ed &lt;- x\n      nb_ed\n    }\n    if(deb &gt; 1916 & fi &gt; 1944){\n      nb_ed &lt;- x-2\n      nb_ed\n    }\n    \n    if (deb &gt; 1944 & fi &gt; 1944){\n      nb_ed &lt;- x\n      nb_ed\n    }\n    \n    return(nb_ed)\n  }\n  \n  \n  for (j in 1:nrow(r)) {\n    if (any(str_detect(r[j, 1], \"-\"))) {\n      e &lt;- sapply(str_split(r[j, 1], \"-\"), as.integer)\n      debut &lt;- e[1, 1]\n      fin &lt;- e[2, 1]\n      res_tiret &lt;- calcul_nb_editions_interne(debut, fin)\n    }\n    \n    if (any(str_detect(r[j, 1], \"depuis\"))) {\n      d &lt;- as.integer(str_extract_all(r[j, 1], \"\\\\d+\"))\n      res_depuis &lt;- calcul_nb_editions_int(d)\n    }\n    \n    if(is.character(r[j, 1])){\n      sp &lt;- str_detect(r,\"P\")\n      res_annee &lt;- sum(sp)  # premier cas de fonction \n    }\n  }\n  \n  resultat &lt;- sum(res_depuis, res_tiret, res_annee)\n  return(resultat)\n}\n\n#Exemple d'utilisation\ncalcul_nb_editions_str(\"1896, 1904, depuis 1920\")\n\n[1] 26\n\n\n\nFonction générique calcul_nb_editions et deux implémentations calcul_nb_editions.integer et calcul_nb_editions.character\n\n\n# Définition de la fonction génerique\ncalcul_nb_editions &lt;- function(x) {\n  UseMethod(\"calcul_nb_editions\")\n}\n\n#Fonction integer\n\ncalcul_nb_editions.integer &lt;- function(x) {\n  nb_participations &lt;- length(unique(x))\n  return(nb_participations)\n}\n\ncalcul_nb_editions.numeric &lt;- function(x) {\n  # Vérifier si x est entier\n  if (x == floor(x)) {\n    return(calcul_nb_editions.integer(as.integer(x)))\n  } else {\n    stop(\"La méthode n'est définie que pour les entiers\")\n  }\n}\n\n#Fonction caractère\n\ncalcul_nb_editions.character &lt;- function(x) {\n  annee_actuelle &lt;- as.integer(format(Sys.Date(), \"%Y\"))\n  annees &lt;- unlist(str_extract_all(x, \"\\\\b\\\\d{4}\\\\b\"))\n  depuis_annees &lt;- as.integer(unlist(str_extract_all(x, regex(\"(?&lt;=depuis )\\\\d{4}\", ignore_case = TRUE)))) \n  \n  plages &lt;- str_extract_all(x, \"\\\\d{4}–\\\\d{4}\")\n  plages &lt;- unlist(plages)\n  for (plage in plages) {\n    annees_debut_fin &lt;- as.integer(unlist(str_extract_all(plage, \"\\\\d{4}\")))\n    annees &lt;- c(annees, seq(annees_debut_fin[1], annees_debut_fin[2], by = 4))\n  }\n\n  for (depuis_annee in depuis_annees) {\n    annees_depuis &lt;- seq(depuis_annee, annee_actuelle, by = 4)\n    annees &lt;- c(annees, annees_depuis)\n  }\n  nb_editions &lt;- length(unique(as.integer(annees)))\n  \n  return(nb_editions)\n}\n\nRésultats pour les appels:\n\ncalcul_nb_editions(2000)\n\n[1] 1\n\ncalcul_nb_editions(\"1904–1924, depuis 1948\")\n\n[1] 26\n\n\nLe résultat du premier calcul de la fonction est égal à 1 et celui du deuxième calcul le deuxième est égal à 26.\n\nFonction calcul_medailles_individuelles qui détermine le nombre de médaille maximal a été obtenu par un athlète lors d’olympiades\n\n\ncalcul_medailles_individuelles &lt;- function(athletes_medailles) {\n  # Dataframe pour stocker les résultats\n  resultats_df &lt;- data.frame(Athlete = character(), TotalMedailles = integer(), stringsAsFactors = FALSE)\n  \n  # Séparation des athlètes\n  athletes_details &lt;- strsplit(athletes_medailles, \"\\\\)\\\\s*(?=[A-Z])\", perl = TRUE)[[1]]\n  \n  for (detail in athletes_details) {\n    detail &lt;- gsub(\"[()]\", \"\", detail) # Nettoyer la chaîne\n    name_medals_parts &lt;- strsplit(detail, \"\\\\s(?=\\\\d+-\\\\d+-\\\\d+)\", perl = TRUE)[[1]] # \n    \n  #Séparer le nom des médailles\n    \n    if (length(name_medals_parts) &lt; 2) next\n    \n    athlete_name &lt;- name_medals_parts[1]\n    medals_info &lt;- strsplit(name_medals_parts[2], \"-\")[[1]]\n    total_medals &lt;- sum(as.integer(medals_info))\n    \n    resultats_df &lt;- rbind(resultats_df, data.frame(Athlete = athlete_name, TotalMedailles = total_medals, stringsAsFactors = FALSE))\n  }\n  \n  return(resultats_df)\n}\n\n#Exemple d'utilisation\ncalcul_medailles_individuelles(\"Paavo Nurmi (FIN) (9-3-0)\")\n\n          Athlete TotalMedailles\n1 Paavo Nurmi FIN             12\n\n\n\nTop 3 des athlètes\n\n\n# On applique la fonction précédemment créée calcul_medailles_individuelles\nliste_df &lt;- lapply(data_medailles_sport_ete$Athletes_medailles, calcul_medailles_individuelles)\n\n# Un seul dataframe\ndf_athletes_medailles &lt;- do.call(rbind, liste_df)\n\n# On sélectionne les trois premiers athlètes\ntop_3_athletes &lt;- head(df_athletes_medailles[order(-df_athletes_medailles$TotalMedailles),], 3)\n\ntop_3_athletes\n\n                   Athlete TotalMedailles\n39      Michael Phelps USA             28\n23    Larissa Latynina URS             18\n18 Edoardo Mangiarotti ITA             13\n\n\nLes top 3 des athlètes sont Michael Phelps, Larissa Latynina et Edoardo Mangiarotti avec un total de 28, 18 et 13 médailles respectivement.\n\nLes 3 nationalités les plus représentées, pour toutes les épreuves, au classement du nombre de médailles d’or individuelles reccueillies\n\n\nextraire_nationalite_athlete &lt;- function(texte) {\n  # Extraire les morceaux contenant la nationalité et les médailles d'or\n  matches &lt;- gregexpr(\"\\\\([A-Z]{3}\\\\) \\\\((\\\\d+)-\", texte)\n  nationalites_et_or &lt;- regmatches(texte, matches)\n  \n  # Vecteur pour les nationalités\n  nationalites &lt;- vector(\"list\", length(nationalites_et_or[[1]]))\n  \n  for(i in seq_along(nationalites_et_or[[1]])) {\n    match &lt;- nationalites_et_or[[1]][i]\n    nationalite &lt;- substr(match, 2, 4) # Extraire la nationalité\n    or &lt;- as.numeric(substr(match, 8, nchar(match)-1)) # Extraire le nombre de médailles d'or\n    \n    if(or &gt; 0) { \n      if(!is.null(nationalites[[nationalite]])) {\n        nationalites[[nationalite]] &lt;- nationalites[[nationalite]] + 1\n      } else {\n        nationalites[[nationalite]] &lt;- 1\n      }\n    }\n  }\n  \n  return(unlist(nationalites))\n}\n\nresultats_nationalites &lt;- lapply(data_medailles_sport_ete$Athletes_medailles, extraire_nationalite_athlete)\n\nnationalites_agg &lt;- Reduce(`+`, resultats_nationalites)\n\n# Les 3 nationalités les plus représentées\ntop_3_nationalites &lt;- sort(nationalites_agg, decreasing = TRUE)[1:3]\n\ntop_3_nationalites\n\nCUB HUN URS \n 35  33  33 \n\n\nLe Cuba, L’Hungrie et l’Union Soviétique sont les 3 nationalités les plus représentées."
  },
  {
    "objectID": "R.html",
    "href": "R.html",
    "title": "R & Python",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n17 May 2025\n\n\nPython (NLP) : Can we distinguish AI from human writing?\n\n\nJasmine, Emma, Valorys, Yava\n\n\n\n\n07 Mar 2025\n\n\nPython: Support Vector Machines\n\n\nYava Vilar Valera\n\n\n\n\n01 Apr 2024\n\n\nPython Dash: Making interactive presentations\n\n\nYava Vilar Valera\n\n\n\n\n03 Mar 2024\n\n\nAnalyzing Olympic Games : Basic code in R\n\n\nYava Vilar Valera\n\n\n\n\n02 Mar 2024\n\n\nAnalyzing Olympic Games : Fonctions in R\n\n\nYava Vilar Valera\n\n\n\n\n01 Mar 2024\n\n\nAnalyzing Olympic Games : Reading large datasets in R\n\n\nYava Vilar Valera\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "R/TD1/TD1.html#analyzing-olympic-games-paris-2004-basic-fonctions-in-r",
    "href": "R/TD1/TD1.html#analyzing-olympic-games-paris-2004-basic-fonctions-in-r",
    "title": "Analyzing Olympic Games Paris 2004: Basic fonctions in R",
    "section": "",
    "text": "This series of exercises in R focuses on analyzing data relating to the Olympic infrastructure for the 2024 Games in Paris. We’ll be using R’s basic functions exclusively to explore, analyze and extract key information about Olympic and Paralympic venues.\n\nRead file with database, name and type of columns, number of rows\n\n\n#Lire et nommer le fichier \ndata_ex &lt;- read.csv(\"C:/Users/yavav/Downloads/paris-2024-sites-olympiques-et-paralympiques-franciliens (1).csv\", sep=\";\", dec=\",\", quote=\"\\\"\", na.strings = NA)\n\n#Noms et nature des colonnes \nstr(data_ex)\n\n'data.frame':   31 obs. of  4 variables:\n $ geo_point                     : chr  \"48.841319, 2.253076\" \"48.924388, 2.359871\" \"48.92467, 2.332428\" \"48.815115, 2.08208\" ...\n $ nom                           : chr  \"Parc des Princes\" \"Stade de France\" \"Village olympique\" \"Château de Versailles\" ...\n $ sites_olympiques_paralympiques: chr  \"Site olympique\" \"Site olympique,Site paralympique\" \"\" \"Site olympique,Site paralympique\" ...\n $ sports                        : chr  \"Football\" \"Athlétisme,Para athlétisme,Rugby\" \"\" \"Sports équestres,Para équitation,Pentathlon moderne\" ...\n\n#Nombre de lignes \nnrow(data_ex) \n\n[1] 31\n\n\nThe dataframe contains 4 columns: geo_point, olympic village, olympic_venues and sports, coded in chr (character), and31 columns.\n\nHow many Olympic venues are there?\n\n\nx &lt;- 0\n\nfor (value in data_ex$sites_olympiques_paralympiques) {\n  if (value == \"Site olympique\" | value == \"Site olympique,Site paralympique\") {\n    x &lt;- x + 1\n  } } \n\nprint(paste(\"Número total de Sites olympiques:\", x))\n\n[1] \"Número total de Sites olympiques: 26\"\n\n\n\nHow many Paralympic venues are there?\n\n\ny &lt;- 0\n\nfor (value in data_ex$sites_olympiques_paralympiques) {\n  if (value == \"Site paralympique\" | value == \"Site olympique,Site paralympique\") {\n    y &lt;- y + 1\n  } } \n\nprint(paste(\"Número total de Sites paralympiques:\", y))\n\n[1] \"Número total de Sites paralympiques: 19\"\n\n\n\nWhich sites host several sporting disciplines?\n\n\n#On cherche  les cellules de la colonne sports qui continnent des éléments séparés par un virgule, indice d'acceuil de plus d'un sport\n\nsites_plusieurs_sports &lt;- data_ex[grepl(\",\", data_ex$sports), c(\"nom\")]\nsites_plusieurs_sports\n\n [1] \"Stade de France\"                                \n [2] \"Château de Versailles\"                          \n [3] \"Grand Palais\"                                   \n [4] \"Invalides\"                                      \n [5] \"Pont d'Iéna\"                                    \n [6] \"Arena Bercy\"                                    \n [7] \"Arena Paris nord\"                               \n [8] \"Arena Paris Sud 6 (Porte de Versailles)\"        \n [9] \"Stade de la Concorde\"                           \n[10] \"Arena Champs de Mars\"                           \n[11] \"Stade Tour Eiffel\"                              \n[12] \"Arena La Chapelle\"                              \n[13] \"Centre aquatique\"                               \n[14] \"Arena Paris Sud 1 (Porte de Versailles)\"        \n[15] \"Arena Paris Sud 4 (Porte de Versailles)\"        \n[16] \"La Défense Arena\"                               \n[17] \"Stade nautique\"                                 \n[18] \"Vélodrome National de Saint-Quentin-en-Yvelines\"\n[19] \"Stade Roland Garros\"                            \n\n\n\nWhat para-Olympic disciplines are hosted at these venues in the Paris region?\n\n\ndisciplines_plusieurs_sports &lt;- data_ex[grepl(\",\", data_ex$sports), c(\"nom\", \"sports\")] \ndisciplines_plusieurs_sports\n\n                                               nom\n2                                  Stade de France\n4                            Château de Versailles\n8                                     Grand Palais\n9                                        Invalides\n11                                     Pont d'Iéna\n12                                     Arena Bercy\n13                                Arena Paris nord\n14         Arena Paris Sud 6 (Porte de Versailles)\n15                            Stade de la Concorde\n17                            Arena Champs de Mars\n18                               Stade Tour Eiffel\n20                               Arena La Chapelle\n22                                Centre aquatique\n23         Arena Paris Sud 1 (Porte de Versailles)\n24         Arena Paris Sud 4 (Porte de Versailles)\n26                                La Défense Arena\n27                                  Stade nautique\n29 Vélodrome National de Saint-Quentin-en-Yvelines\n30                             Stade Roland Garros\n                                                             sports\n2                                  Athlétisme,Para athlétisme,Rugby\n4               Sports équestres,Para équitation,Pentathlon moderne\n8                 Escrime,Escrime fauteuil,Taekwondo,Para taekwondo\n9        Tir à l'arc,Para tir à l'arc,Athlétisme,Cyclisme sur route\n11                                    Cyclisme sur route,Athlétisme\n12     Basketball,Basket fauteuil,Gymnastique artistique,trampoline\n13                Boxe,Escrime,Pentathlon moderne,Volley-ball assis\n14                                  Haltérophilie,Handball,Goalball\n15                 Basketball 3x3,BMX freestyle,Breaking,Skateboard\n17                              Judo,Para judo,Lutte,Rugby fauteuil\n18                                     Volleyball de plage,Cécifoot\n20 Badminton,Para Badminton,Gymnastique rythmique,Para powerlifting\n22                          Natation artistique,Plongeon,Water-polo\n23                                                Volleyball,Boccia\n24                             Tennis de table,Para tennis de table\n26                                Natation,Para natation,Water-polo\n27                              Canoë,Para canoë,Aviron,Para aviron\n29                       Cyclisme sur piste,Para cyclisme sur piste\n30                                      Tennis,Tennis fauteuil,Boxe\n\n\nThe resulting dataframe displays the name of the site with its associated disciplines.\n\nSites with the most followers\n\n\nmax_sports &lt;- max(sapply(strsplit(data_ex$sports, \",\"),length))\n\nfiles_max_sports &lt;- which(sapply(strsplit(data_ex$sports, \",\"),length)==max_sports)\n\ndata_max_sports &lt;- data_ex[files_max_sports, c(\"nom\", \"sports\")]\n\ndata_max_sports\n\n                    nom\n8          Grand Palais\n9             Invalides\n12          Arena Bercy\n13     Arena Paris nord\n15 Stade de la Concorde\n17 Arena Champs de Mars\n20    Arena La Chapelle\n27       Stade nautique\n                                                             sports\n8                 Escrime,Escrime fauteuil,Taekwondo,Para taekwondo\n9        Tir à l'arc,Para tir à l'arc,Athlétisme,Cyclisme sur route\n12     Basketball,Basket fauteuil,Gymnastique artistique,trampoline\n13                Boxe,Escrime,Pentathlon moderne,Volley-ball assis\n15                 Basketball 3x3,BMX freestyle,Breaking,Skateboard\n17                              Judo,Para judo,Lutte,Rugby fauteuil\n20 Badminton,Para Badminton,Gymnastique rythmique,Para powerlifting\n27                              Canoë,Para canoë,Aviron,Para aviron\n\n\nWe can see that there are a total of 8 venues hosting a total of 4 sports each.\n\nWhich discipline will take place at the greatest number of venues? What are these venues?\n\n\n# Conter la fréquence de chaque discipline\nfrequence_disciplines &lt;- table(unlist(strsplit(data_ex$sports, \",\")))\n\n# Discipline avec plus grande fréquence\ndiscipline_plus_frequente &lt;- names(frequence_disciplines)[which.max(frequence_disciplines)]\n\n# Résultat\ncat(\"La discipline qui aura lieu sur le plus grand nombre de sites est:\", discipline_plus_frequente, \"\\n\")\n\nLa discipline qui aura lieu sur le plus grand nombre de sites est: Athlétisme \n\n\n\n# Filtrer les sites qui incluent le mot \"Athlétisme\" dans la colonne Discipline\nsites_atletisme &lt;- data_ex[grepl(\"Athlétisme\", data_ex$sports, ignore.case = TRUE), c(\"nom\", \"sports\")]\n\n# Imprimer les résultats\ncat(\"Les sites où l'athlétisme aura lieu sont les suivants:\\n\")\n\nLes sites où l'athlétisme aura lieu sont les suivants:\n\nprint(sites_atletisme)\n\n                                   nom\n2                      Stade de France\n6  Terrain des Essences - La Courneuve\n9                            Invalides\n11                         Pont d'Iéna\n25             Hôtel de ville de Paris\n                                                       sports\n2                            Athlétisme,Para athlétisme,Rugby\n6                                             Para athlétisme\n9  Tir à l'arc,Para tir à l'arc,Athlétisme,Cyclisme sur route\n11                              Cyclisme sur route,Athlétisme\n25                                                 Athlétisme\n\n\nAthletics will be held at 5 venues in Paris.\n\nWhich are the two nearest venues?\n\n\n#Libraires \n\nlibrary(geosphere)\nlibrary(tidyr)\n\n#Séparer les coordonnées géographiques en deux colonnes par la virgule \n\ndata_ex &lt;- separate(data_ex, geo_point, into = c(\"lat\", \"long\"), sep = \", \")\n\n# Fonction pour calculer la distance euclidienne\n\ndata_ex$lat &lt;- as.numeric(data_ex$lat)\ndata_ex$long &lt;- as.numeric(data_ex$long)\n\ndistances &lt;- distHaversine(data_ex[, c(\"long\", \"lat\")])\nprint(distances)\n\n [1] 12109.0289  2007.5022 22017.3512 28908.6007  1204.2823 36658.5531\n [7] 26702.8086   422.2544 11450.5703 13052.7520  6674.7313 18001.9809\n[13] 22981.5082  4582.2876  8949.4822  9269.0396   436.5820 20677.9533\n[19] 27022.4177 10476.8609 12176.4034 11437.7799   419.6006  5441.5332\n[25]  9902.1865 30003.3283 42913.1029  5092.1872 16782.0752 23146.9519\n\n# Matrice pour garder les distances\ndistances &lt;- matrix(NA, nrow = nrow(data_ex), ncol = nrow(data_ex))\n\n# Calculer les distances entre toutes les coordonnées avec un boucle \nfor (i in 1:(nrow(data_ex) - 1)) {\n  for (j in (i + 1):nrow(data_ex)) {\n    distances[i, j] &lt;- distHaversine(\n      c(data_ex$lat[i], data_ex$long[i]),\n      c(data_ex$lat[j], data_ex$long[j])\n    )\n    distances[j, i] &lt;- distances[i, j]  # Symétrie\n  }\n}\n\n# Convertir la matirce en dataframe\ndistances_df &lt;- as.data.frame(distances)\n\n#Le sites les plus proches\nmin_distance &lt;- min(distances_df, na.rm=TRUE)\nmin_distance \n\n[1] 91.69166\n\n#Min distances\nmin_distannce &lt;- which(distances_df==min_distance, arr.ind=TRUE)\nmin_distannce\n\n     row col\n[1,]  29  19\n[2,]  19  29\n\n\nThe minimum distance is 91.69 km, identified by min_distance. Min_distannce identifies the row and column of the 91.69 value within the matrix. This corresponds to the names of the sites in columns 19 and 29 of the dataframe data_ex. Stade BMX de Saint-Quentin-en-Yvelines and Vélodrome National de Saint-Quentin-en-Yvelines are the nearest sites.\n\nWhich are the most distant sites\n\n\nmax_distance &lt;- max(distances_df, na.rm=TRUE)\nprint(max_distance)\n\n[1] 74882.55\n\nmax_distannce &lt;- which(distances_df==max_distance, arr.ind=TRUE)\nmax_distannce\n\n     row col\n[1,]  27   7\n[2,]   7  27\n\n\nColline d’Elancourt (7) and Stade Nautique (27) are the furthest away, with a distance of 74882.55 km.\n\nApartment at the center of gravity of all Olympic sites\n\n\n# Calculer les coordonnées moyennes (barycentriques)\nbarycentre &lt;- c(mean(data_ex$lat), mean(data_ex$long))\n\n# Trouver le site le plus proche du barycentre\nsite_proche_barycentre &lt;- data_ex[which.min(geosphere::distVincentySphere(barycentre, cbind(data_ex$lat, data_ex$long))), ]\n\nsite_proche_barycentre\n\n        lat     long               nom   sites_olympiques_paralympiques\n18 48.85723 2.296084 Stade Tour Eiffel Site olympique,Site paralympique\n                         sports\n18 Volleyball de plage,Cécifoot\n\n\nThe Eiffel Tower Stadium is at the center of all the Olympic venues."
  },
  {
    "objectID": "R/TD3/TD3.html#analyzing-olympic-games-paris-2004-reading-larga-datasets-exploration-of-dplyr-tidyr-string-lubridate-packages",
    "href": "R/TD3/TD3.html#analyzing-olympic-games-paris-2004-reading-larga-datasets-exploration-of-dplyr-tidyr-string-lubridate-packages",
    "title": "Analyzing Olympic Games Paris 2004: Reading larga datasets & exploration of dplyr, tidyr, string, lubridate packages",
    "section": "",
    "text": "This tutorial focuses on learning how to import files in formats that can handle large databases. Similarly, the dplyr, tidyr, string and lubridate libraries will be used to analyze information relating to the historic Olympic Games.\n\ninstall.packages(\"RSQLite\")\n\n\nRead parquet file with arrow library, dimension and column names\n\n\ndata_parquet &lt;- arrow::read_parquet(\"C:/Users/yavav/OneDrive - Universitat de València/tokyo_athletes.parquet\")\ndata_parquet &lt;- as.data.frame(data_parquet)\n\nView(data_parquet)\n\n\ndim(data_parquet) #Trois variables et 11085 colonnes\n\n[1] 11085     3\n\nstr(data_parquet) #Colonnes présentes: Name, NOC, Discipline\n\n'data.frame':   11085 obs. of  3 variables:\n $ Name      : chr  \"AALERUD Katrine\" \"ABAD Nestor\" \"ABAGNALE Giovanni\" \"ABALDE Alberto\" ...\n $ NOC       : chr  \"Norway\" \"Spain\" \"Italy\" \"Spain\" ...\n $ Discipline: chr  \"Cycling Road\" \"Artistic Gymnastics\" \"Rowing\" \"Basketball\" ...\n\n\n\nRead json file with jsonlite library, column dimensions and names\n\n\nchemin_acces &lt;- \"C:/Users/yavav/OneDrive - Universitat de València/olympics_athletes.json\"\n\nlibrary(jsonlite)\ndatos_json &lt;- fromJSON(chemin_acces)\ndatos_json &lt;- as.data.frame(datos_json)\n\n\ndim(datos_json) #269731 lignes et 13 colonnes\n\n[1] 269731     13\n\nstr(datos_json) #On observe le nom des différentes colonnes\n\n'data.frame':   269731 obs. of  13 variables:\n $ ID    : int  1 2 3 4 5 5 5 5 5 5 ...\n $ Name  : chr  \"A Dijiang\" \"A Lamusi\" \"Gunnar Nielsen Aaby\" \"Edgar Lindenau Aabye\" ...\n $ Sex   : chr  \"M\" \"M\" \"M\" \"M\" ...\n $ Age   : int  24 23 24 34 21 21 25 25 27 27 ...\n $ Height: int  180 170 NA NA 185 185 185 185 185 185 ...\n $ Weight: num  80 60 NA NA 82 82 82 82 82 82 ...\n $ Team  : chr  \"China\" \"China\" \"Denmark\" \"Denmark/Sweden\" ...\n $ NOC   : chr  \"CHN\" \"CHN\" \"DEN\" \"DEN\" ...\n $ Games : chr  \"1992 Summer\" \"2012 Summer\" \"1920 Summer\" \"1900 Summer\" ...\n $ City  : chr  \"Barcelona\" \"London\" \"Antwerpen\" \"Paris\" ...\n $ Sport : chr  \"Basketball\" \"Judo\" \"Football\" \"Tug-Of-War\" ...\n $ Event : chr  \"Basketball Men's Basketball\" \"Judo Men's Extra-Lightweight\" \"Football Men's Football\" \"Tug-Of-War Men's Tug-Of-War\" ...\n $ Medal : chr  NA NA NA \"Gold\" ...\n\n\n\nSQL site database connection with dbConnect\n\n\nlibrary(RSQLite)\n\ncon &lt;- dbConnect(SQLite(), dbname =\"C:/Users/yavav/Documents/R avancé et Git/data.sqlite\")\n\n\nCreate two new tables with dbListTables\n\n\ntables &lt;- dbListTables(con)\nprint(tables) ##Tables: Tokyo_medals et Tokyo_teams\n\n[1] \"C:/Users/yavav/OneDrive - Universitat de València/__MACOSX/olympics_athletes.json\"\n[2] \"olympics_athletes\"                                                                \n[3] \"tokyo_athletes\"                                                                   \n[4] \"tokyo_medals\"                                                                     \n[5] \"tokyo_teams\"                                                                      \n\n\n\nlibrary(RSQLite)\nlibrary(DBI)\ndbWriteTable(con, \"tokyo_athletes\", data_parquet, overwrite=TRUE)\ndbWriteTable(con, \"olympics_athletes\", datos_json, overwrite=TRUE)\n\n\nInspect the olympics_athletes table using the dbListFields function.\n\n\ncolonnes_olympics_athletes &lt;- dbListFields(con, \"olympics_athletes\")\n\n# Afficher la liste des colonnes\nprint(colonnes_olympics_athletes)\n\n [1] \"ID\"     \"Name\"   \"Sex\"    \"Age\"    \"Height\" \"Weight\" \"Team\"   \"NOC\"   \n [9] \"Games\"  \"City\"   \"Sport\"  \"Event\"  \"Medal\" \n\ncolonnes_tokyo_athletes &lt;- dbListFields(con, \"tokyo_athletes\")\n\n# Afficher la liste des colonnes\nprint(colonnes_tokyo_athletes)\n\n[1] \"Name\"       \"NOC\"        \"Discipline\"\n\n\n\nImport this table from the database using the dbReadTable function. Convert table to tibble.\n\n\n##Olympics\n\nlibrary(tibble)\n\n#Lire la table depuis la base de données\ntable_olympics_athletes &lt;- dbReadTable(con, \"olympics_athletes\")\n\n# Convertir la table en tibble\ntibble_olympics_athletes &lt;- as_tibble(table_olympics_athletes)\n\n##Athletes\n\n#Lire la table depuis la base de données\ntable_tokyo_athletes &lt;- dbReadTable(con, \"tokyo_athletes\")\n\n# Convertir la table en tibble\ntibble_tokyo_athletes &lt;- as_tibble(table_tokyo_athletes)"
  },
  {
    "objectID": "R/TD3/TD3.html#dplyr-tidyr-packages",
    "href": "R/TD3/TD3.html#dplyr-tidyr-packages",
    "title": "Analyzing Olympic Games : Reading large datasets in R",
    "section": "Dplyr, tidyr packages",
    "text": "Dplyr, tidyr packages\n\nConvert the Sex column into a categorical variable using the mutate function\n\n\n#Importation des libraires \nlibrary(dplyr)\nlibrary(tidyr)\n\n\ntibble_olympics_athletes &lt;- tibble_olympics_athletes %&gt;% mutate(Sex = as.factor(Sex))\n\n\nCreate two columns from the Games column. The first column Year will be an integer column containing the year of the games. The second column isSummer will be a Boolean column indicating whether the games are summer or winter games.\n\n\nhelp(\"separate_wider_delim\")\n\nseparate_wider_delim(tibble_olympics_athletes, cols = Games, delim= \" \", names=c(\"Year\", \"isSummer\"), cols_remove = FALSE)\n\n# A tibble: 269,731 × 15\n      ID Name   Sex     Age Height Weight Team  NOC   Year  isSummer Games City \n   &lt;int&gt; &lt;chr&gt;  &lt;fct&gt; &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;\n 1     1 A Dij… M        24    180     80 China CHN   1992  Summer   1992… Barc…\n 2     2 A Lam… M        23    170     60 China CHN   2012  Summer   2012… Lond…\n 3     3 Gunna… M        24     NA     NA Denm… DEN   1920  Summer   1920… Antw…\n 4     4 Edgar… M        34     NA     NA Denm… DEN   1900  Summer   1900… Paris\n 5     5 Chris… F        21    185     82 Neth… NED   1988  Winter   1988… Calg…\n 6     5 Chris… F        21    185     82 Neth… NED   1988  Winter   1988… Calg…\n 7     5 Chris… F        25    185     82 Neth… NED   1992  Winter   1992… Albe…\n 8     5 Chris… F        25    185     82 Neth… NED   1992  Winter   1992… Albe…\n 9     5 Chris… F        27    185     82 Neth… NED   1994  Winter   1994… Lill…\n10     5 Chris… F        27    185     82 Neth… NED   1994  Winter   1994… Lill…\n# ℹ 269,721 more rows\n# ℹ 3 more variables: Sport &lt;chr&gt;, Event &lt;chr&gt;, Medal &lt;chr&gt;\n\n\n\nAverage age of athletes for each edition of the Summer Olympics.\n\n\nany(is.na(tibble_olympics_athletes$Age)) #Ce code permet de vérifier la présence des NA's au sein de la colonne Age\n\n[1] TRUE\n\ntibble_olympics_athletes %&gt;% group_by(Games) %&gt;%\n  summarise(mean_age=mean(Age,na.rm=TRUE)) #Ce code calcule la moyenne d'age pour chacune des éditions des JO, en excluant les valeurs manquantes du calcul. \n\n# A tibble: 51 × 2\n   Games       mean_age\n   &lt;chr&gt;          &lt;dbl&gt;\n 1 1896 Summer     23.6\n 2 1900 Summer     28.9\n 3 1904 Summer     26.7\n 4 1906 Summer     27.1\n 5 1908 Summer     27.0\n 6 1912 Summer     27.5\n 7 1920 Summer     29.3\n 8 1924 Summer     28.1\n 9 1924 Winter     27.6\n10 1928 Summer     28.0\n# ℹ 41 more rows\n\n\n\nEdition who counted with the youngest athletes? The oldest?\n\n\ntibble_olympics_athletes %&gt;% group_by(Games) %&gt;%\n  summarise(mean_age=mean(Age,na.rm=TRUE)) %&gt;% \n  arrange(mean_age) %&gt;% slice(1) #Cela permet d'ordonner de manière croissante les moyennes d'âge ainsi que de sélectionner la première ligne (les plus jeunes)\n\n# A tibble: 1 × 2\n  Games       mean_age\n  &lt;chr&gt;          &lt;dbl&gt;\n1 1984 Winter     23.4\n\ntibble_olympics_athletes %&gt;% group_by(Games) %&gt;%\n  summarise(mean_age=mean(Age,na.rm=TRUE)) %&gt;% \n  arrange(desc(mean_age)) %&gt;% slice(1) #Cela ordonne de manière décroissante pour trouver les plus âgés \n\n# A tibble: 1 × 2\n  Games       mean_age\n  &lt;chr&gt;          &lt;dbl&gt;\n1 1932 Summer     30.2\n\n\nIt can be seen that the edition with the youngest athletes was the 1984 Winter Games, while the one with the oldest was the 1932 Summer Games.\n\nWhich summer Olympic discipline has the largest number of female athletes?\n\n\ntibble_olympics_athletes %&gt;% filter(Sex==\"F\") %&gt;% arrange(desc(Height)) %&gt;% slice(1) %&gt;% select(Height, Sport) #On filtre les femmes, on ordonne et sélectionne la ligne ainsi que les variables d'intérêt. \n\n# A tibble: 1 × 2\n  Height Sport     \n   &lt;int&gt; &lt;chr&gt;     \n1    213 Basketball\n\n\nThe discipline with the largest number of female athletes is basketball.\n\nWhat has been this discipline in each of the editions?\n\n\ntibble_olympics_athletes %&gt;% filter(Sex==\"F\" & !is.na(Height)) %&gt;% group_by(Games) %&gt;% \n  arrange(desc(Height)) %&gt;% slice(1) %&gt;% select(Games, Height, Sport) #On filtre pour les femmes et pour la taille sans valeurs manquantes, puis on calcule la taille plus grande pour chaque game et on sélectionne la colonne du sport correspondant. \n\n# A tibble: 45 × 3\n# Groups:   Games [45]\n   Games       Height Sport         \n   &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;         \n 1 1920 Summer    175 Diving        \n 2 1924 Summer    175 Fencing       \n 3 1924 Winter    165 Figure Skating\n 4 1928 Summer    183 Fencing       \n 5 1928 Winter    165 Figure Skating\n 6 1932 Summer    183 Fencing       \n 7 1932 Winter    168 Figure Skating\n 8 1936 Summer    183 Fencing       \n 9 1936 Winter    168 Figure Skating\n10 1948 Summer    180 Athletics     \n# ℹ 35 more rows\n\n\n\nNumber of times each discipline has been the discipline with the greatest athletes\n\n\ntibble_olympics_athletes %&gt;% filter(Sex==\"F\" & !is.na(Height)) %&gt;% group_by(Games) %&gt;% \n  arrange(desc(Height)) %&gt;% slice(1) %&gt;% select(Games, Height, Sport) %&gt;% pull(Sport) %&gt;% table() #On extrait la colonne sport et fait un tablau de fréquence. \n\n.\n       Alpine Skiing            Athletics           Basketball \n                   3                    5                    9 \n           Bobsleigh Cross Country Skiing               Diving \n                   3                    4                    1 \n             Fencing       Figure Skating           Ice Hockey \n                   4                    4                    1 \n                Luge        Speed Skating             Swimming \n                   3                    4                    1 \n          Volleyball \n                   3"
  },
  {
    "objectID": "R/TD3/TD3.html#stringr-lubridate-packages",
    "href": "R/TD3/TD3.html#stringr-lubridate-packages",
    "title": "Analyzing Olympic Games : Reading large datasets in R",
    "section": "Stringr, lubridate packages",
    "text": "Stringr, lubridate packages\nFollowing the text below :\n\ntexte &lt;- \"Les jeux olympiques d’été se déroulent normalement tous les 4 ans, durant les mois de Juillet et Août. Les jeux de Rio ont eu lieu du 5 Août 2016 au 20 Août 2016, ceux de Tokyo du 23 Juillet 2021 au 8 Août 2021, et ceux de Paris auront lieu du 26 Juillet 2024 au 11 Août 2024. Plus de 10000 athlètes sont attendus du monde\nentier à Paris.\"\n\n\nExtract the dates of the various editions of the Olympic Games\n\n\n#Chargement de librairie \nlibrary(stringr)\n\n\nstr_extract_all(texte, \"\\\\b(\\\\d{1,2} \\\\w+ \\\\d{4})\\\\b\")\n\n[[1]]\n[1] \"5 Août 2016\"     \"20 Août 2016\"    \"23 Juillet 2021\" \"8 Août 2021\"    \n[5] \"26 Juillet 2024\" \"11 Août 2024\"   \n\n\n\nReplace the names of the months with their numbers to make these elements convertible into dates using the str_replace function.\n\n\nstr_replace_all(texte, c(\"Juillet\"=\"7\", \"Août\"=\"8\"))\n\n[1] \"Les jeux olympiques d’été se déroulent normalement tous les 4 ans, durant les mois de 7 et 8. Les jeux de Rio ont eu lieu du 5 8 2016 au 20 8 2016, ceux de Tokyo du 23 7 2021 au 8 8 2021, et ceux de Paris auront lieu du 26 7 2024 au 11 8 2024. Plus de 10000 athlètes sont attendus du monde\\nentier à Paris.\"\n\n\n\nlibrary(lubridate)\n\n\npatron_dates &lt;- \"\\\\b(\\\\d{1,2} \\\\w+ \\\\d{4})\\\\b\"\n\n# Extraire et convertir toutes les dates\ndates &lt;- str_extract_all(texte, patron_dates) %&gt;%\n  lapply(function(matches) {\n    dmy(matches)\n  })\n\n#Imprimer le résultat\nprint(dates)\n\n[[1]]\n[1] \"2016-08-05\" \"2016-08-20\" \"2021-07-23\" \"2021-08-08\" \"2024-07-26\"\n[6] \"2024-08-11\"\n\n\n\nHow many days separated the Rio and Tokyo editions? And will the Tokyo and Paris editions be separated? Do the same calculation in weeks.\n\n\n##Différence de jours entre les éditions Rio et Tokyo \n\nRio_Tokyo &lt;- difftime(dates[[1]][3], dates[[1]][2], units = \"days\") #Diff entre le troisième et deuxième élément de la liste dates en jour\nprint(Rio_Tokyo)\n\nTime difference of 1798 days\n\n##Différence de jours entre les éditions Tokyo et Paris\n\nTokyo_Paris &lt;- difftime(dates[[1]][5], dates[[1]][4], units = \"days\") #Diff entre le cinquième et sixième élément de la liste dates en jour\nprint(Tokyo_Paris)\n\nTime difference of 1083 days\n\n##Différence de semaines entre les éditions Rio et Tokyo\n\nRio_Tokyo_semaines &lt;- difftime(dates[[1]][3], dates[[1]][2], units = \"weeks\") #Diff entre le troisième et deuxième élément de la liste dates en semaines\nprint(Rio_Tokyo_semaines)\n\nTime difference of 256.8571 weeks\n\n##Différence de semaines entre les éditions Tokyo et Paris\n\nTokyo_Paris_semaines &lt;- difftime(dates[[1]][5], dates[[1]][4], units = \"weeks\") #Diff entre le cinquième et sixième élément de la liste dates en semaines\nprint(Tokyo_Paris_semaines)\n\nTime difference of 154.7143 weeks"
  },
  {
    "objectID": "R/TD1/TD1.html#analyzing-olympic-games-paris-2004-basic-code-in-r",
    "href": "R/TD1/TD1.html#analyzing-olympic-games-paris-2004-basic-code-in-r",
    "title": "Analyzing Olympic Games Paris 2004: Basic code in R",
    "section": "",
    "text": "This series of exercises in R focuses on analyzing data relating to the Olympic infrastructure for the 2024 Games in Paris. We’ll be using R’s basic functions exclusively to explore, analyze and extract key information about Olympic and Paralympic venues.\n\nRead file with database, name and type of columns, number of rows\n\n\n#Lire et nommer le fichier \ndata_ex &lt;- read.csv(\"C:/Users/yavav/Downloads/paris-2024-sites-olympiques-et-paralympiques-franciliens (1).csv\", sep=\";\", dec=\",\", quote=\"\\\"\", na.strings = NA)\n\n#Noms et nature des colonnes \nstr(data_ex)\n\n'data.frame':   31 obs. of  4 variables:\n $ geo_point                     : chr  \"48.841319, 2.253076\" \"48.924388, 2.359871\" \"48.92467, 2.332428\" \"48.815115, 2.08208\" ...\n $ nom                           : chr  \"Parc des Princes\" \"Stade de France\" \"Village olympique\" \"Château de Versailles\" ...\n $ sites_olympiques_paralympiques: chr  \"Site olympique\" \"Site olympique,Site paralympique\" \"\" \"Site olympique,Site paralympique\" ...\n $ sports                        : chr  \"Football\" \"Athlétisme,Para athlétisme,Rugby\" \"\" \"Sports équestres,Para équitation,Pentathlon moderne\" ...\n\n#Nombre de lignes \nnrow(data_ex) \n\n[1] 31\n\n\nThe dataframe contains 4 columns: geo_point, olympic village, olympic_venues and sports, coded in chr (character), and31 columns.\n\nHow many Olympic venues are there?\n\n\nx &lt;- 0\n\nfor (value in data_ex$sites_olympiques_paralympiques) {\n  if (value == \"Site olympique\" | value == \"Site olympique,Site paralympique\") {\n    x &lt;- x + 1\n  } } \n\nprint(paste(\"Número total de Sites olympiques:\", x))\n\n[1] \"Número total de Sites olympiques: 26\"\n\n\n\nHow many Paralympic venues are there?\n\n\ny &lt;- 0\n\nfor (value in data_ex$sites_olympiques_paralympiques) {\n  if (value == \"Site paralympique\" | value == \"Site olympique,Site paralympique\") {\n    y &lt;- y + 1\n  } } \n\nprint(paste(\"Número total de Sites paralympiques:\", y))\n\n[1] \"Número total de Sites paralympiques: 19\"\n\n\n\nWhich sites host several sporting disciplines?\n\n\n#On cherche  les cellules de la colonne sports qui continnent des éléments séparés par un virgule, indice d'acceuil de plus d'un sport\n\nsites_plusieurs_sports &lt;- data_ex[grepl(\",\", data_ex$sports), c(\"nom\")]\nsites_plusieurs_sports\n\n [1] \"Stade de France\"                                \n [2] \"Château de Versailles\"                          \n [3] \"Grand Palais\"                                   \n [4] \"Invalides\"                                      \n [5] \"Pont d'Iéna\"                                    \n [6] \"Arena Bercy\"                                    \n [7] \"Arena Paris nord\"                               \n [8] \"Arena Paris Sud 6 (Porte de Versailles)\"        \n [9] \"Stade de la Concorde\"                           \n[10] \"Arena Champs de Mars\"                           \n[11] \"Stade Tour Eiffel\"                              \n[12] \"Arena La Chapelle\"                              \n[13] \"Centre aquatique\"                               \n[14] \"Arena Paris Sud 1 (Porte de Versailles)\"        \n[15] \"Arena Paris Sud 4 (Porte de Versailles)\"        \n[16] \"La Défense Arena\"                               \n[17] \"Stade nautique\"                                 \n[18] \"Vélodrome National de Saint-Quentin-en-Yvelines\"\n[19] \"Stade Roland Garros\"                            \n\n\n\nWhat para-Olympic disciplines are hosted at these venues in the Paris region?\n\n\ndisciplines_plusieurs_sports &lt;- data_ex[grepl(\",\", data_ex$sports), c(\"nom\", \"sports\")] \ndisciplines_plusieurs_sports\n\n                                               nom\n2                                  Stade de France\n4                            Château de Versailles\n8                                     Grand Palais\n9                                        Invalides\n11                                     Pont d'Iéna\n12                                     Arena Bercy\n13                                Arena Paris nord\n14         Arena Paris Sud 6 (Porte de Versailles)\n15                            Stade de la Concorde\n17                            Arena Champs de Mars\n18                               Stade Tour Eiffel\n20                               Arena La Chapelle\n22                                Centre aquatique\n23         Arena Paris Sud 1 (Porte de Versailles)\n24         Arena Paris Sud 4 (Porte de Versailles)\n26                                La Défense Arena\n27                                  Stade nautique\n29 Vélodrome National de Saint-Quentin-en-Yvelines\n30                             Stade Roland Garros\n                                                             sports\n2                                  Athlétisme,Para athlétisme,Rugby\n4               Sports équestres,Para équitation,Pentathlon moderne\n8                 Escrime,Escrime fauteuil,Taekwondo,Para taekwondo\n9        Tir à l'arc,Para tir à l'arc,Athlétisme,Cyclisme sur route\n11                                    Cyclisme sur route,Athlétisme\n12     Basketball,Basket fauteuil,Gymnastique artistique,trampoline\n13                Boxe,Escrime,Pentathlon moderne,Volley-ball assis\n14                                  Haltérophilie,Handball,Goalball\n15                 Basketball 3x3,BMX freestyle,Breaking,Skateboard\n17                              Judo,Para judo,Lutte,Rugby fauteuil\n18                                     Volleyball de plage,Cécifoot\n20 Badminton,Para Badminton,Gymnastique rythmique,Para powerlifting\n22                          Natation artistique,Plongeon,Water-polo\n23                                                Volleyball,Boccia\n24                             Tennis de table,Para tennis de table\n26                                Natation,Para natation,Water-polo\n27                              Canoë,Para canoë,Aviron,Para aviron\n29                       Cyclisme sur piste,Para cyclisme sur piste\n30                                      Tennis,Tennis fauteuil,Boxe\n\n\nThe resulting dataframe displays the name of the site with its associated disciplines.\n\nSites with the most followers\n\n\nmax_sports &lt;- max(sapply(strsplit(data_ex$sports, \",\"),length))\n\nfiles_max_sports &lt;- which(sapply(strsplit(data_ex$sports, \",\"),length)==max_sports)\n\ndata_max_sports &lt;- data_ex[files_max_sports, c(\"nom\", \"sports\")]\n\ndata_max_sports\n\n                    nom\n8          Grand Palais\n9             Invalides\n12          Arena Bercy\n13     Arena Paris nord\n15 Stade de la Concorde\n17 Arena Champs de Mars\n20    Arena La Chapelle\n27       Stade nautique\n                                                             sports\n8                 Escrime,Escrime fauteuil,Taekwondo,Para taekwondo\n9        Tir à l'arc,Para tir à l'arc,Athlétisme,Cyclisme sur route\n12     Basketball,Basket fauteuil,Gymnastique artistique,trampoline\n13                Boxe,Escrime,Pentathlon moderne,Volley-ball assis\n15                 Basketball 3x3,BMX freestyle,Breaking,Skateboard\n17                              Judo,Para judo,Lutte,Rugby fauteuil\n20 Badminton,Para Badminton,Gymnastique rythmique,Para powerlifting\n27                              Canoë,Para canoë,Aviron,Para aviron\n\n\nWe can see that there are a total of 8 venues hosting a total of 4 sports each.\n\nWhich discipline will take place at the greatest number of venues? What are these venues?\n\n\n# Conter la fréquence de chaque discipline\nfrequence_disciplines &lt;- table(unlist(strsplit(data_ex$sports, \",\")))\n\n# Discipline avec plus grande fréquence\ndiscipline_plus_frequente &lt;- names(frequence_disciplines)[which.max(frequence_disciplines)]\n\n# Résultat\ncat(\"La discipline qui aura lieu sur le plus grand nombre de sites est:\", discipline_plus_frequente, \"\\n\")\n\nLa discipline qui aura lieu sur le plus grand nombre de sites est: Athlétisme \n\n\n\n# Filtrer les sites qui incluent le mot \"Athlétisme\" dans la colonne Discipline\nsites_atletisme &lt;- data_ex[grepl(\"Athlétisme\", data_ex$sports, ignore.case = TRUE), c(\"nom\", \"sports\")]\n\n# Imprimer les résultats\ncat(\"Les sites où l'athlétisme aura lieu sont les suivants:\\n\")\n\nLes sites où l'athlétisme aura lieu sont les suivants:\n\nprint(sites_atletisme)\n\n                                   nom\n2                      Stade de France\n6  Terrain des Essences - La Courneuve\n9                            Invalides\n11                         Pont d'Iéna\n25             Hôtel de ville de Paris\n                                                       sports\n2                            Athlétisme,Para athlétisme,Rugby\n6                                             Para athlétisme\n9  Tir à l'arc,Para tir à l'arc,Athlétisme,Cyclisme sur route\n11                              Cyclisme sur route,Athlétisme\n25                                                 Athlétisme\n\n\nAthletics will be held at 5 venues in Paris.\n\nWhich are the two nearest venues?\n\n\n#Libraires \n\nlibrary(geosphere)\nlibrary(tidyr)\n\n#Séparer les coordonnées géographiques en deux colonnes par la virgule \n\ndata_ex &lt;- separate(data_ex, geo_point, into = c(\"lat\", \"long\"), sep = \", \")\n\n# Fonction pour calculer la distance euclidienne\n\ndata_ex$lat &lt;- as.numeric(data_ex$lat)\ndata_ex$long &lt;- as.numeric(data_ex$long)\n\ndistances &lt;- distHaversine(data_ex[, c(\"long\", \"lat\")])\nprint(distances)\n\n [1] 12109.0289  2007.5022 22017.3512 28908.6007  1204.2823 36658.5531\n [7] 26702.8086   422.2544 11450.5703 13052.7520  6674.7313 18001.9809\n[13] 22981.5082  4582.2876  8949.4822  9269.0396   436.5820 20677.9533\n[19] 27022.4177 10476.8609 12176.4034 11437.7799   419.6006  5441.5332\n[25]  9902.1865 30003.3283 42913.1029  5092.1872 16782.0752 23146.9519\n\n# Matrice pour garder les distances\ndistances &lt;- matrix(NA, nrow = nrow(data_ex), ncol = nrow(data_ex))\n\n# Calculer les distances entre toutes les coordonnées avec un boucle \nfor (i in 1:(nrow(data_ex) - 1)) {\n  for (j in (i + 1):nrow(data_ex)) {\n    distances[i, j] &lt;- distHaversine(\n      c(data_ex$lat[i], data_ex$long[i]),\n      c(data_ex$lat[j], data_ex$long[j])\n    )\n    distances[j, i] &lt;- distances[i, j]  # Symétrie\n  }\n}\n\n# Convertir la matirce en dataframe\ndistances_df &lt;- as.data.frame(distances)\n\n#Le sites les plus proches\nmin_distance &lt;- min(distances_df, na.rm=TRUE)\nmin_distance \n\n[1] 91.69166\n\n#Min distances\nmin_distannce &lt;- which(distances_df==min_distance, arr.ind=TRUE)\nmin_distannce\n\n     row col\n[1,]  29  19\n[2,]  19  29\n\n\nThe minimum distance is 91.69 km, identified by min_distance. Min_distannce identifies the row and column of the 91.69 value within the matrix. This corresponds to the names of the sites in columns 19 and 29 of the dataframe data_ex. Stade BMX de Saint-Quentin-en-Yvelines and Vélodrome National de Saint-Quentin-en-Yvelines are the nearest sites.\n\nWhich are the most distant sites\n\n\nmax_distance &lt;- max(distances_df, na.rm=TRUE)\nprint(max_distance)\n\n[1] 74882.55\n\nmax_distannce &lt;- which(distances_df==max_distance, arr.ind=TRUE)\nmax_distannce\n\n     row col\n[1,]  27   7\n[2,]   7  27\n\n\nColline d’Elancourt (7) and Stade Nautique (27) are the furthest away, with a distance of 74882.55 km.\n\nApartment at the center of gravity of all Olympic sites\n\n\n# Calculer les coordonnées moyennes (barycentriques)\nbarycentre &lt;- c(mean(data_ex$lat), mean(data_ex$long))\n\n# Trouver le site le plus proche du barycentre\nsite_proche_barycentre &lt;- data_ex[which.min(geosphere::distVincentySphere(barycentre, cbind(data_ex$lat, data_ex$long))), ]\n\nsite_proche_barycentre\n\n        lat     long               nom   sites_olympiques_paralympiques\n18 48.85723 2.296084 Stade Tour Eiffel Site olympique,Site paralympique\n                         sports\n18 Volleyball de plage,Cécifoot\n\n\nThe Eiffel Tower Stadium is at the center of all the Olympic venues."
  },
  {
    "objectID": "R/Python/Python Dash.html",
    "href": "R/Python/Python Dash.html",
    "title": "Python Dash: Making interactive presentations",
    "section": "",
    "text": "Python Dash, developed by Plotly, allows us to create interactive web applications and visualize data. Below you will find an example of a coded model using a given database on the sales of a company alongside its resulting outputs, which, when executed with Python, provide interactive and dynamic graphics.\n\nimport dash \nfrom dash import Input, Output, html, callback, dcc\nimport dash_bootstrap_components as dbc\nimport plotly.graph_objects as go\nimport pandas as pd\nimport numpy as np\nfrom calendar import month_abbr, month_name\nimport plotly.express as px\nfrom dash import dash_table\n\n#Télécharger la base de données \ndf = pd.read_csv(\"C:/Users/yavav/Downloads/data (1).csv\", index_col=0)\n\n#Traitement des données\ndf = df[['CustomerID', 'Gender', 'Location', 'Product_Category', 'Quantity', 'Avg_Price', 'Transaction_Date', 'Month', 'Discount_pct']]\n\ndf['CustomerID'] = df['CustomerID'].fillna(0).astype(int)\ndf['Transaction_Date'] = pd.to_datetime(df['Transaction_Date'])\n\ndf['Total_price'] = df['Quantity'] * df['Avg_Price'] * (1 - (df['Discount_pct'] / 100)).round(3)\n\n#Création de la table des 100 dernières ventes\ndef table_ventes(data):\n    table_ventes = data[['Transaction_Date', 'Gender', 'Location', 'Product_Category', 'Quantity', 'Avg_Price', 'Discount_pct']].copy()\n    table_ventes['Date'] = table_ventes['Transaction_Date'].dt.date\n    table_ventes = table_ventes[['Date', 'Gender', 'Location', 'Product_Category', 'Quantity', 'Avg_Price', 'Discount_pct']]\n    table_ventes = table_ventes.sort_values(by='Date', ascending=False).head(100)\n    return table_ventes.to_dict('records')\n   \ncolonnes = [\n    {'name': 'Transaction_Date', 'id': 'Transaction_Date'},\n    {'name': 'Gender', 'id': 'Gender'},\n    {'name': 'Location', 'id': 'Location'},\n    {'name': 'Product_Category', 'id': 'Product Category'},\n    {'name': 'Quantity', 'id': 'Quantity'},\n    {'name': 'Avg_Price', 'id': 'Avg_Price'},\n    {'name': 'Discount_pct', 'id': 'Discount_pct'}\n]\n\n#Définition des fonctions qui seront ensuites appelées\ndef calculer_chiffre_affaire(data):\n    return data['Total_price'].sum()\n\ndef frequence_meilleure_vente(data, top=10, ascending=False):\n    resultat = pd.crosstab(\n        [data['Gender'], data['Product_Category']], \n        'Total vente', \n        values=data['Total_price'], \n        aggfunc= lambda x : len(x), \n        rownames=['Sexe', 'Categorie du produit'],\n        colnames=['']\n    ).reset_index().groupby(\n        ['Sexe'], as_index=False, group_keys=True\n    ).apply(\n        lambda x: x.sort_values('Total vente', ascending=ascending).iloc[:top, :]\n    ).reset_index(drop=True).set_index(['Sexe', 'Categorie du produit'])\n\n    return resultat\n\ndef indicateur_du_mois(data, current_month = 12, freq=True, abbr=False): \n    previous_month = current_month - 1 if current_month &gt; 1 else 12\n    if freq : \n        resultat = data['Month'][(data['Month'] == current_month) | (data['Month'] == previous_month)].value_counts()\n        # sort by index\n        resultat = resultat.sort_index()\n        resultat.index = [(month_abbr[i] if abbr else month_name[i]) for i in resultat.index]\n        return resultat\n    else:\n        resultat = data[(data['Month'] == current_month) | (data['Month'] == previous_month)].groupby('Month').apply(calculer_chiffre_affaire)\n        resultat.index = [(month_abbr[i] if abbr else month_name[i]) for i in resultat.index]\n        return resultat\n\n# Barplot top 10 ventes\ndef barplot_top_10_ventes(data) :\n    df_plot = frequence_meilleure_vente(data, ascending=True)\n    graph = px.bar(\n        df_plot,\n        x='Total vente', \n        y=df_plot.index.get_level_values(1),\n        color=df_plot.index.get_level_values(0), \n        barmode='group',\n        title=\"Frequence des 10 meilleures ventes\",\n        labels={\"x\": \"Fréquence\", \"y\": \"Categorie du produit\", \"color\": \"Sexe\"},\n        width=680, height=600\n    ).update_layout(\n        margin = dict(t=60)\n    )\n    return graph\n   \n# Evolution chiffre d'affaire\ndef plot_evolution_chiffre_affaire(data) :\n    df_plot = data.groupby(pd.Grouper(key='Transaction_Date', freq='W')).apply(calculer_chiffre_affaire)[:-1]\n    chiffre_evolution = px.line(\n        x=df_plot.index, y=df_plot,\n        title=\"Evolution du chiffre d'affaire par semaine\",\n        labels={\"x\": \"Semaine\", \"y\": \"Chiffre d'affaire\"},\n    ).update_layout( \n        width=1000, height=400,\n        margin=dict(t=60, b=0),\n        \n    )\n    return chiffre_evolution\n\n## Chiffre d'affaire du mois\ndef plot_chiffre_affaire_mois(data) :\n    df_plot = indicateur_du_mois(data, freq=False)\n    indicateur = go.Figure(\n        go.Indicator(\n            mode = \"number+delta\",\n            value = df_plot[1],\n            delta = {'reference': df_plot[0]},\n            domain = {'row': 0, 'column': 1},\n            title=f\"{df_plot.index[1]}\",\n        )\n    ).update_layout(\n        width=200, height=200, \n        margin=dict(l=0, r=20, t=20, b=0)\n    )\n    return indicateur\n\n# Ventes du mois\ndef plot_vente_mois(data, abbr=False) :\n    df_plot = indicateur_du_mois(data, freq=True, abbr=abbr)\n    indicateur = go.Figure(\n        go.Indicator(\n            mode = \"number+delta\",\n            value = df_plot[1],\n            delta = {'reference': df_plot[0]},\n            domain = {'row': 0, 'column': 1},\n            title=f\"{df_plot.index[1]}\",\n        )\n    ).update_layout( \n        width=200, height=200, \n        margin=dict(l=0, r=20, t=20, b=0)\n    )\n    return indicateur\n\n###La strcuture de la maquette \n\napp=dash.Dash(external_stylesheets=[dbc.themes.BOOTSTRAP])\n\nNEW_STYLE = {\n    \"display\": \"flex\",\n    \"justify-content\": \"center\",\n    \"align-items\": \"center\",\n    \"margin-bottom\": \"10px\", \n}\n\napp.layout = html.Div(children=[\n\n    #header \n    dbc.Row(children=[\n        dbc.Col(width=6, children=[\n            html.H1(children='ECAP Store'),\n        ]),\n        dbc.Col(width=6, children=[\n                    dcc.Dropdown(\n                        id='Choississez des zones',\n                        multi=True,\n                        options=[{'value' : loc, 'label' : loc} for loc in df['Location'].dropna().unique()],\n                        placeholder=\"Choississez des zones\",\n                        style={'width': '100%', 'margin':  '10px 0'},\n            ),\n        ]),\n    ],  \n            style = {'background-color':'lightblue', 'margin-bottom':'30px'}),\n    \n\n    #Body\n    dbc.Row(children=[\n\n    #left column\n    dbc.Col(width=6, children=[\n\n        #Row of indicators\n        dbc.Row(children=[\n            dbc.Col(width=6, children=[\n                html.Div(children=[\n                    dcc.Graph(id='ind-1')\n                ]),\n                ],\n                style={**NEW_STYLE, \n                  }\n            ),\n            dbc.Col(width=6, children=[\n                html.Div(children=[\n                    dcc.Graph(id='ind-2')\n                ]),\n            ],\n                style={\n                    **NEW_STYLE,\n                }\n            )\n        ]),\n\n        #Row of barplot\n        dbc.Row(children=[\n            html.Div(children=[\n                dcc.Graph(id=\"ind-3\")\n            ]),\n        ]),\n    ]),  \n\n    # Right column\n    dbc.Col(width=6, children=[\n\n        # Row of indicators\n        dbc.Row(children=[\n            html.Div(children=[\n                dcc.Graph(id=\"ind-4\")\n            ]),\n        ], style={\n            **NEW_STYLE,\n            'min-height': '300px',\n            'margin': '5px'\n        }),\n\n        # Row of barplot\n        dbc.Row(children=[\n            html.H3(\"Table des 100 dernières ventes\", style={'textAlign': 'left', 'fontSize': '18px'}),\n            html.Div(dash_table.DataTable(id=\"ind-5\",\n                                          page_size=10,\n                                          style_table={'width': '625px', 'height': '350px', 'overflowY': 'auto'},\n                                          style_header={'backgroundColor': 'lightgrey'},  \n                                          style_cell={'textAlign': 'right'},\n                                         )),\n        ]),\n    ]),\n    ])\n    ])\n\n#Callbacks\n@callback(\n    Output(\"ind-1\", \"figure\"),\n    Output(\"ind-2\", \"figure\"),\n    Output(\"ind-3\", \"figure\"),\n    Output(\"ind-4\", \"figure\"),\n    Output(\"ind-5\", \"data\"),\n    Input(\"Choississez des zones\", \"value\"),\n)\n\n#Mettre à jour les figures en fonction de la zone \ndef update_indicators(locations):\n    df_filtered = df[df['Location'].isin(locations)] if locations else df\n    fig_chiffre_affaire = plot_chiffre_affaire_mois(df_filtered)\n    fig_ventes = plot_vente_mois(df_filtered)\n    bar_ventes = barplot_top_10_ventes(df_filtered)\n    evolution_affaires = plot_evolution_chiffre_affaire(df_filtered)\n    table_data = table_ventes(df_filtered)\n    \n    #Modifier la taille des figures\n    fig_chiffre_affaire.update_layout(width=160, height=160)\n    fig_ventes.update_layout(width=160, height=160)\n    bar_ventes.update_layout(width=550, height=500)\n    evolution_affaires.update_layout(width=675, height=275)\n\n    return fig_chiffre_affaire, fig_ventes, bar_ventes, evolution_affaires, table_data\n    \n\nif __name__ == '__main__': \n    app.run(debug=True, port=8051, jupyter_mode=\"external\")\n\nOutput:"
  },
  {
    "objectID": "posts/Penalized regressions/Penalized Regressions.html",
    "href": "posts/Penalized regressions/Penalized Regressions.html",
    "title": "Penalized Regressions: Analysis of Employment Growth Rate in Canada, 1981-2024",
    "section": "",
    "text": "In many practical scenarios, managing large datasets becomes essential. To effectively handle these, advanced techniques beyond traditional econometric methods are required. Dimension reduction approaches and penalized regressions, aimed at conducting variable selection, are able to address this issue and provide confident and interpretable results. This study explores all these methods using a high-dimensional database containing economic indicators for Canada. The outcome of interest is the employment rate, with more than 200 variables representing its potential determinants. Results have shown similar predictors deemed relevant across penalized regressions, with unemployment rate values and lagged employment levels as the primary determinants.\nBelow you will find my project and you can also click here."
  },
  {
    "objectID": "about me.html#personal-interests",
    "href": "about me.html#personal-interests",
    "title": "Yava Vilar Valera",
    "section": "Personal interests",
    "text": "Personal interests\nhola hola"
  },
  {
    "objectID": "Arbres de décision.html",
    "href": "Arbres de décision.html",
    "title": "About the blog",
    "section": "",
    "text": "import pandas as pd                 # for data manipulation and analysis\nimport numpy as np                  # for mathematical operations\nimport plotly.express as px         # for data visualization\nimport seaborn as sns               # for data visualization\nimport matplotlib.pyplot as plt     # for data visualization\n\n\ndata = pd.read_csv(\"C:/Users/yavav/Downloads/horse.csv\")\n\n\ndata.head\n\n&lt;bound method NDFrame.head of     surgery    age  hospital_number  rectal_temp  pulse  respiratory_rate  \\\n0        no  adult           530101         38.5   66.0              28.0   \n1       yes  adult           534817         39.2   88.0              20.0   \n2        no  adult           530334         38.3   40.0              24.0   \n3       yes  young          5290409         39.1  164.0              84.0   \n4        no  adult           530255         37.3  104.0              35.0   \n..      ...    ...              ...          ...    ...               ...   \n294     yes  adult           533886          NaN  120.0              70.0   \n295      no  adult           527702         37.2   72.0              24.0   \n296     yes  adult           529386         37.5   72.0              30.0   \n297     yes  adult           530612         36.5  100.0              24.0   \n298     yes  adult           534618         37.2   40.0              20.0   \n\n    temp_of_extremities peripheral_pulse mucous_membrane  \\\n0                  cool          reduced             NaN   \n1                   NaN              NaN   pale_cyanotic   \n2                normal           normal       pale_pink   \n3                  cold           normal   dark_cyanotic   \n4                   NaN              NaN   dark_cyanotic   \n..                  ...              ...             ...   \n294                cold              NaN   pale_cyanotic   \n295                cool        increased   pale_cyanotic   \n296                cold          reduced   pale_cyanotic   \n297                cool          reduced       pale_pink   \n298                 NaN              NaN             NaN   \n\n    capillary_refill_time  ... packed_cell_volume total_protein  \\\n0              more_3_sec  ...               45.0           8.4   \n1              less_3_sec  ...               50.0          85.0   \n2              less_3_sec  ...               33.0           6.7   \n3              more_3_sec  ...               48.0           7.2   \n4              more_3_sec  ...               74.0           7.4   \n..                    ...  ...                ...           ...   \n294            more_3_sec  ...               55.0          65.0   \n295            more_3_sec  ...               44.0           NaN   \n296            less_3_sec  ...               60.0           6.8   \n297            less_3_sec  ...               50.0           6.0   \n298                   NaN  ...               36.0          62.0   \n\n    abdomo_appearance abdomo_protein     outcome  surgical_lesion lesion_1  \\\n0                 NaN            NaN        died               no    11300   \n1              cloudy            2.0  euthanized               no     2208   \n2                 NaN            NaN       lived               no        0   \n3       serosanguious            5.3        died              yes     2208   \n4                 NaN            NaN        died               no     4300   \n..                ...            ...         ...              ...      ...   \n294               NaN            NaN  euthanized               no     3205   \n295     serosanguious            3.3  euthanized              yes     2208   \n296               NaN            NaN        died              yes     3205   \n297     serosanguious            3.4       lived              yes     2208   \n298             clear            1.0  euthanized               no     6112   \n\n    lesion_2  lesion_3  cp_data  \n0          0         0       no  \n1          0         0       no  \n2          0         0      yes  \n3          0         0      yes  \n4          0         0       no  \n..       ...       ...      ...  \n294        0         0       no  \n295        0         0      yes  \n296        0         0       no  \n297        0         0      yes  \n298        0         0       no  \n\n[299 rows x 28 columns]&gt;\n\n\n\ndata.shape\n\n(299, 28)\n\n\n\ndata.dtypes\n\nsurgery                   object\nage                       object\nhospital_number            int64\nrectal_temp              float64\npulse                    float64\nrespiratory_rate         float64\ntemp_of_extremities       object\nperipheral_pulse          object\nmucous_membrane           object\ncapillary_refill_time     object\npain                      object\nperistalsis               object\nabdominal_distention      object\nnasogastric_tube          object\nnasogastric_reflux        object\nnasogastric_reflux_ph    float64\nrectal_exam_feces         object\nabdomen                   object\npacked_cell_volume       float64\ntotal_protein            float64\nabdomo_appearance         object\nabdomo_protein           float64\noutcome                   object\nsurgical_lesion           object\nlesion_1                   int64\nlesion_2                   int64\nlesion_3                   int64\ncp_data                   object\ndtype: object\n\n\n\ndata.info\n\n&lt;bound method DataFrame.info of     surgery    age  hospital_number  rectal_temp  pulse  respiratory_rate  \\\n0        no  adult           530101         38.5   66.0              28.0   \n1       yes  adult           534817         39.2   88.0              20.0   \n2        no  adult           530334         38.3   40.0              24.0   \n3       yes  young          5290409         39.1  164.0              84.0   \n4        no  adult           530255         37.3  104.0              35.0   \n..      ...    ...              ...          ...    ...               ...   \n294     yes  adult           533886          NaN  120.0              70.0   \n295      no  adult           527702         37.2   72.0              24.0   \n296     yes  adult           529386         37.5   72.0              30.0   \n297     yes  adult           530612         36.5  100.0              24.0   \n298     yes  adult           534618         37.2   40.0              20.0   \n\n    temp_of_extremities peripheral_pulse mucous_membrane  \\\n0                  cool          reduced             NaN   \n1                   NaN              NaN   pale_cyanotic   \n2                normal           normal       pale_pink   \n3                  cold           normal   dark_cyanotic   \n4                   NaN              NaN   dark_cyanotic   \n..                  ...              ...             ...   \n294                cold              NaN   pale_cyanotic   \n295                cool        increased   pale_cyanotic   \n296                cold          reduced   pale_cyanotic   \n297                cool          reduced       pale_pink   \n298                 NaN              NaN             NaN   \n\n    capillary_refill_time  ... packed_cell_volume total_protein  \\\n0              more_3_sec  ...               45.0           8.4   \n1              less_3_sec  ...               50.0          85.0   \n2              less_3_sec  ...               33.0           6.7   \n3              more_3_sec  ...               48.0           7.2   \n4              more_3_sec  ...               74.0           7.4   \n..                    ...  ...                ...           ...   \n294            more_3_sec  ...               55.0          65.0   \n295            more_3_sec  ...               44.0           NaN   \n296            less_3_sec  ...               60.0           6.8   \n297            less_3_sec  ...               50.0           6.0   \n298                   NaN  ...               36.0          62.0   \n\n    abdomo_appearance abdomo_protein     outcome  surgical_lesion lesion_1  \\\n0                 NaN            NaN        died               no    11300   \n1              cloudy            2.0  euthanized               no     2208   \n2                 NaN            NaN       lived               no        0   \n3       serosanguious            5.3        died              yes     2208   \n4                 NaN            NaN        died               no     4300   \n..                ...            ...         ...              ...      ...   \n294               NaN            NaN  euthanized               no     3205   \n295     serosanguious            3.3  euthanized              yes     2208   \n296               NaN            NaN        died              yes     3205   \n297     serosanguious            3.4       lived              yes     2208   \n298             clear            1.0  euthanized               no     6112   \n\n    lesion_2  lesion_3  cp_data  \n0          0         0       no  \n1          0         0       no  \n2          0         0      yes  \n3          0         0      yes  \n4          0         0       no  \n..       ...       ...      ...  \n294        0         0       no  \n295        0         0      yes  \n296        0         0       no  \n297        0         0      yes  \n298        0         0       no  \n\n[299 rows x 28 columns]&gt;\n\n\n\ndata.describe\n\n&lt;bound method NDFrame.describe of     surgery    age  hospital_number  rectal_temp  pulse  respiratory_rate  \\\n0        no  adult           530101         38.5   66.0              28.0   \n1       yes  adult           534817         39.2   88.0              20.0   \n2        no  adult           530334         38.3   40.0              24.0   \n3       yes  young          5290409         39.1  164.0              84.0   \n4        no  adult           530255         37.3  104.0              35.0   \n..      ...    ...              ...          ...    ...               ...   \n294     yes  adult           533886          NaN  120.0              70.0   \n295      no  adult           527702         37.2   72.0              24.0   \n296     yes  adult           529386         37.5   72.0              30.0   \n297     yes  adult           530612         36.5  100.0              24.0   \n298     yes  adult           534618         37.2   40.0              20.0   \n\n    temp_of_extremities peripheral_pulse mucous_membrane  \\\n0                  cool          reduced             NaN   \n1                   NaN              NaN   pale_cyanotic   \n2                normal           normal       pale_pink   \n3                  cold           normal   dark_cyanotic   \n4                   NaN              NaN   dark_cyanotic   \n..                  ...              ...             ...   \n294                cold              NaN   pale_cyanotic   \n295                cool        increased   pale_cyanotic   \n296                cold          reduced   pale_cyanotic   \n297                cool          reduced       pale_pink   \n298                 NaN              NaN             NaN   \n\n    capillary_refill_time  ... packed_cell_volume total_protein  \\\n0              more_3_sec  ...               45.0           8.4   \n1              less_3_sec  ...               50.0          85.0   \n2              less_3_sec  ...               33.0           6.7   \n3              more_3_sec  ...               48.0           7.2   \n4              more_3_sec  ...               74.0           7.4   \n..                    ...  ...                ...           ...   \n294            more_3_sec  ...               55.0          65.0   \n295            more_3_sec  ...               44.0           NaN   \n296            less_3_sec  ...               60.0           6.8   \n297            less_3_sec  ...               50.0           6.0   \n298                   NaN  ...               36.0          62.0   \n\n    abdomo_appearance abdomo_protein     outcome  surgical_lesion lesion_1  \\\n0                 NaN            NaN        died               no    11300   \n1              cloudy            2.0  euthanized               no     2208   \n2                 NaN            NaN       lived               no        0   \n3       serosanguious            5.3        died              yes     2208   \n4                 NaN            NaN        died               no     4300   \n..                ...            ...         ...              ...      ...   \n294               NaN            NaN  euthanized               no     3205   \n295     serosanguious            3.3  euthanized              yes     2208   \n296               NaN            NaN        died              yes     3205   \n297     serosanguious            3.4       lived              yes     2208   \n298             clear            1.0  euthanized               no     6112   \n\n    lesion_2  lesion_3  cp_data  \n0          0         0       no  \n1          0         0       no  \n2          0         0      yes  \n3          0         0      yes  \n4          0         0       no  \n..       ...       ...      ...  \n294        0         0       no  \n295        0         0      yes  \n296        0         0       no  \n297        0         0      yes  \n298        0         0       no  \n\n[299 rows x 28 columns]&gt;\n\n\n\ncolonnes = ['surgery', 'age', 'temp_of_extremities', 'peripheral_pulse', 'mucous_membrane', 'capillary_refill_time', 'pain', 'peristalsis', 'abdominal_distention', 'nasogastric_tube', 'nasogastric_reflux', 'rectal_exam_feces', 'abdomen', 'abdomo_appearance', 'outcome', 'surgical_lesion', 'cp_data']\nfor colonne in colonnes: \n    data[colonne]=data[colonne].astype('category')\n\n\ndata.dtypes\n\nsurgery                  category\nage                      category\nhospital_number             int64\nrectal_temp               float64\npulse                     float64\nrespiratory_rate          float64\ntemp_of_extremities      category\nperipheral_pulse         category\nmucous_membrane          category\ncapillary_refill_time    category\npain                     category\nperistalsis              category\nabdominal_distention     category\nnasogastric_tube         category\nnasogastric_reflux       category\nnasogastric_reflux_ph     float64\nrectal_exam_feces        category\nabdomen                  category\npacked_cell_volume        float64\ntotal_protein             float64\nabdomo_appearance        category\nabdomo_protein            float64\noutcome                  category\nsurgical_lesion          category\nlesion_1                    int64\nlesion_2                    int64\nlesion_3                    int64\ncp_data                  category\ndtype: object\n\n\n\ndata['surgery'].value_counts\n\n&lt;bound method IndexOpsMixin.value_counts of 0       no\n1      yes\n2       no\n3      yes\n4       no\n      ... \n294    yes\n295     no\n296    yes\n297    yes\n298    yes\nName: surgery, Length: 299, dtype: category\nCategories (2, object): ['no', 'yes']&gt;\n\n\n\nsns.countplot(data, x='surgery')\n\nC:\\Users\\yavav\\anaconda3\\Lib\\site-packages\\seaborn\\categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n\n\n&lt;Axes: xlabel='surgery', ylabel='count'&gt;\n\n\n\n\n\n\npx.bar(data, x='surgery')\n\n\n                                                \n\n\n\ndata.hist(column='hospital_number')\n\narray([[&lt;Axes: title={'center': 'hospital_number'}&gt;]], dtype=object)\n\n\n\n\n\n\nsns.pairplot(data)\n\nC:\\Users\\yavav\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\nC:\\Users\\yavav\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\nC:\\Users\\yavav\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\nC:\\Users\\yavav\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\nC:\\Users\\yavav\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\nC:\\Users\\yavav\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\nC:\\Users\\yavav\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\nC:\\Users\\yavav\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\nC:\\Users\\yavav\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\nC:\\Users\\yavav\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\nC:\\Users\\yavav\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\n\n\n\n\nsns.set_theme(style='white')\n\ncorr = data.corr(numeric_only=True)\n\nmask = np.triu(np.ones_like(corr,dtype=bool))\n\nfig, ax = plt.subplots(figsize=(20,20))\n\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n&lt;Axes: &gt;\n\n\n\n\n\n\npd.crosstab(data['surgical_lesion'], data['temp_of_extremities'], normalize='index')\n\n\n\n\n\n\n\ntemp_of_extremities\ncold\ncool\nnormal\nwarm\n\n\nsurgical_lesion\n\n\n\n\n\n\n\n\nno\n0.077778\n0.311111\n0.411111\n0.200000\n\n\nyes\n0.130719\n0.522876\n0.267974\n0.078431\n\n\n\n\n\n\n\n\nfig = px.histogram(data, x=\"surgical_lesion\", color=\"surgery\", marginal=\"rug\", # can be `box`, `violin`\n                         hover_data=data.columns)\nfig.show()\n\nC:\\Users\\yavav\\anaconda3\\Lib\\site-packages\\plotly\\express\\_core.py:1958: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n                                                \n\n\n\ndata.boxplot(rot=270)\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "posts/Panel/Panel.html",
    "href": "posts/Panel/Panel.html",
    "title": "Assessing the Empirical Validity of the Kuznets Environmental Curve",
    "section": "",
    "text": "This study examines the empirical validity of the Environmental Kuznets Curve for Latin America and the Caribbean region over the period 2002–2020. The curve explores the relationship between economic growth and environmental degradation, suggesting that in the early stages of economic development, environmental pollution tends to increase. However, once a given region reaches a certain level of economic wealth, environmental quality begins to improve.\nBy comparing different panel models, our final fixed-effects model (accounting for both country and year) confirms the Kuznets hypothesis in our study context.\nYou can access our project here and below, written in French."
  },
  {
    "objectID": "posts/Séries temporelles multivariées/Séries multivariées.html",
    "href": "posts/Séries temporelles multivariées/Séries multivariées.html",
    "title": "Analyzing the Determinants of Real Estate Prices in France",
    "section": "",
    "text": "This study analyzes the evolution of the house price index in France between 2001 and 2019, using a structural variable (SVAR) model. The motivation stems from growing concerns about the sustained rise in house prices, which has been higher than consumer price growth in OECD countries. In France, the house price index has increased significantly, from 80.5 in 2000 to 131.7 in 2022, generating significant economic and social effects, such as greater inequality in access to housing between different economic strata, rising cost of living and reduced household consumption, and greater population indebtedness.\nIn response to this problem, the study aims to identify the macroeconomic determinants that influence house prices in France at the national level, analyzing how structural shocks to key variables affect their evolution.\nBelow you can find a summary of the data, methodology and key findings, and you can access the whole document here."
  },
  {
    "objectID": "posts/Séries temporelles multivariées/Séries multivariées.html#data",
    "href": "posts/Séries temporelles multivariées/Séries multivariées.html#data",
    "title": "Analyzing the Determinants of Real Estate Prices in France",
    "section": "Data",
    "text": "Data\nQuarterly data from 2001 to 2019 were used from INSEE and the Crédit Logement/CSA Observatory. The three variables selected for the analysis were:\n\nHouse Price Index (HPI): Represents the index of new and existing housing prices in metropolitan France.\nMortgage Credit Rate (Tx_CIP): Captures the financing costs for home purchases.\nGross Disposable Income (GDI): Represents the amount of income available to households after taxes and social security contributions.\n\n\nData treatment\nA stationarity test was performed using the Dickey-Fuller (DFT) test, revealing that the housing price index and gross disposable income are non-stationary series and were transformed into growth rates for the analysis. Similarly, an outlier was detected in the PPI in the fourth quarter of 2016, but it was considered valid because it represents an economic event and not a measurement mistake. Finally, the presence of seasonality in the series was ruled out."
  },
  {
    "objectID": "posts/Séries temporelles multivariées/Séries multivariées.html#data-treatment",
    "href": "posts/Séries temporelles multivariées/Séries multivariées.html#data-treatment",
    "title": "Real estate prices in France",
    "section": "Data treatment",
    "text": "Data treatment\nA stationarity test was performed using the Dickey-Fuller (DFT) test, revealing that the housing price index and gross disposable income are non-stationary series and were transformed into growth rates for the analysis. Similarly, an outlier was detected in the PPI in the fourth quarter of 2016, but it was considered valid because it represents an economic event and not a measurement mistake. Finally, the presence of seasonality in the series was ruled out."
  },
  {
    "objectID": "posts/Séries temporelles multivariées/Séries multivariées.html#methodology",
    "href": "posts/Séries temporelles multivariées/Séries multivariées.html#methodology",
    "title": "Analyzing the Determinants of Real Estate Prices in France",
    "section": "Methodology",
    "text": "Methodology\nTo analyze the relationship between variables, a structural VAR model (SVAR) was used. This model allows to identify the causal impact of economic shocks on housing prices and decompose the effects of each variable on the others using the Cholesky decomposition.\nTo identify shocks in the SVAR model, the following hierarchical order of variables was established based on economic theory:\n\nGross disposable income (GDI): This is considered the most exogenous variable, as it directly affects housing demand and credit, but is not immediately impacted by other variables.\nMortgage lending rate (Tx_CIP): This is affected by household income, but does not immediately influence it.\nHouse Price Index (HPI): The most endogenous variable, as it responds to changes in credit and income, but does not immediately affect them."
  },
  {
    "objectID": "posts/Séries temporelles multivariées/Séries multivariées.html#results",
    "href": "posts/Séries temporelles multivariées/Séries multivariées.html#results",
    "title": "Analyzing the Determinants of Real Estate Prices in France",
    "section": "Results",
    "text": "Results\n\nGenger causality test\nTo start with, we applied the Genger causality test, which allows us to determine whether a time series helps predict another one, evaluating whether past values ​​of a variable X contain useful information for predicting future values ​​of another variable Y, beyond the information already provided by variable Y itself. If so, X is said to “Granger-cause” Y. The results have shown that:\n\nGrowth in the house price index Genger causes changes in the mortgage lending rate and gross disposable income.\nThe mortgage lending rate causes changes in house prices and gross disposable income.\nGross disposable income does not cause house prices or the lending rate.\nThe lending rate and disposable income jointly cause the house price index.\n\n\n\n\nShock Response Functions\nImpulse Response Functions (IRFs) analyze how one variable responds to an unexpected change in another within a dynamic system. When an exogenous shock occurs, such as a sudden increase in interest rates, IRFs show how this disturbance affects other variables, such as housing prices. Through graphs, these functions reveal the magnitude, duration, and direction of the impact. When confidence intervals do not contain zero, the impact is significant.\nThe Shock Response Funtions of our study are available in figure 1 and show that a positive shock to gross disposable income increases the growth of the house price index briefly and temporarily (approximately two quarters). On the contrary, a positive shock to the mortgage lending rate persistently reduces the growth of the house price index, with a significant negative impact for seven quarters. Finally, a shock to the house price index generates a positive impact on its own growth for up to five quarters, indicating an inertia effect in the market.\n                                     Figure 1: Shock Response Functions\n\n\n\nVariance Decomposition of Forecast Errors\nFinally, the variance decomposition of forecast errors allows us to understand how much of the future variability of a variable is due to shocks in itself and how much is explained by shocks in other variables. Figure 2 reveals that, in the short term (horizon 1), the growth of the house price index is explained by 95% of its own history. In the long term (horizon 10), the mortgage loan rate explains up to 33% of the variability in the house price index, while gross disposable income has a much smaller impact (6%). It is noteworthy that the impact of income and the housing price index itself diminishes over time, while mortgage credit becomes progressively more important in the early periods.\n                                     Figure 2: Variance Decomposition of Forecast Errors"
  },
  {
    "objectID": "posts/Séries temporelles multivariées/Séries multivariées.html#conclusion",
    "href": "posts/Séries temporelles multivariées/Séries multivariées.html#conclusion",
    "title": "Analyzing the Determinants of Real Estate Prices in France",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, the study highlights the crucial role of monetary policy and interest rates, as well as the lesser influence of household income, in regulating the French real estate market. It is particularly recommended that interest rate developments be considered as a mechanism to control inflation in the housing sector and avoid real estate bubbles that could lead to economic crises."
  },
  {
    "objectID": "R/SVM Python/SVM code.html",
    "href": "R/SVM Python/SVM code.html",
    "title": "Python: Support Vector Machines",
    "section": "",
    "text": "An SVM (Support Vector Machine) model is a supervised machine learning algorithm used primarily for classification tasks. It works by finding a hyperplane that best separates data into distinct classes. SVMs aim to maximize the margin between the closest points of the different classes, called support vectors. The algorithm is effective in high-dimensional spaces and is particularly useful for tasks where data is not linearly separable.\nThe notebook that you will find below focuses on the explainability of Support Vector Machines (SVMs) through practical exercises. It discusses methods for interpreting SVM models, including techniques to visualize decision boundaries, assess feature importance, and understand how input features influence predictions.\nhttps://github.com/YavaVilar/SVM/blob/main/td2_explicabilite.ipynb"
  },
  {
    "objectID": "Writings/stay abroad.html",
    "href": "Writings/stay abroad.html",
    "title": "Being a student far away: what doesn’t fit in a suitcase",
    "section": "",
    "text": "When I decided to study abroad, I packed my suitcases with great enthusiasm, happy to be able to complete a journey that I had long awaited. I calculated what I would need, I packed clothes, blankets, personal belongings, books. Knowing what to leave and what to pack was almost an art: I should not overpack, but nothing could be missing either.\nWhat I didn’t know was everything that would come with me without being packed. Everything that cannot be measured or weighed.\nIt is the feelings, the moments, the people.\nThe illusion and curiosity was what most characterized my beginning, before the first flight. I was nervous, intrigued by the new, I felt strong, capable, and with a lot of determination.\nThen came the first flight. I had the privilege of being accompanied by my parents, until they left. Then came the first cry. I was alone in front of the new, a new country. No more living under their shelter and protection. Maybe it was the pain of love, union and separation at the same time.\nSoon I overcame that first break with the familiar, although new forms of farewell awaited me later.\nNew challenges. Studies, it was difficult, I was a perfectionist. Friendships, there were many from my culture, I got along with others. The rain, I liked the sun.\nI can’t fit in my suitcase the times I felt strong for solving something, alone. For creating a friendship. For passing an exam. For knowing how to ask questions.\nThere is no room for the moments of doubt, of emptiness. Nor the moments of clarity and purpose. I had many clear things, that stopped making sense, until they gained strength again.\nOne year passed, and another one came. Another one in which I would never fit in my suitcase the great number of people I met, who touched my soul. Those who let themselves be known, and who helped me to let myself be known.\nThe suitcase is too small for the immensity that they brought me. The parties, the dances, the hugs, the love.\nMy thoughts in double language do not enter, nor the links I created with my faculty, nor the days of companionship classes. With them, with people of the country. Nor the endless jobs that, as well as restlessness, brought me a lot of learning.\nThe last year has arrived. And I’m still here, with no room for each additional day, full of a new adventure, a new discovery, a new emotion.\nAnxiety, nostalgia, uncertainty, fear and illusion. They are feelings that could not enter, that first day, and that will not be able to enter either, in the return. But they are always present.\nThe people, the experiences, the bumps, the mistakes, the learning, the progress. All this characterizes my stay abroad, and will always be present, even if they don’t check it at the airport, even if they don’t understand it when I return, even if sometimes I don’t even know how to explain it.\nMy suitcase will come back lighter, but not me."
  },
  {
    "objectID": "Posts.html",
    "href": "Posts.html",
    "title": "Writings",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "Writings/Stay abroad/stay abroad.html",
    "href": "Writings/Stay abroad/stay abroad.html",
    "title": "Being a student far away: what doesn’t fit in a suitcase",
    "section": "",
    "text": "When I decided to study abroad, I packed my suitcases with great enthusiasm, happy to be able to complete a journey that I had long awaited.\nI calculated what I would need, I packed clothes, blankets, personal belongings, books. Knowing what to leave and what to pack was almost an art: I should not overpack, but nothing could be missing either.\nWhat I didn’t know was everything that would come with me without being packed. Everything that cannot be measured or weighed.\nIt is the feelings, the moments, the people.\nThe illusion and curiosity was what most characterized my beginning, before the first flight. I was nervous, intrigued by the new, I felt strong, capable, and with a lot of determination.\nThen came the first flight. I had the privilege of being accompanied by my parents, until they left. The moment when the first cry came. I was alone in front of the new, a new country. No more living under their shelter and protection. Maybe it was the pain of love, union and separation at the same time.\nSoon I overcame that first break with the familiar, although new forms of farewell awaited me later.\nNew challenges. Studies, it was difficult, I was perfectionist. Friendships, there were many from my culture, I got along with others. The rain, I liked the sun.\nI can’t fit in my suitcase the times I felt strong for solving something, alone. For creating a friendship. For passing an exam. For knowing how to ask questions.\nThere is no room for the moments of doubt, of emptiness. Nor the moments of clarity and purpose. I had many clear things, that stopped making sense, until they gained strength again.\nOne year passed, and another one came. Another one in which I would never fit in my suitcase the great number of people I met, who touched my soul. Those who let themselves be known, and who helped me let myself be known.\nThe suitcase is too small for the immensity that they brought me. The parties, the dances, the hugs, the love.\nMy thoughts in double language do not enter, nor the links I created with my faculty, nor the days of companionship classes. With them, with people of the country. Nor the endless projects that, as well as restlessness, brought me a lot of learning.\nThe last year has arrived. And I’m still here, with no room for each additional day, full of a new adventure, a new discovery, a new emotion.\nAnxiety, nostalgia, uncertainty, fear and illusion. These are feelings that could not enter, that first day, and that will not be able to enter either, in the return. But they are always present.\nThe people, the experiences, the bumps, the mistakes, the learning, the progress. All this characterizes my stay abroad, and will always come with me, even if they don’t check it at the airport, even if they don’t understand it when I return, even if sometimes I don’t even know how to explain it.\nMy suitcase will come back lighter, but not me."
  },
  {
    "objectID": "posts/Penalized regressions/prévision et conjocture/prev et conj.html",
    "href": "posts/Penalized regressions/prévision et conjocture/prev et conj.html",
    "title": "Forecasting: Evolution of the Spanish Population",
    "section": "",
    "text": "Introduction\nSince the beginning of humanity, world’s population has been continously developing and growing. From a wide range of biological to economic causes that shape it, the population follows a trajectory of great relevance to analyze since this trajectory has numerous implications in the economic and social structure, in addition to other areas as important such as the environment and biodiversity. On the one hand, economic growth has historically driven population growth while a greater number of people stimulates demand for goods and services, employment, and economic activity in general. On the other hand, the limitation of available resources and environmental degradation seems to be greater as the population continues to grow. This work focuses on making prediction tests about the population of Spain based on historical data going from 1277 to 1838, in order find a suitable model that could be used in a real context.\n\n\n1. Data\nData source comes from databases provided by “La Fundación Real del Pino”, a private non-profit organization located in Madrid, Spain. Founded in 2006, its main goal is to promote research in diverse social sciences fields, particularly in economics and economic history. The series is composed of 561 years, plus 12 years that will be reserved to compare model-predictions with actual values. It is important to note that data before 1750 has been constructed from mathematical equations and models, which may not capture the complexity of the phenomenon they seek to represent. Therefore, careful attention is required when interpreting the results of this analysis. The original series is available in Figure 1.\n                                       Figure 1: Total Population in Spain, 1277-1838\n\nAn adjusted series to correct for one Level-Shift (LS) outlier has been constructed. Outliers are atypical, extreme values that can affect the quality of model fit and the reliability of predictions. In this case, the outlier detected in the year 1508 reflects a change of the population’s trajectory from a significant slowdown driven by the expulsion of the Jews from the country to a rapid and natural recovery.\nSimilarly, a differentiation has been applied to the original series in order to meet the stationarity hypotheses (constant mean and variance over time), essential to be able to model the data. The final series corrected from outliers and differenciated is available in Figure 2.\n                                       Figure 2: Adjusted series, 1277-1838\n\n\n\n2. Linear predictions\n\n2.1. Model’s parameters\nIn order to analyze temporal series and make future predictions, it is essential to previously estimate models that capture the dependence parameters of past values and other predictor variables. The main assumption of linear models is that the relationship between variables is linear. The models’ parameters that have been monopolized are summarized in Table 1 and explained next:\n\nTable 1: Model’s parameters\n\n\n\n\n\n\n\n\n\n\n\n\nAR(1)\nARIMA(0,2,2)\nHolt-Winters\nADAM-ETS\nSSARIMA\nCES\n\n\n\n\nAR1\n0.4820\n\n\n\n\n\n\n\nMA1\n\n-0.5602\n\n\n0.5009\n\n\n\nMA2\n\n-0.4167\n\n\n0.2134\n\n\n\nMA3\n\n\n\n\n0.0755\n\n\n\nAlpha\n\n\n1\n1\n\n\n\n\nBeta\n\n\n0.4401\n0.9999\n\n\n\n\nPhi\n\n\n\n1\n\n\n\n\nDistribution\nNormal\nNormal\n\nNormal Generalised\nNormal\nPartial\n\n\nAIC\n- 1367.96\n\n-1344.38\n-3395.009\n-1367.81\n-1231.55\n\n\nAICc\n\n\n-1344.33\n-3394.748\n-1367.70\n-1231.50\n\n\n\n\nAR(1): describes how the current value of a variable is influenced by its immediately preceding value and a stochastic error term. The parameter of this model is 0.4820. A value lower than 1 indicates a stationary series that tends to revert its mean, that is, that tends to converge towards a mean value in the long term. The positiveness of the coefficient reveals an increase of each current value as long as time goes up. Indeed, a positive term AR(1) implies that when the value of the time series t-1 is greater than its mean, the value at time t is also greater than its mean, suggesting a pattern of growth over time.\nARIMA: It combines three components: autoregression (AR), differencing (I for Integrated), and moving averages (MA). The AR part of the model uses the relationship between an observation and a number of lagged observations (previous time points). p is the number of lagged observations included in the model. The I component involves differencing the data to make it stationary. The parameter d indicates the number of times the series is differenciated. The MA part of the model uses the relationship between an observation and a residual error from a moving average model applied to lagged observations. The parameter q is the number of lagged forecast errors in the prediction equation. Our model is characterized by having two parameters Moving Average processus significant at the 1% level. This shows a dependency relationship between the value at a specific point of time t and error term both at the immediate previous period, t-1, and at the second previous period t-2. Negative coefficients indicates this relationship is negative and the greater coefficient of MA(1) compared to MA(2) suggests that the error term at t-1 has a higher effect on the current value.\nHolt-Winters: Appeared in the 1960s, Holt-Winters method is a prognostic technique that can capture both tendence and seasonality in data, with the advantage of being able to adapt to no-seasonal series. Updates to the level, trend, and seasonality components are based on three smoothing parameters: α, β, and γ, where α (alpha) controls the smoothing of the level component, β (beta) controls the trend component, and γ (gamma) the seasonality component. As the series analyzed on this work does not have a seasonal component, Holt-Winters model has been estimated in R specifying an unexistent gamma parameter. In the instances when α and β parameters are close to 1, we talk about a “smooth” forecast, while if they are close to 0, they reflect a “strong smoothing”. In the first case, recent past has a big weight on forecasting. In the second case, a higher importance is given to distant past. In this case, we could say near past has a bigger weight than remote past, because α is completely equal to 1 and β coefficient is relatively important.\nADAM-ETS: The ADAM-ETS method is an approach that combines exponential smoothing (ETS) approaches with state space models to model time series with error, trend, and seasonality components, also allowing the inclusion of other covariates. The smoothing coefficient alpha controls the relative importance of past observations in estimating the current level in the ETS model. In this case, the estimated value of alpha is 1.0, specifying that full weight is given to the most recent observation in the calculation of the current level. Beta captures the relevance of diƯerences between past observations in estimating the current trend in the ETS model, which has a value of 0.9999. This indicates that diƯerences between past observations have a very important weight in approximating the current trend. Phi represents the smoothing coefficient associated with the trend. A value equal to 1 suggests a complete weight to recent observations when estimating the trend, which means that the trend will adjust more quickly to recent changes in the data.\nSSARIMA: They are a prediction technique that combines the State Space with ARIMA methodology for the analysis and forecasting of temporal data, whose acronym meaning is: “State Space ARIMA”. In this way, these models can include ARIMA components to model the time structure of the series, as well as additional state space components to shape other unobservable factors that can influence the time series. Auto.sarima function in R preconised an ARIMA(0,0,3) model with a constant. We can see in Table 1 that the weight of each previous period MA decreases with the higher number of parameters. That is, the influence of the most distant periods is smaller compared to the closest ones. Indeed, coefficient of MA(1) is 0,5 while that of MA(3) equals 0,075.\nCES: Proposed by Svetunkov, Kourentzes and Ord in 2022, CES models refer to “Complex Exponential Smoothing” models. They are an extension of traditional exponential lysing methods that are based on the theory of complex variables and oƯer an innovative way to model and forecast time series. They can predict both stationary and non-stationary series, and can be adapted to seasonal data through their formulation in a state space form. Auto.ces function of R software estimated a CES(n) model, where n represents the non-seasonality term.\n\nAdditionally, AIC (Akaike Information Criterion) and AICc (Corrected Information Criterion) have been included in order to compare models and be able to determine which one should be prioritized if we factor in this criterion. The AIC seeks to minimize the discrepancy between the model and the data, penalizing the complexity of the model. This means that a good model will be one that has a good fit but with a minimum number of parameters. The AICc is a correction to the AIC, especially useful when the sample size is small compared to the number of parameters.\nADAM-ETS offers the lowest AIC and AICc. This is the reason why this model is preferred to others if we consider this criterion. However, the differences of AIC and AICc between the different methods are small, meaning they have a similar quality in terms of their ability to fit the data and their complexity.\n\n\n2.2. Model’s predictions\nOnce the model’s parameters are defined, we can proceed to make predictions. For this, nine years of the original series had been kept with the aim to compare them with the forecast generated by the models. Figure 3 shows the forecast evolution of these nine years for each of the model along with actual data, going from 1839 to 1848. A naive prediction has also been included, which is a simple forecasting approach based on the assumption that the future value of a time series will be equal to the last observed value in the series. In other words, this method assumes there is no change in the time series and that the future value will be equal to the last observed one. Thus, this approach is useful as a reference to determine whether more complex models provide any significant prediction improvement.\n                                       Figure 3: Model’s predictions\n\n\n\n\n\nThe models that seem to better fit the real series are Holt-Winters, ARIMA and CES models, because they are the ones that have predicted a higher increase from 1838 to 1847. We observe the preference of ARIMA, which only contained MA components, compared to models with only an AR component. This suggests that the time series is more influenced by past errors than by past values. That is, the data may exhibit erratic or random behaviour, where future values depend primarily on past errors and not on a clear trend or pattern. Likewise, we can observe that Holt-Winters and CES methods have managed to fit the data relatively well. Indeed, they are designed to capture the trend and seasonality of the series (that in this case, there is no seasonality). The series grows rapidly over time, with a clear increasing trend, and these models, in particular Holt-Winters, are the ones, with ARIMA, that have the most predicted a pronounced increase with a higher growth rate. It appears that ADAM-ETS, on the contrary, even if it had the lowest AIC and AICc, presents higher error rates.\n\n\n\n3. Prediction evaluation\nTo evaluate the model’s performance, we are going to use four evaluation metrics:\n\nMean Squated Errors (MSE) : a statistical measure that provides a measure of how well the model achieves this goal. Indeed, this metric offers a quantitative assessment of the accuracy of a prediction model relative to the actual values. It is based on the difference between model predictions and true observations, squared to penalize larger errors, and then the average of these squared differences is calculated.\nDiebold-Mariano (DM) test: a statistical test used to compare the predictive accuracy of two time series models. This test was proposed by Francis X. Diebold and Robert S. Mariano in 1995, whose purpose is to determine whether there is a significant difference in the predictive performance between two models. The null hypothesis is that both models have the same predictive accuracy, while the alternative hypothesis is that at least one of the models offers a more accurate forecast. It will be used to compare each of the models with the naive forecast, which helps determine if the model in question is more interesting than making a simple prediction.\n𝑅 OOS: a measure of the predictive ability of a model when applied to new data that were not used to train the model. In essence, it measures how much variability in the dependent variable the model can explain in data not used in the fitting process. A higher out-of-sample R-squared value indicates a better predictive ability of the model on new data. It is also a valuable tool for comparing and evaluating the predictive performance of different models relative to a reference or benchmark model. If the value of this metric is greater than 0, it will be interpreted that the model in question has a superior predictive ability to the benchmark model. If it is less than zero, it will be the opposite, while if it is equal to 0, there will be no predictive difference. The naive forecast will be used as benchmark.\nCumulative Squared Prediction Error (CSPE): often used to evaluate the accuracy of predictions in time series models. It is calculated by adding the squared errors of the predictions over a given period. Squared errors are used to give more weight to large errors, reflecting the importance of avoiding large deviations in predictions.\nTable 2 summarizes the values of the three first mentioned metrics, while Figure 4 shows the CSPE.\n\nTable 2: Model’s performance indicators\n\n\nModel\nMSE\nR OOS\nDM statistic\nDM p. value\n\n\n\n\nAR(1)\n0.1109\n0.2472\n0\n1\n\n\nARIMA\n0.0796\n0.4597\n0\n1\n\n\nHolt-Winters\n0.0918\n0.3769\n0\n1\n\n\nADAM-ETS\n0.1359\n0.0778\n0\n1\n\n\nCES\n0.0857\n0.4183\n0\n1\n\n\nSSARIMA\n0.1130\n0.2331\n0\n1\n\n\nNaive\n0.1474\n0\n\n\n\n\n\n                                       Figure 4: Model’s CSPE\n\n\n\n\n\n\nWe can judge that all the models seem to provide better prediction results compared to a simple naive forecast. Indeed, the 𝑅 𝑂𝑂𝑆 is always positive and even some models have relatively elevated values such as ARIMA or CES, between 0.4 and 0.5. Equally important, the Mean Squared Error of each model is lower than 0.147 , the MSE of the naive model. However, if we look at the DM test statistic and p. value, we can see that null hypothesis is accepted, meaning that there is no significant difference in the forecast accuracy between model i and the naive prediction. If we consider the CSPE, ARIMA seems to be the best model because the accumulated sum of errors is the smallest one, even if at the firsts stages, it starts with a higher prediction error than other methods such as SSARIMA, ADAM-ETS and AR1. On the contrary, these last models end up having much higher CSPE values. This observation can suggest that there exists a difference on the prediction ability between models in function of the forecast horizon. At the very short-term, ADAM-ETS, SSARIMA and AR(1) provide more reliable results, whereas from a horizon of about 7 or 8 periods, ARIMA, CES and Holt-Winters become more accurate. Finally, if we had to choose a better model on which we could comparatively rely, it would be ARIMA, because it presents the highest 𝑅 𝑂𝑂𝑆 value and the lowest MSE and CSPE.\n\n\nConclusion\nIn conclusion, this work analyzed the trend of the historic evolution of the Spanish population. We have chosen and selected different prediction models to evaluate which one is more accurately adjusted to the structure of the series and which can be more reliable for use in a real-life situation. The result is that the ARIMA structure with three Moving Average processes has a minor prediction error. This model has predicted a tendency of increase for the near historical years, adjusting to the real life, with a satisfactory error term. Nevertheless, the analysis and exploration of other techniques might continue to get models with even lower prediction errors, and cross-validation with other independent data sources is recommended to confirm the robustness of the conclusions drawn from this constructed series."
  },
  {
    "objectID": "R/NLP Python/NLP.html",
    "href": "R/NLP Python/NLP.html",
    "title": "Python: Can we distinguish IA from human writing?",
    "section": "",
    "text": "Access to code and full analysis.\nIn recent years, tools like ChatGPT have made it increasingly common to generate texts using artificial intelligence. This raises the question of whether it is possible to distinguish between human-written and machine-generated text. While in some cases it might be difficult to answer this question based on human judgment alone, natural language processing (NLP) techniques and machine learning models can be trained to learn from the discrepancies between human- and AI-written texts, and then classify and predict the likelihood that a given text was produced by a non-human author. To achieve this, it is necessary to have a dataset containing essay excerpts that we can confidently attribute to either source.\n\nData and Preprocessing\nThe analysis was conducted on a dataset from Kaggle, containing 10,000 short English texts. Each entry was labeled with a binary variable: generated = 1 for AI-generated text and 0 for human-written. In order to analyze the data, the texts underwent some necessary standard preprocessing steps:\n\nStopword removal: Common but uninformative words such as “the” or “is” were excluded.\nLemmatization: Words were reduced to their base form (e.g., “running” → “run”).\nTokenization: Texts were segmented into meaningful units (tokens).\nText Representation: TF-IDF To feed the texts into machine learning models, the TF-IDF (Term Frequency-Inverse Document Frequency) method was used to transform textual data into numerical vectors. TF-IDF assigns higher weight to words that are frequent in a document but rare across the corpus, allowing the model to identify distinctive terms.\n\n\n\nExploratory Analysis of Textual Features\nBeyond model training, the project also involved a deep exploration of the texts’ linguistic characteristics.\nOn the one hand, a variety of syntactic and stylistic features were extracted and compared between AI- and human-generated texts: use of punctuation, average sentence length, number of paragraphs, grammatical quality (spelling and coherence), sentence complexity (syntactic depth), readability (Flesch index), lexical diversity, frequency of grammatical categories (nouns, verbs, adverbs), and semantic coherence (cosine similarity between sentence embeddings)\n\nResults: These analyses revealed that human-written texts tend to show greater variability, richer structure, and a more diverse use of grammar, while AI-generated texts are generally more formal, uniform, and neutral in tone.\n\nIn addition, we visualized the words frequency by plotting word clouds, as demonstrated in the following figure.\n\nWe can highligh a first notable difference between the two categories: human-written texts contain several highly frequent, often repeated words (“people,” “think,” etc.), while AI-generated texts present a more diverse lexicon, with a more evenly distributed frequency. A notable exception is “electoral college,” which appears frequently in both corpora. These elements suggest a tendency for humans to repeat themselves more, whereas AI mobilizes a more extensive vocabulary.\n\n\nSentiment and Topic Modeling\n\n- Topic Modeling\nWe now move on to identifying the topics present in the texts. The objective is to highlight the major topics addressed in each category (human vs. generated texts). To do this, we used the Latent Dirichlet Allocation (LDA) method, a topic modeling algorithm that automatically detects groups of co-occurring words reflecting three latent themes defined in our study. To obtain interpretable results, we first imposed rules on the text-to-digital conversion using the CountVectorizer function, such as: excluding words that appear in more than 90% of paragraphs, deleting words contained in fewer than two paragraphs, and ignoring stopwords from the English dictionary. The figure below shows the top 15 most frequent words in each of the topics. From these words, we can inferlatent variables.\n\nThe first topic seem to concern learning, education, and technology. Indeed, words related to school, teaching, and technology are prominent, accompanied by other, more general terms such as “life,” “help,” and “people.” This suggests that this topic is quite broad, encompassing personal or academic learning in a digital or scientific context. The second topic evokes politics and the electoral system, with terms specific to these themes such as “election,” “candidate,” “vote,” “president,” and “electoral.” Finally, the last top 15 refers to the transportation dimension, with modern issues such as pollution, urban mobility, autonomous cars, and sustainability all referred to by “pollution,” “driverless,” “usage,” and “reduce.”\n\n\n- Sentiment analysis\nThen, we performed the sentiment analysis using the VADER (Valence Aware Dictionary for Sentiment Reasoning) method, which involves assigning a polarity score to each text: a score greater than 0.05 = a “positive” text, a score of zero = a “neutral” text, and a score less than -0.05 = a “negative” text.\n\nFigure above shows that topics related to education (topic 0) and politics (topic 1) show a very strong predominance of positive sentiments, with 95% and 94% of texts having a positive tone, respectively. We can also observe that the AI-generated texts present a lower proportion of negative sentiments (5% versus 12% for human texts) and a higher proportion of positive sentiments (95% versus 87%). Thus, the AI ​​productions appear overall more positive and less polarized than those of humans.\n\n\n\nClassification Models\nFive supervised machine learning models were tested: Logistic Regression, Linear SVC (Support Vector Classifier), Non-linear SVC, Random Forest, and Multinomial Naive Bayes. Each model was evaluated using two key metrics: Accuracy: The percentage of correctly classified texts, and F1-score: A balance between precision and recall, especially useful to assess model robustness when categories are unbalanced (more observations of one category than the other).\nThese results demonstrate that traditional NLP techniques and classic machine learning models are effective at distinguishing between AI-generated and human-written texts, even without access to deep learning models. In particular, the LinearSVC model delivered the best generalization performance with a training accuracy of 99.7% and a test accuracy of 97.9%, followed closely by logistic regression and random forest. The non-linear SVC underperformed due to poor generalization (around 50% of accuracy).\nFigure below represents the confusion matrix from our best model: LinearSVC. We observe that only 29 out of 1,537 texts were falsely predicted as AI-generated when they were actually human-generated, while only 33 out of 1,463 texts were incorrectly classified as human-generated when they were AI-generated. This confirms the robustness of our model.\n\n\n\nConclusion\nThe study confirms that there are identifiable linguistic and structural differences between AI- and human-generated texts, and that it is feasible to automate the detection process using NLP and machine learning. The LinearSVC model, in particular, proved to be well-suited for high-dimensional sparse data like TF-IDF vectors.\nHowever, there are also certain limitations: The dataset consists of a single type of short-form text, limiting generalizability. Likewise, the dataset was likely balanced and annotated artificially, which may not reflect real-world distributions. In future works, we could integrate deeper linguistic variables (e.g., rhetorical structure, discourse markers), use advanced language models such as BERT or RoBERTa, and expand the analysis to more diverse text genres (e.g., tweets, dialogue, narratives)."
  },
  {
    "objectID": "R/NLP/NLP code.html",
    "href": "R/NLP/NLP code.html",
    "title": "Python (NLP) : Can we distinguish AI from human writing?",
    "section": "",
    "text": "You can access Python code of our project “Can we distinguish AI from human writing?."
  },
  {
    "objectID": "posts/NLP Python/NLP.html",
    "href": "posts/NLP Python/NLP.html",
    "title": "Can we distinguish AI from human writing?",
    "section": "",
    "text": "In recent years, the emergence of tools like ChatGPT have made it increasingly common to generate texts using artificial intelligence. This raises the question of whether it is possible to distinguish between human-written and machine-generated text. While in some cases it might be difficult to answer this question based on human judgment alone, natural language processing (NLP) techniques and machine learning models can be trained to learn from the discrepancies between human- and AI-written texts, and then classify and predict the likelihood that a given text was produced by a non-human author. To achieve this, it is necessary to have a dataset containing essay excerpts that we can confidently attribute to either source. Access to full analysis and to Python code.\n\nData and Preprocessing\nThe analysis was conducted on a dataset from Kaggle, containing 10,000 short English texts. Each entry was labeled with a binary variable: generated = 1 for AI-generated text and 0 for human-written. In order to analyze the data, the texts underwent some necessary standard preprocessing steps:\n\nStopword removal: Common but uninformative words such as “the” or “is” were excluded.\nLemmatization: Words were reduced to their base form (e.g., “running” → “run”).\nTokenization: Texts were segmented into meaningful units (tokens).\nText Representation: TF-IDF To feed the texts into machine learning models, the TF-IDF (Term Frequency-Inverse Document Frequency) method was used to transform textual data into numerical vectors. TF-IDF assigns higher weight to words that are frequent in a document but rare across the corpus, allowing the model to identify distinctive terms.\n\n\n\nExploratory Analysis of Textual Features\nBeyond model training, the project also involved a deep exploration of the texts’ linguistic characteristics.\nOn the one hand, a variety of syntactic and stylistic features were extracted and compared between AI- and human-generated texts: use of punctuation, average sentence length, number of paragraphs, grammatical quality (spelling and coherence), sentence complexity (syntactic depth), readability (Flesch index), lexical diversity, frequency of grammatical categories (nouns, verbs, adverbs), and semantic coherence (cosine similarity between sentence embeddings)\n\nResults: These analyses revealed that human-written texts tend to show greater variability, richer structure, and a more diverse use of grammar, while AI-generated texts are generally more formal, uniform, and neutral in tone.\n\nIn addition, we visualized the words frequency by plotting word clouds, as demonstrated in the following figure.\n\nWe can highligh a first notable difference between the two categories: human-written texts contain several highly frequent, often repeated words (“people,” “think,” etc.), while AI-generated texts present a more diverse lexicon, with a more evenly distributed frequency. A notable exception is “electoral college,” which appears frequently in both corpora. These elements suggest a tendency for humans to repeat themselves more, whereas AI mobilizes a more extensive vocabulary.\n\n\nSentiment and Topic Modeling\n\n- Topic Modeling\nWe now move on to identifying the topics present in the texts. The objective is to highlight the major topics addressed in each category (human vs. generated texts). To do this, we used the Latent Dirichlet Allocation (LDA) method, a topic modeling algorithm that automatically detects groups of co-occurring words reflecting three latent themes defined in our study. To obtain interpretable results, we first imposed rules on the text-to-digital conversion using the CountVectorizer function, such as: excluding words that appear in more than 90% of paragraphs, deleting words contained in fewer than two paragraphs, and ignoring stopwords from the English dictionary. The figure below shows the top 15 most frequent words in each of the topics. From these words, we can infer latent variables.\n\nThe first topic seem to concern learning, education, and technology. Indeed, words related to school, teaching, and technology are prominent, accompanied by other, more general terms such as “life,” “help,” and “people.” This suggests that this topic is quite broad, encompassing personal or academic learning in a digital or scientific context. The second topic evokes politics and the electoral system, with terms specific to these themes such as “election,” “candidate,” “vote,” “president,” and “electoral.” Finally, the last top 15 refers to the transportation dimension, with modern issues such as pollution, urban mobility, autonomous cars, and sustainability all referred to by “pollution,” “driverless,” “usage,” and “reduce.”\n\n\n- Sentiment analysis\nThen, we performed the sentiment analysis using the VADER (Valence Aware Dictionary for Sentiment Reasoning) method, which involves assigning a polarity score to each text: a score greater than 0.05 = a “positive” text, a score of zero = a “neutral” text, and a score less than -0.05 = a “negative” text.\n\nFigure above shows that topics related to education (topic 0) and politics (topic 1) show a very strong predominance of positive sentiments, with 95% and 94% of texts having a positive tone, respectively. We can also observe that the AI-generated texts present a lower proportion of negative sentiments (5% versus 12% for human texts) and a higher proportion of positive sentiments (95% versus 87%). Thus, the AI ​​productions appear overall more positive and less polarized than those of humans.\n\n\n\nClassification Models\nFive supervised machine learning models were tested: Logistic Regression, Linear SVC (Support Vector Classifier), Non-linear SVC, Random Forest, and Multinomial Naive Bayes. Each model was evaluated using two key metrics: Accuracy: The percentage of correctly classified texts, and F1-score: A balance between precision and recall, especially useful to assess model robustness when categories are unbalanced (more observations of one category than the other).\nThese results demonstrate that traditional NLP techniques and classic machine learning models are effective at distinguishing between AI-generated and human-written texts, even without access to deep learning models. In particular, the LinearSVC model delivered the best generalization performance with a training accuracy of 99.7% and a test accuracy of 97.9%, followed closely by logistic regression and random forest. The non-linear SVC underperformed due to poor generalization (around 50% of accuracy).\nFigure below represents the confusion matrix from our best model: LinearSVC. We observe that only 29 out of 1,537 texts were falsely predicted as AI-generated when they were actually human-generated, while only 33 out of 1,463 texts were incorrectly classified as human-generated when they were AI-generated. This confirms the robustness of our model.\n\n\n\nConclusion\nThe study confirms that there are identifiable linguistic and structural differences between AI- and human-generated texts, and that it is feasible to automate the detection process using NLP and machine learning. The LinearSVC model, in particular, proved to be well-suited for high-dimensional sparse data like TF-IDF vectors.\nHowever, there are also certain limitations: The dataset consists of a single type of short-form text, limiting generalizability. Likewise, the dataset was likely balanced and annotated artificially, which may not reflect real-world distributions. In future works, we could integrate deeper linguistic variables (e.g., rhetorical structure, discourse markers), use advanced language models such as BERT or RoBERTa, and expand the analysis to more diverse text genres (e.g., tweets, dialogue, narratives)."
  },
  {
    "objectID": "posts/Nepal thesis/Nepal project.html#short-synthesis-of-the-main-results-of-the-project",
    "href": "posts/Nepal thesis/Nepal project.html#short-synthesis-of-the-main-results-of-the-project",
    "title": "Master’s Thesis: Assessing the Impact of the Nepal 2015 Gorkha Earthquake on Children’s Health",
    "section": "",
    "text": "Earthquakes, being one of the most devastating natural phenomena on Earth, can cause great damage to human health, fostering, in addition to numerous emotional damages, the emergence of diseases. Among them, cough can reduce the quality of life of those who suffer from it, and its frequency in the population may increase after such an event, given the resulting dust and pollution, the loss of economic means and the worsening of the quality and quantity of clinical services available, among others.\nOn the other hand, Nepal is an underdeveloped country where many children still do not have access to decent and dignified living conditions, and its frequent exposure to earthquakes aggravate these issues. Children under five represent a group of individuals in society who are particularly susceptible to diseases, while their physical and emotional well-being is essential for their development as individuals. The analysis seeks to identify the extent of damage caused, in terms of prevalence of childhood coughing, by the 2015 Nepal Gorkha earthquake.\nThis earthquake, which struck on April 25, 2015, had a magnitude of 7.8 and caused massive destruction and loss of life across central Nepal. It resulted in nearly 9,000 deaths and over 22,000 injuries, while economic losses were estimated at around $7 billion USD, equivalent to about one-third of Nepal’s GDP at the time.\n\n\nData were sourced from Demographic and Health Surveys (DHS), a set of nationally representative surveys that collect information on health and demographics in developing countries. Conducted by ICF International and funded primarily by the United States Agency for International Development (USAID), these surveys address topics such as infant mortality, nutrition, reproductive health, and family planning, providing detailed and internationally comparable information. DHS contain several data files for different family members, such as woman, man, children, and households as a whole. In the analysis, children and households’ files regarding the country of Nepal for the years 2011 (5,038 observations) and 2016 (5,306 observations) are managed. Additionally, geographic coordinates of each surveyed household were required to compute distances to the earthquake’s epicenters, a key aspect of the research. Geospatial data have also been included to specify control variables in the model.\n\n\n\nThe Difference-in-Differences is an econometric technique used to estimate the causal effect of an intervention or policy by comparing changes in outcomes over time between a treatment group, made up of units who receive the intervention, and a control group that serves as counterfactual for treated individuals (i.e. what would have happened to the treatment group in the absence of the intervention). In this case, the earthquake is interpreted as the intervention. Therefore, we consider that individuals under the treatment are children living in areas close to epicentres, whereas the control set is composed of children who live in remote areas. The impact on cough prevalence was measured considering as treated the children living within a wide range of kilometres of either of the two major epicenters (Gorkha district on 15 April, 7,8 magnitude; and Dolakha district, on 12 may, 7,3 magnitude), with the remaining area of the country as control units, the main results of which will be shown below. The mathematical equation is presented next:\nYit = α + β1𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡𝑖 + β2𝑃𝑜𝑠𝑡𝑡 + β3 (Treatment ∗ Post) 𝑖𝑡 + β4X𝑖𝑡 + β5D𝑖𝑡 + ϵ\nWhere 𝑌 reflects the outcome of interest (cough prevalence) for individual 𝑖 at time 𝑡; α is a constant; β the coefficient of each correspondent variable; 𝑇𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡 indicates whether the child belongs to the treatment group (1 if that is the case, 0 otherwise); and 𝑃𝑜𝑠𝑡 takes the value 1 if individuals belong to the post-earthquake period, 0 if not. X corresponds to a set of control variables and D represents region fixed effects. Finally, ϵ is the error term.\n\n\n\nFindings have demonstrated a significant impact up to a distance of 80 kilometers. Table 1 presents the estimates obtained from the above equation considering this distance range. β3 , the coefficient corresponding to the variable Treatment*After, is the DiD estimator which measures the causal effect of the treatment. Its significance at the 10% threshold indicates a significant difference, over the two periods analyzed, in the cough rate in the areas affected by the earthquake relative to non-affected regions. The positiveness of the sign indicates that being within 80 kilometers in the post-treatment group increases the likelihood of coughing compared to the remaining areas.\n\n\n\n\n\nFigure 1 shows the evolution of cough prevalence in both treatment and control groups from the year 2001. We can observe how the cough incidence index, measured from 0 to 1 as a proportion of the sample analyzed, has decreased for both groups in the ten years prior to the earthquake, although it increased slowly in 2011 relative to 2006. After the shock in 2015, the proportion of children suffering from the disease goes up, even if not in a very pronounced way, for treated children. The control region, on the contrary, slightly lowers its cough incidence. The groups compared do not perfectly exhibit parallel trends during the pre-earthquake period, although they are similar. Even though covariates have been included in the model, possibly they would not have followed an exactly parallel evolution had the earthquake not taken place, and this could represent some source of little bias in the results. Figure 2 exhibits a map of Nepal showing the two major epicenters as well as the households surveyed who have been found to be injured by the earthquake.\n\n\n\n\nIn conclusion, the study has analyzed the impact that the 2015 Gorkha earthquake of Nepal had on cough prevalence in children under five. For that, children close to epicenters have been compared to the remaining regions of the country over time. Results have revealed an increase in the rate of the disease in areas within a circle of up to 80 kilometers of any of the two major epicenters. Given the frequency of earthquakes in Nepal and its low buildings construction quality, it is recommended to invest in resistant material to these hazards. Likewise, the importance of counting with health services and qualified professionals after such an event is highlighted. Children are the future of society, and their care is crucial for their development and happiness."
  }
]